{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\">\n",
    "## Открытый курс по машинному обучению\n",
    "<center>Автор материала: Артём Асмоловский"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Скрапинг с использованием библиотеки BeautifulSoup и регулярных выражений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прежде чем вы вы будете создавать собственные скраперы, помните, что случайным образом вы можете оказаться на страницах, содержащих информацию, непредназначенную для публичного использования. Будьте внимательны, когда выбираете ресурс, с которого извлекаете информацию. Если вы скачаете информацию по всем пользователям, скажем, вконтакте, то, как минимум, получите предупреждение прекратить заниматься подобными делами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Веб-скрапинг представляет собой автоматизированняй сбор данных с различных веб-ресурсов. Безусловно, если определённый ресурс имеет свой API, удобнее воспользоваться им, потому как данные там уже удобно структурированы, но тот же твитер ограничивает количество обращений до примерно 150/час."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Именно в такие моменты на помощь и приходит веб-скрапинг. Пусть данные и имеют несколько усложнённую структуру, зато одновременно обрабатывать можно множество сайтов, да и написать собственного бота всегда приятнее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /home/artyom/anaconda3/lib/python3.6/site-packages\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Устанавливаем BS\n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К слову, BeautifulSoup - одна из наиболее популярных и удобных библиотек для парсинга html/xml-разметки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Импортируем необходимые библиотеки\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "import re\n",
    "import csv\n",
    "import warnings\n",
    "import sys\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "#urlopen необходим для открытия веб-страниц"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве веб-сервиса, с которого будем скрапить данные, возьмём англоязычную википедию. А задачу сформулируем следующим образом: перейдём на страничку Стивена Кинга и итеративно будем переходить по всем ссылкам (ссылающимся только на страницы википедии), которые указывают на актёров и актрис и заполним небольшой датасет, извлекая интересную информацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Считываем необходимую страницу и передаём в конструктор BS\n",
    "first_page = urlopen('https://en.wikipedia.org/wiki/Stephen_King')\n",
    "bs = BeautifulSoup(first_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На данный момент мы создали BS-объект, который содержит urlopen со множеством html-тегов и соответствующую им информацию, которая находится на заданной странице. С помощью метода get_text() можно очистить объект от всех тегов. Однако сделав это сейчас, мы потеряем важную информацию, например, что является текстом, а что - названием изображения внутри статьи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'his popularity was an accident. An alternate explanation was that publishing standards at the time allowed only a single book a year.[26] He picked up the name from the hard rock band Bachman-Turner Overdrive, of which he is a fan.[27]\\nRichard Bachma'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Информация представлена следующим образом\n",
    "bs.get_text()[17000:17250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь важный момент. У нас есть объект, содержащий html-разметку, мы хотит из каждой страницы извлекать Имя и Фамилию, а так же возраст. Чтобы понять, где вся эта информация расположена, необходимо взглянуть на страницу википедии через \"режим инспектирования\" (перейти можно, нажав f12 в браузере)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../img/scraping1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BS имеет два метода, которые извлекают информацию по тегам: find() и findAll(). В конструктор им передаётся название интересующего тега, и словарь атрибутов, по которому необходимо извлечь объект. В нашем случае тег - это 'span', а словарь атрибутов - {'class' : 'fn'}. Различия этих методов в том, что первый остановится на первом найденном теге 'span' и соответствущих ему атрибутах, а второй пройдётся по всем на данной странице. Соответственно, первый метод возвращает просто BS-объект, а второй - BS-список, состоящий из BS-объектов. Поэтому, если есть необходимость очистить объект от тегов с помощью get_text(), но при этом до этого был использован метод findAll(), из полученного списка необходимо извлечь объект по индексу, как из простого питоновского списка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stephen King'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs.find('span', {'class' : 'fn'}).get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stephen King'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs.findAll('span', {'class' : 'fn'})[0].get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если методу find передать несуществующий тег, то вернётся пустой объект. Однако если попытаться дальше с ним работать, вызывая дополнительные методы, как у BS-объекта, то будет сгенерировано исключение AttributeError. Поэтому перед запуском скрапера стоит убедиться, что данное исключение перехватывается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caught an error\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    bs.error_tag.error\n",
    "except AttributeError as e:\n",
    "    print('caught an error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь создадим список статей актёров и актрис, на которых имеются ссылки с исследуемой страницы. Проинспектируем блок основного текста, он хранится внутри тега 'div' с атрибутами {'id' : 'bodycontent'}. Затем проинспектируем любую гиперссылку внутри этого блока, тег в этом случае: 'a', атрибуты: {'href' : 'ссылка'}. Как раз здесь и понадобятся регулярные выражения, чтобы ограничить свободу скрапера только вики.\n",
    "Возвращаемый BS-объект может иметь несколько атрибутов, чтобы получить список всех имеющихся, достаточно вызвать .attrs. Применительно к ссылкам, они хранятся в атрибуте с названием 'href'.\n",
    "Если посмотреть на список вики-ссылок, то можно заметить, что все они начинаются с /wiki/, поэтому регулярка в данном случае будет достаточно простой, за тем исключением, что некоторые ссылки могут указывать на изображения (в таких ссылках встречается двоеточие '/wiki/File:Stephen_King,_Comicon.jpg'), которые необходимо игнорировать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/wiki/Stephen_King_(disambiguation)',\n",
       " '/wiki/Portland,_Maine',\n",
       " '/wiki/Richard_Bachman',\n",
       " '/wiki/Horror_fiction',\n",
       " '/wiki/Fantasy_fiction',\n",
       " '/wiki/Science_fiction_literature',\n",
       " '/wiki/Supernatural_fiction',\n",
       " '/wiki/Drama_(fiction)',\n",
       " '/wiki/Gothic_fiction',\n",
       " '/wiki/Genre_fiction']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = []\n",
    "for link in bs.find('div', {'id' : 'bodyContent'}).findAll('a', href = re.compile('\\/wiki\\/((?!:).)*$')):\n",
    "    links.append(link.attrs['href'])\n",
    "\n",
    "links[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Остаётся создать метод, который будет рукурсивно обрабатывать каждую ссылку, и, если, в поле Occupation встретится actor или actress, то парсить необходимую информацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Сюда будем вносить посещённые ссылки, чтобы не оказаться на одной и той же странице несколько раз\n",
    "links = set()\n",
    "def make_scraping(Url):\n",
    "        \n",
    "    global links\n",
    "    name = None\n",
    "    surname = None\n",
    "    birthdate = None\n",
    "    films = None\n",
    "        \n",
    "    html = urlopen('https://en.wikipedia.org' + Url)\n",
    "    bs = BeautifulSoup(html)\n",
    "        \n",
    "    #Если на заданной странице нет поля \"Occupation\", то эта страница, скорее всего, не про человека,\n",
    "    #и будет сгенерировано исключение AttribiteError. Просто перехватим его и пропустим страницу.\n",
    "    try:\n",
    "        #Ищем поле Occupation. Как правило, у знаменитых людей в этом поле несколько объектов, \n",
    "        #иногда и певец может оказаться актёром. Более того, если искомый нами \"actor\" или \"actress\" окажутся \n",
    "        #первыми в этом списке, то будут начинаться с заглавной буквы, поэтому приведём все объекты списка \n",
    "        #к нижнему регистру.\n",
    "        occupation = bs.find('td', {'class' : 'role'}).get_text().lower().split('\\n')\n",
    "        \n",
    "        if (('actor' in occupation) or\n",
    "            ('actress' in occupation)):\n",
    "                \n",
    "            #На всякий случай, если мы вдруг оказались на заготовке статьи об актёре, то некоторые поля могут\n",
    "            #отсутствовать, поэтому введём дополнительную обработку исключений.\n",
    "            try:\n",
    "                name_obj = bs.find('span', {'class' : 'fn'}).get_text()\n",
    "                name = name_obj.split(' ')[0]\n",
    "                surname = name_obj.split(' ')[1]\n",
    "            except AttributeError as e:\n",
    "                name = None\n",
    "                surname = None\n",
    "            try:\n",
    "                age = re.findall('\\d+', bs.find('span', {'class' : 'ForceAgeToShow'}).get_text())[0]\n",
    "            except AttributeError as e:\n",
    "                age = None\n",
    "            #Укажите свой репозиторий\n",
    "            with open('actors.csv', 'a', newline='') as file:\n",
    "                writer = csv.writer(file, delimiter = ',')\n",
    "                writer.writerow([name, surname, age])\n",
    "            \n",
    "            #Извлечём все ссылки из блока основного текста. В данном случае регулярное выражение чуть-чуть сложнее.\n",
    "            #Если гиперссылка ведёт, например, на фотографию, то в ней присутствует двоеточие. Просто укажем, что\n",
    "            #двоеточий быть не должно.\n",
    "            for link in bs.find('div', {'id' : 'bodyContent'}).findAll('a', href = re.compile('\\/wiki\\/((?!:).)*$')):\n",
    "                if link.attrs['href'] not in links:\n",
    "                    links.add(link.attrs['href'])\n",
    "                    try:\n",
    "                        make_scraping(link.attrs['href'])\n",
    "                    except:\n",
    "                        print('an exception' in link.attrs['href'])\n",
    "                        \n",
    "    except AttributeError as e:\n",
    "        pass            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_scraping('/wiki/Stephen_King')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если строить скрапер в надежде, что за достаточно длительное количество времени он соберёт необходимый объём информации, то стоит иметь в виду, что стандартные питоновские ограничения относительно рекурссии составляют 1000 вызовов. Соответственно, для изменения этого значения, необходимо импортировать билиотеку sys, а затем вызвать метод sys.setrecursionlimit(), передав в качестве аргумента необходимое количество вызовов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В процессе скрапинга могут возникать различные ошибки. Наиболее часто встречающиеся: AttributeError и HTTPError из билиотеки urllib.error, которые мы обработали. Однако если оставлять что-то подобное на длительнео время, я бы советовал дополнительно перехватывать базовый класс Exception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, какое-то время паук поперемещался по сайтам и что-то извлекал, стоит оценить результаты его работы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Surname</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Janet</td>\n",
       "      <td>Jackson</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stephen</td>\n",
       "      <td>King</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>John</td>\n",
       "      <td>Mellencamp</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>India</td>\n",
       "      <td>Arie</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alicia</td>\n",
       "      <td>Keys</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Swizz</td>\n",
       "      <td>Beatz</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Busta</td>\n",
       "      <td>Rhymes</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sean</td>\n",
       "      <td>Combs</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lil'</td>\n",
       "      <td>Kim</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Missy</td>\n",
       "      <td>Elliott</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Name     Surname   Age\n",
       "0    Janet     Jackson  51.0\n",
       "1  Stephen        King  70.0\n",
       "2     John  Mellencamp  66.0\n",
       "3    India        Arie  42.0\n",
       "4   Alicia        Keys   NaN\n",
       "5    Swizz       Beatz  39.0\n",
       "6    Busta      Rhymes  45.0\n",
       "7     Sean       Combs  48.0\n",
       "8     Lil'         Kim  43.0\n",
       "9    Missy     Elliott  46.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Укажите ваш репозиторий\n",
    "df = pd.read_csv('actors.csv', names = ['Name', 'Surname', 'Age'])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, парсер действительно честно работал. Правда в поле Age у Alicia Keys стоит None, следует разобраться почему."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = '../../img/scraping2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дело в том, что возраст не находится внутри специального тега ForceAgeToShow. То есть скрапер отработал верно, он честно не нашёл тег и заменил возраст на None.\n",
    "Забавно, но когда я решил проверить Баста Раймса на принадлежность к актёрской профессии, то такое поле действительно указано на википедии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = '../../img/scraping3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В заключение. Создать собственный парсер, извлекающий информацию плоским списком, несложно. Гораздо труднее продумывать архитектуру вложенной извлекаемой информации. Так, в данном примере можно было бы дополнительно извлекать фильмы, к которым каждый рассматриваемый актёр имеет отношение. Переходя на новую ссылку со страницы актёра, если в первом абзаце присутствует слово \"film\", то статья в большинстве случаев о фильме (по собственным наблюдениям). Однако в данном случае труднее извлекать эти названия с n-ого уровня рекурссии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Что стоит почитать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://pythonscraping.com/ - Ryan Mitchell - Web Scraping with Python: Collecting Data from the Modern Web. Эта книга есть и на русском языке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/ - документация по библиотеке BeautifulSoup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://realpython.com/python-web-scraping-practical-introduction/ - интересная ветка с примерами."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
