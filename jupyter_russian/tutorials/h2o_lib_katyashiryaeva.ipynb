{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\">\n",
    "## Открытый курс по машинному обучению\n",
    "<center>Автор материала: Екатерина Ширяева"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Есть такая либа H20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Согласно исследованиям [Gartner в феврале 2018](https://www.gartner.com/doc/reprints?id=1-4RQ3VEZ&ct=180223&st=sb), H2O занимает уверенное место в лидерах рынка среди DataScience и Machine Learning платформ.\n",
    "Gartner считают H2O.ai технологическим лидером, эта платформа мспользуется более чем 100000 data scientistами и удоволетверенность клиентами самая высокая (поддержка, обучение и продажи).  \n",
    "В этом обзоре я хочу показать отличия от реализаций алгоритмов в обычном sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.gartner.com/resources/326400/326456/326456_0001.png;wa8addaec1755a1f0c?reprintKey=1-4RQ3VEZ\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для примера можно взять датасет из [1ой лекции: отток клиентов телекома](https://habrahabr.ru/company/ods/blog/322626/)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/telecom_churn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3333 entries, 0 to 3332\n",
      "Data columns (total 20 columns):\n",
      "State                     3333 non-null object\n",
      "Account length            3333 non-null int64\n",
      "Area code                 3333 non-null int64\n",
      "International plan        3333 non-null object\n",
      "Voice mail plan           3333 non-null object\n",
      "Number vmail messages     3333 non-null int64\n",
      "Total day minutes         3333 non-null float64\n",
      "Total day calls           3333 non-null int64\n",
      "Total day charge          3333 non-null float64\n",
      "Total eve minutes         3333 non-null float64\n",
      "Total eve calls           3333 non-null int64\n",
      "Total eve charge          3333 non-null float64\n",
      "Total night minutes       3333 non-null float64\n",
      "Total night calls         3333 non-null int64\n",
      "Total night charge        3333 non-null float64\n",
      "Total intl minutes        3333 non-null float64\n",
      "Total intl calls          3333 non-null int64\n",
      "Total intl charge         3333 non-null float64\n",
      "Customer service calls    3333 non-null int64\n",
      "Churn                     3333 non-null bool\n",
      "dtypes: bool(1), float64(8), int64(8), object(3)\n",
      "memory usage: 498.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Описание всех признаков можно посмотреть в [1ой лекции](https://habrahabr.ru/company/ods/blog/322626/)  \n",
    "В качестве предварительной обработки заменю все значения churn на 0/1,   \n",
    "для International plan и Voice mail plan сделаю замену yes / no на 0 / 1  \n",
    "а категориальную переменную State пока удалю из обработки (данных недостаточно, чтобы сделать OHE, и укрупнение категорий не стоит в целях этого тьюториала)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Account length</th>\n",
       "      <th>Area code</th>\n",
       "      <th>International plan</th>\n",
       "      <th>Voice mail plan</th>\n",
       "      <th>Number vmail messages</th>\n",
       "      <th>Total day minutes</th>\n",
       "      <th>Total day calls</th>\n",
       "      <th>Total day charge</th>\n",
       "      <th>Total eve minutes</th>\n",
       "      <th>Total eve calls</th>\n",
       "      <th>Total eve charge</th>\n",
       "      <th>Total night minutes</th>\n",
       "      <th>Total night calls</th>\n",
       "      <th>Total night charge</th>\n",
       "      <th>Total intl minutes</th>\n",
       "      <th>Total intl calls</th>\n",
       "      <th>Total intl charge</th>\n",
       "      <th>Customer service calls</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128</td>\n",
       "      <td>415</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>265.1</td>\n",
       "      <td>110</td>\n",
       "      <td>45.07</td>\n",
       "      <td>197.4</td>\n",
       "      <td>99</td>\n",
       "      <td>16.78</td>\n",
       "      <td>244.7</td>\n",
       "      <td>91</td>\n",
       "      <td>11.01</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107</td>\n",
       "      <td>415</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>161.6</td>\n",
       "      <td>123</td>\n",
       "      <td>27.47</td>\n",
       "      <td>195.5</td>\n",
       "      <td>103</td>\n",
       "      <td>16.62</td>\n",
       "      <td>254.4</td>\n",
       "      <td>103</td>\n",
       "      <td>11.45</td>\n",
       "      <td>13.7</td>\n",
       "      <td>3</td>\n",
       "      <td>3.70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>137</td>\n",
       "      <td>415</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>243.4</td>\n",
       "      <td>114</td>\n",
       "      <td>41.38</td>\n",
       "      <td>121.2</td>\n",
       "      <td>110</td>\n",
       "      <td>10.30</td>\n",
       "      <td>162.6</td>\n",
       "      <td>104</td>\n",
       "      <td>7.32</td>\n",
       "      <td>12.2</td>\n",
       "      <td>5</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84</td>\n",
       "      <td>408</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>299.4</td>\n",
       "      <td>71</td>\n",
       "      <td>50.90</td>\n",
       "      <td>61.9</td>\n",
       "      <td>88</td>\n",
       "      <td>5.26</td>\n",
       "      <td>196.9</td>\n",
       "      <td>89</td>\n",
       "      <td>8.86</td>\n",
       "      <td>6.6</td>\n",
       "      <td>7</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75</td>\n",
       "      <td>415</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>166.7</td>\n",
       "      <td>113</td>\n",
       "      <td>28.34</td>\n",
       "      <td>148.3</td>\n",
       "      <td>122</td>\n",
       "      <td>12.61</td>\n",
       "      <td>186.9</td>\n",
       "      <td>121</td>\n",
       "      <td>8.41</td>\n",
       "      <td>10.1</td>\n",
       "      <td>3</td>\n",
       "      <td>2.73</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Account length  Area code  International plan  Voice mail plan  \\\n",
       "0             128        415                   0                1   \n",
       "1             107        415                   0                1   \n",
       "2             137        415                   0                0   \n",
       "3              84        408                   1                0   \n",
       "4              75        415                   1                0   \n",
       "\n",
       "   Number vmail messages  Total day minutes  Total day calls  \\\n",
       "0                     25              265.1              110   \n",
       "1                     26              161.6              123   \n",
       "2                      0              243.4              114   \n",
       "3                      0              299.4               71   \n",
       "4                      0              166.7              113   \n",
       "\n",
       "   Total day charge  Total eve minutes  Total eve calls  Total eve charge  \\\n",
       "0             45.07              197.4               99             16.78   \n",
       "1             27.47              195.5              103             16.62   \n",
       "2             41.38              121.2              110             10.30   \n",
       "3             50.90               61.9               88              5.26   \n",
       "4             28.34              148.3              122             12.61   \n",
       "\n",
       "   Total night minutes  Total night calls  Total night charge  \\\n",
       "0                244.7                 91               11.01   \n",
       "1                254.4                103               11.45   \n",
       "2                162.6                104                7.32   \n",
       "3                196.9                 89                8.86   \n",
       "4                186.9                121                8.41   \n",
       "\n",
       "   Total intl minutes  Total intl calls  Total intl charge  \\\n",
       "0                10.0                 3               2.70   \n",
       "1                13.7                 3               3.70   \n",
       "2                12.2                 5               3.29   \n",
       "3                 6.6                 7               1.78   \n",
       "4                10.1                 3               2.73   \n",
       "\n",
       "   Customer service calls  Churn  \n",
       "0                       1      0  \n",
       "1                       1      0  \n",
       "2                       0      0  \n",
       "3                       2      0  \n",
       "4                       3      0  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'No' : 0, 'Yes' : 1}\n",
    "df['Churn'] = df['Churn'].apply(lambda x : int(x))\n",
    "df['International plan'] = df['International plan'].map(d)\n",
    "df['Voice mail plan'] = df['Voice mail plan'].map(d)\n",
    "df.drop('State', axis=1, inplace=True)\n",
    "\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим на тест и обучающую выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,:-1], df.iloc[:,-1], test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для работы h2o необходимо установить эту библиотеку и запустить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install h2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: java version \"1.8.0_161\"; Java(TM) SE Runtime Environment (build 1.8.0_161-b12); Java HotSpot(TM) 64-Bit Server VM (build 25.161-b12, mixed mode)\n",
      "  Starting server from /Users/katya/venv/lib/python3.6/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /var/folders/w8/wh9cnmfd7mx5w0mt8yx7xkjh0000gn/T/tmpcc5v4dlf\n",
      "  JVM stdout: /var/folders/w8/wh9cnmfd7mx5w0mt8yx7xkjh0000gn/T/tmpcc5v4dlf/h2o_katya_started_from_python.out\n",
      "  JVM stderr: /var/folders/w8/wh9cnmfd7mx5w0mt8yx7xkjh0000gn/T/tmpcc5v4dlf/h2o_katya_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>02 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>Europe/Moscow</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.18.0.4</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>26 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_katya_0q1dh1</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>7.111 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.4 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ----------------------------------------\n",
       "H2O cluster uptime:         02 secs\n",
       "H2O cluster timezone:       Europe/Moscow\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.18.0.4\n",
       "H2O cluster version age:    26 days\n",
       "H2O cluster name:           H2O_from_python_katya_0q1dh1\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    7.111 Gb\n",
       "H2O cluster total cores:    8\n",
       "H2O cluster allowed cores:  8\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.6.4 final\n",
       "--------------------------  ----------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import h2o\n",
    "import os\n",
    "h2o.init(nthreads=-1, max_mem_size=8)\n",
    "# nthreads - количество ядер процессора для вычислений\n",
    "# max_mem_size - максимальный размер оперативной памяти "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все данные надо перевести в специальную структуру h2o : H2OFrame  \n",
    "h2o [поддерживает](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/getting-data-into-h2o.html) большое количество источников, однако у меня не получилось перекодировать csr-матрицу из задания про Элис :)  очень долго висел :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "training = h2o.H2OFrame(pd.concat([X_train, y_train], axis=1))\n",
    "validation = h2o.H2OFrame(pd.concat([X_test, y_test], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на структуру (это обязательно надо делать, т.к. не всегда корректно происходит переход форматов) <br>\n",
    "Здесь будут описаны основные параметры каждой переменной (тип, максимум, минимум, среднее, стандартное отклонение, количество нулевых и пропущенных значений и первые 10 наблюдений)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows:2333\n",
      "Cols:19\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>       </th><th>Account length    </th><th>Area code         </th><th>International plan  </th><th>Voice mail plan   </th><th>Number vmail messages  </th><th>Total day minutes  </th><th>Total day calls   </th><th>Total day charge  </th><th>Total eve minutes  </th><th>Total eve calls   </th><th>Total eve charge  </th><th>Total night minutes  </th><th>Total night calls  </th><th>Total night charge  </th><th>Total intl minutes  </th><th>Total intl calls  </th><th>Total intl charge  </th><th>Customer service calls  </th><th>Churn              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>type   </td><td>int               </td><td>int               </td><td>int                 </td><td>int               </td><td>int                    </td><td>real               </td><td>int               </td><td>real              </td><td>real               </td><td>int               </td><td>real              </td><td>real                 </td><td>int                </td><td>real                </td><td>real                </td><td>int               </td><td>real               </td><td>int                     </td><td>int                </td></tr>\n",
       "<tr><td>mins   </td><td>1.0               </td><td>408.0             </td><td>0.0                 </td><td>0.0               </td><td>0.0                    </td><td>2.6                </td><td>30.0              </td><td>0.44              </td><td>0.0                </td><td>0.0               </td><td>0.0               </td><td>23.2                 </td><td>33.0               </td><td>1.04                </td><td>0.0                 </td><td>0.0               </td><td>0.0                </td><td>0.0                     </td><td>0.0                </td></tr>\n",
       "<tr><td>mean   </td><td>100.37848264037734</td><td>436.71924560651513</td><td>0.09515645092156022 </td><td>0.2726103729104158</td><td>8.032576082297462      </td><td>180.0195027861121  </td><td>100.62280325760835</td><td>30.60383197599656 </td><td>200.95752250321465 </td><td>100.05400771538778</td><td>17.081633090441525</td><td>200.67038148306924   </td><td>99.94813544792119  </td><td>9.030210030004287   </td><td>10.242777539648538  </td><td>4.444492070295745 </td><td>2.7660522931847398 </td><td>1.568795542220319       </td><td>0.14573510501500214</td></tr>\n",
       "<tr><td>maxs   </td><td>232.0             </td><td>510.0             </td><td>1.0                 </td><td>1.0               </td><td>51.0                   </td><td>346.8              </td><td>165.0             </td><td>58.96             </td><td>363.7              </td><td>170.0             </td><td>30.91             </td><td>395.0                </td><td>175.0              </td><td>17.77               </td><td>20.0                </td><td>18.0              </td><td>5.4                </td><td>9.0                     </td><td>1.0                </td></tr>\n",
       "<tr><td>sigma  </td><td>39.815132404220186</td><td>42.11342758508376 </td><td>0.2934938203721862  </td><td>0.4453975630897078</td><td>13.722524774971957     </td><td>54.503148533784056 </td><td>19.89235683817998 </td><td>9.265512312565042 </td><td>50.771196810709434 </td><td>20.081856448464883</td><td>4.315580894314363 </td><td>50.935130537595086   </td><td>19.586623410722094 </td><td>2.292113726067487   </td><td>2.791145550244814   </td><td>2.4515950038863097</td><td>0.7536382712677757 </td><td>1.3337241215350106      </td><td>0.35291609524195916</td></tr>\n",
       "<tr><td>zeros  </td><td>0                 </td><td>0                 </td><td>2111                </td><td>1697              </td><td>1697                   </td><td>0                  </td><td>0                 </td><td>0                 </td><td>1                  </td><td>1                 </td><td>1                 </td><td>0                    </td><td>0                  </td><td>0                   </td><td>13                  </td><td>13                </td><td>13                 </td><td>493                     </td><td>1993               </td></tr>\n",
       "<tr><td>missing</td><td>0                 </td><td>0                 </td><td>0                   </td><td>0                 </td><td>0                      </td><td>0                  </td><td>0                 </td><td>0                 </td><td>0                  </td><td>0                 </td><td>0                 </td><td>0                    </td><td>0                  </td><td>0                   </td><td>0                   </td><td>0                 </td><td>0                  </td><td>0                       </td><td>0                  </td></tr>\n",
       "<tr><td>0      </td><td>80.0              </td><td>510.0             </td><td>0.0                 </td><td>0.0               </td><td>0.0                    </td><td>202.4              </td><td>118.0             </td><td>34.41             </td><td>260.2              </td><td>67.0              </td><td>22.12             </td><td>177.4                </td><td>112.0              </td><td>7.98                </td><td>9.2                 </td><td>5.0               </td><td>2.48               </td><td>3.0                     </td><td>0.0                </td></tr>\n",
       "<tr><td>1      </td><td>63.0              </td><td>510.0             </td><td>0.0                 </td><td>0.0               </td><td>0.0                    </td><td>132.9              </td><td>122.0             </td><td>22.59             </td><td>67.0               </td><td>62.0              </td><td>5.7               </td><td>160.4                </td><td>121.0              </td><td>7.22                </td><td>9.9                 </td><td>2.0               </td><td>2.67               </td><td>3.0                     </td><td>0.0                </td></tr>\n",
       "<tr><td>2      </td><td>116.0             </td><td>510.0             </td><td>0.0                 </td><td>1.0               </td><td>12.0                   </td><td>221.0              </td><td>108.0             </td><td>37.57             </td><td>151.0              </td><td>118.0             </td><td>12.84             </td><td>179.0                </td><td>80.0               </td><td>8.06                </td><td>9.0                 </td><td>6.0               </td><td>2.43               </td><td>2.0                     </td><td>0.0                </td></tr>\n",
       "<tr><td>3      </td><td>71.0              </td><td>415.0             </td><td>0.0                 </td><td>0.0               </td><td>0.0                    </td><td>278.9              </td><td>110.0             </td><td>47.41             </td><td>190.2              </td><td>67.0              </td><td>16.17             </td><td>255.2                </td><td>84.0               </td><td>11.48               </td><td>11.7                </td><td>7.0               </td><td>3.16               </td><td>0.0                     </td><td>1.0                </td></tr>\n",
       "<tr><td>4      </td><td>120.0             </td><td>510.0             </td><td>0.0                 </td><td>1.0               </td><td>43.0                   </td><td>177.9              </td><td>117.0             </td><td>30.24             </td><td>175.1              </td><td>70.0              </td><td>14.88             </td><td>161.3                </td><td>117.0              </td><td>7.26                </td><td>11.5                </td><td>4.0               </td><td>3.11               </td><td>1.0                     </td><td>0.0                </td></tr>\n",
       "<tr><td>5      </td><td>132.0             </td><td>510.0             </td><td>0.0                 </td><td>0.0               </td><td>0.0                    </td><td>181.1              </td><td>121.0             </td><td>30.79             </td><td>314.4              </td><td>109.0             </td><td>26.72             </td><td>246.7                </td><td>81.0               </td><td>11.1                </td><td>4.2                 </td><td>9.0               </td><td>1.13               </td><td>2.0                     </td><td>0.0                </td></tr>\n",
       "<tr><td>6      </td><td>105.0             </td><td>415.0             </td><td>0.0                 </td><td>0.0               </td><td>0.0                    </td><td>156.5              </td><td>102.0             </td><td>26.61             </td><td>140.2              </td><td>134.0             </td><td>11.92             </td><td>227.4                </td><td>111.0              </td><td>10.23               </td><td>12.2                </td><td>2.0               </td><td>3.29               </td><td>2.0                     </td><td>0.0                </td></tr>\n",
       "<tr><td>7      </td><td>117.0             </td><td>510.0             </td><td>1.0                 </td><td>0.0               </td><td>0.0                    </td><td>198.4              </td><td>121.0             </td><td>33.73             </td><td>249.5              </td><td>104.0             </td><td>21.21             </td><td>162.8                </td><td>115.0              </td><td>7.33                </td><td>10.5                </td><td>5.0               </td><td>2.84               </td><td>1.0                     </td><td>0.0                </td></tr>\n",
       "<tr><td>8      </td><td>64.0              </td><td>510.0             </td><td>0.0                 </td><td>0.0               </td><td>0.0                    </td><td>216.9              </td><td>78.0              </td><td>36.87             </td><td>211.0              </td><td>115.0             </td><td>17.94             </td><td>179.8                </td><td>116.0              </td><td>8.09                </td><td>11.4                </td><td>5.0               </td><td>3.08               </td><td>3.0                     </td><td>0.0                </td></tr>\n",
       "<tr><td>9      </td><td>143.0             </td><td>510.0             </td><td>0.0                 </td><td>1.0               </td><td>33.0                   </td><td>141.4              </td><td>130.0             </td><td>24.04             </td><td>186.4              </td><td>114.0             </td><td>15.84             </td><td>210.0                </td><td>111.0              </td><td>9.45                </td><td>7.7                 </td><td>6.0               </td><td>2.08               </td><td>1.0                     </td><td>0.0                </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зависимая переменная для бинарной классификации должна быть не количественной переменной, а категориальной, преобразуем с помощью метода asfactor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['Churn'] = training['Churn'].asfactor()\n",
    "validation['Churn'] = validation['Churn'].asfactor()\n",
    "\n",
    "training['International plan'] = training['International plan'].asfactor()\n",
    "training['Voice mail plan'] = training['Voice mail plan'].asfactor()\n",
    "\n",
    "validation['International plan'] = validation['International plan'].asfactor()\n",
    "validation['Voice mail plan'] = validation['Voice mail plan'].asfactor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sklearn RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC для sklearn RandomForestClassifier: 0.9404\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators=800, random_state=152, n_jobs=-1)\n",
    "forest.fit(X_train, y_train)\n",
    "print('AUC для sklearn RandomForestClassifier: {:.4f}'.format(roc_auc_score(y_test, forest.predict_proba(X_test)[:, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H2ORandomForestEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.estimators import H2ORandomForestEstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "необходимо задать список зависимых переменных и предикторов (это будут X и y в коде)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drf Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "Model Details\n",
      "=============\n",
      "H2ORandomForestEstimator :  Distributed Random Forest\n",
      "Model Key:  tutorial1\n",
      "\n",
      "\n",
      "ModelMetricsBinomial: drf\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.04684101277272021\n",
      "RMSE: 0.21642784657414169\n",
      "LogLoss: 0.20074033630931348\n",
      "Mean Per-Class Error: 0.10923157521914939\n",
      "AUC: 0.8994753696762197\n",
      "Gini: 0.7989507393524393\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.4256055363321799: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>1972.0</td>\n",
       "<td>21.0</td>\n",
       "<td>0.0105</td>\n",
       "<td> (21.0/1993.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>77.0</td>\n",
       "<td>263.0</td>\n",
       "<td>0.2265</td>\n",
       "<td> (77.0/340.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>2049.0</td>\n",
       "<td>284.0</td>\n",
       "<td>0.042</td>\n",
       "<td> (98.0/2333.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0     1    Error    Rate\n",
       "-----  ----  ---  -------  -------------\n",
       "0      1972  21   0.0105   (21.0/1993.0)\n",
       "1      77    263  0.2265   (77.0/340.0)\n",
       "Total  2049  284  0.042    (98.0/2333.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.4256055</td>\n",
       "<td>0.8429487</td>\n",
       "<td>146.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.2397901</td>\n",
       "<td>0.8105802</td>\n",
       "<td>204.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.4746598</td>\n",
       "<td>0.8958924</td>\n",
       "<td>137.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4256055</td>\n",
       "<td>0.9579940</td>\n",
       "<td>146.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9790575</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0034502</td>\n",
       "<td>1.0</td>\n",
       "<td>397.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9790575</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.4256055</td>\n",
       "<td>0.8233476</td>\n",
       "<td>146.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1664149</td>\n",
       "<td>0.8470588</td>\n",
       "<td>235.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.2397901</td>\n",
       "<td>0.8907684</td>\n",
       "<td>204.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.425606     0.842949  146\n",
       "max f2                       0.23979      0.81058   204\n",
       "max f0point5                 0.47466      0.895892  137\n",
       "max accuracy                 0.425606     0.957994  146\n",
       "max precision                0.979058     1         0\n",
       "max recall                   0.00345018   1         397\n",
       "max specificity              0.979058     1         0\n",
       "max absolute_mcc             0.425606     0.823348  146\n",
       "max min_per_class_accuracy   0.166415     0.847059  235\n",
       "max mean_per_class_accuracy  0.23979      0.890768  204"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 14,57 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0102872</td>\n",
       "<td>0.9293375</td>\n",
       "<td>6.8617647</td>\n",
       "<td>6.8617647</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0705882</td>\n",
       "<td>0.0705882</td>\n",
       "<td>586.1764706</td>\n",
       "<td>586.1764706</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0201457</td>\n",
       "<td>0.8840787</td>\n",
       "<td>6.8617647</td>\n",
       "<td>6.8617647</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0676471</td>\n",
       "<td>0.1382353</td>\n",
       "<td>586.1764706</td>\n",
       "<td>586.1764706</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300043</td>\n",
       "<td>0.8429612</td>\n",
       "<td>6.8617647</td>\n",
       "<td>6.8617647</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0676471</td>\n",
       "<td>0.2058824</td>\n",
       "<td>586.1764706</td>\n",
       "<td>586.1764706</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0402915</td>\n",
       "<td>0.8114106</td>\n",
       "<td>6.8617647</td>\n",
       "<td>6.8617647</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0705882</td>\n",
       "<td>0.2764706</td>\n",
       "<td>586.1764706</td>\n",
       "<td>586.1764706</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0501500</td>\n",
       "<td>0.7814183</td>\n",
       "<td>6.8617647</td>\n",
       "<td>6.8617647</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0676471</td>\n",
       "<td>0.3441176</td>\n",
       "<td>586.1764706</td>\n",
       "<td>586.1764706</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1003000</td>\n",
       "<td>0.5665646</td>\n",
       "<td>6.3925842</td>\n",
       "<td>6.6271745</td>\n",
       "<td>0.9316239</td>\n",
       "<td>0.9658120</td>\n",
       "<td>0.3205882</td>\n",
       "<td>0.6647059</td>\n",
       "<td>539.2584213</td>\n",
       "<td>562.7174460</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500214</td>\n",
       "<td>0.2865951</td>\n",
       "<td>2.9576572</td>\n",
       "<td>5.4109916</td>\n",
       "<td>0.4310345</td>\n",
       "<td>0.7885714</td>\n",
       "<td>0.1470588</td>\n",
       "<td>0.8117647</td>\n",
       "<td>195.7657201</td>\n",
       "<td>441.0991597</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2001715</td>\n",
       "<td>0.1805324</td>\n",
       "<td>0.6451232</td>\n",
       "<td>4.2169732</td>\n",
       "<td>0.0940171</td>\n",
       "<td>0.6145610</td>\n",
       "<td>0.0323529</td>\n",
       "<td>0.8441176</td>\n",
       "<td>-35.4876823</td>\n",
       "<td>321.6973170</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000429</td>\n",
       "<td>0.1057973</td>\n",
       "<td>0.1472482</td>\n",
       "<td>2.8623361</td>\n",
       "<td>0.0214592</td>\n",
       "<td>0.4171429</td>\n",
       "<td>0.0147059</td>\n",
       "<td>0.8588235</td>\n",
       "<td>-85.2751830</td>\n",
       "<td>186.2336134</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.3999143</td>\n",
       "<td>0.0762791</td>\n",
       "<td>0.0883489</td>\n",
       "<td>2.1695826</td>\n",
       "<td>0.0128755</td>\n",
       "<td>0.3161844</td>\n",
       "<td>0.0088235</td>\n",
       "<td>0.8676471</td>\n",
       "<td>-91.1651098</td>\n",
       "<td>116.9582624</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5002143</td>\n",
       "<td>0.0559567</td>\n",
       "<td>0.1172951</td>\n",
       "<td>1.7580700</td>\n",
       "<td>0.0170940</td>\n",
       "<td>0.2562125</td>\n",
       "<td>0.0117647</td>\n",
       "<td>0.8794118</td>\n",
       "<td>-88.2704877</td>\n",
       "<td>75.8069963</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6000857</td>\n",
       "<td>0.0424644</td>\n",
       "<td>0.2355971</td>\n",
       "<td>1.5046870</td>\n",
       "<td>0.0343348</td>\n",
       "<td>0.2192857</td>\n",
       "<td>0.0235294</td>\n",
       "<td>0.9029412</td>\n",
       "<td>-76.4402929</td>\n",
       "<td>50.4686975</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999571</td>\n",
       "<td>0.0315723</td>\n",
       "<td>0.2355971</td>\n",
       "<td>1.3236105</td>\n",
       "<td>0.0343348</td>\n",
       "<td>0.1928965</td>\n",
       "<td>0.0235294</td>\n",
       "<td>0.9264706</td>\n",
       "<td>-76.4402929</td>\n",
       "<td>32.3610461</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7998285</td>\n",
       "<td>0.0229590</td>\n",
       "<td>0.2061474</td>\n",
       "<td>1.1840773</td>\n",
       "<td>0.0300429</td>\n",
       "<td>0.1725616</td>\n",
       "<td>0.0205882</td>\n",
       "<td>0.9470588</td>\n",
       "<td>-79.3852562</td>\n",
       "<td>18.4077297</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8997000</td>\n",
       "<td>0.0136415</td>\n",
       "<td>0.2650467</td>\n",
       "<td>1.0820601</td>\n",
       "<td>0.0386266</td>\n",
       "<td>0.1576941</td>\n",
       "<td>0.0264706</td>\n",
       "<td>0.9735294</td>\n",
       "<td>-73.4953295</td>\n",
       "<td>8.2060085</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.2639140</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0384615</td>\n",
       "<td>0.1457351</td>\n",
       "<td>0.0264706</td>\n",
       "<td>1.0</td>\n",
       "<td>-73.6085973</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ---------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0102872                   0.929337           6.86176    6.86176            1                1                           0.0705882       0.0705882                  586.176   586.176\n",
       "    2        0.0201457                   0.884079           6.86176    6.86176            1                1                           0.0676471       0.138235                   586.176   586.176\n",
       "    3        0.0300043                   0.842961           6.86176    6.86176            1                1                           0.0676471       0.205882                   586.176   586.176\n",
       "    4        0.0402915                   0.811411           6.86176    6.86176            1                1                           0.0705882       0.276471                   586.176   586.176\n",
       "    5        0.05015                     0.781418           6.86176    6.86176            1                1                           0.0676471       0.344118                   586.176   586.176\n",
       "    6        0.1003                      0.566565           6.39258    6.62717            0.931624         0.965812                    0.320588        0.664706                   539.258   562.717\n",
       "    7        0.150021                    0.286595           2.95766    5.41099            0.431034         0.788571                    0.147059        0.811765                   195.766   441.099\n",
       "    8        0.200171                    0.180532           0.645123   4.21697            0.0940171        0.614561                    0.0323529       0.844118                   -35.4877  321.697\n",
       "    9        0.300043                    0.105797           0.147248   2.86234            0.0214592        0.417143                    0.0147059       0.858824                   -85.2752  186.234\n",
       "    10       0.399914                    0.0762791          0.0883489  2.16958            0.0128755        0.316184                    0.00882353      0.867647                   -91.1651  116.958\n",
       "    11       0.500214                    0.0559567          0.117295   1.75807            0.017094         0.256213                    0.0117647       0.879412                   -88.2705  75.807\n",
       "    12       0.600086                    0.0424644          0.235597   1.50469            0.0343348        0.219286                    0.0235294       0.902941                   -76.4403  50.4687\n",
       "    13       0.699957                    0.0315723          0.235597   1.32361            0.0343348        0.192897                    0.0235294       0.926471                   -76.4403  32.361\n",
       "    14       0.799829                    0.022959           0.206147   1.18408            0.0300429        0.172562                    0.0205882       0.947059                   -79.3853  18.4077\n",
       "    15       0.8997                      0.0136415          0.265047   1.08206            0.0386266        0.157694                    0.0264706       0.973529                   -73.4953  8.20601\n",
       "    16       1                           0                  0.263914   1                  0.0384615        0.145735                    0.0264706       1                          -73.6086  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomial: drf\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE: 0.04181281142387741\n",
      "RMSE: 0.20448181196350304\n",
      "LogLoss: 0.17373883641178992\n",
      "Mean Per-Class Error: 0.07810625780287395\n",
      "AUC: 0.9416814224282135\n",
      "Gini: 0.883362844856427\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.29251602564007045: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>838.0</td>\n",
       "<td>19.0</td>\n",
       "<td>0.0222</td>\n",
       "<td> (19.0/857.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>21.0</td>\n",
       "<td>122.0</td>\n",
       "<td>0.1469</td>\n",
       "<td> (21.0/143.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>859.0</td>\n",
       "<td>141.0</td>\n",
       "<td>0.04</td>\n",
       "<td> (40.0/1000.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0    1    Error    Rate\n",
       "-----  ---  ---  -------  -------------\n",
       "0      838  19   0.0222   (19.0/857.0)\n",
       "1      21   122  0.1469   (21.0/143.0)\n",
       "Total  859  141  0.04     (40.0/1000.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.2925160</td>\n",
       "<td>0.8591549</td>\n",
       "<td>120.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.2635938</td>\n",
       "<td>0.8644537</td>\n",
       "<td>129.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.58375</td>\n",
       "<td>0.8959538</td>\n",
       "<td>81.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.3804167</td>\n",
       "<td>0.961</td>\n",
       "<td>108.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9825</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0138066</td>\n",
       "<td>1.0</td>\n",
       "<td>381.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9825</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.2925160</td>\n",
       "<td>0.8358744</td>\n",
       "<td>120.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1825</td>\n",
       "<td>0.9090909</td>\n",
       "<td>164.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.2635938</td>\n",
       "<td>0.9218937</td>\n",
       "<td>129.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.292516     0.859155  120\n",
       "max f2                       0.263594     0.864454  129\n",
       "max f0point5                 0.58375      0.895954  81\n",
       "max accuracy                 0.380417     0.961     108\n",
       "max precision                0.9825       1         0\n",
       "max recall                   0.0138066    1         381\n",
       "max specificity              0.9825       1         0\n",
       "max absolute_mcc             0.292516     0.835874  120\n",
       "max min_per_class_accuracy   0.1825       0.909091  164\n",
       "max mean_per_class_accuracy  0.263594     0.921894  129"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 14,30 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.011</td>\n",
       "<td>0.9075</td>\n",
       "<td>6.9930070</td>\n",
       "<td>6.9930070</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0769231</td>\n",
       "<td>0.0769231</td>\n",
       "<td>599.3006993</td>\n",
       "<td>599.3006993</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.02</td>\n",
       "<td>0.8775500</td>\n",
       "<td>6.9930070</td>\n",
       "<td>6.9930070</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0629371</td>\n",
       "<td>0.1398601</td>\n",
       "<td>599.3006993</td>\n",
       "<td>599.3006993</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.03</td>\n",
       "<td>0.8400375</td>\n",
       "<td>6.9930070</td>\n",
       "<td>6.9930070</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0699301</td>\n",
       "<td>0.2097902</td>\n",
       "<td>599.3006993</td>\n",
       "<td>599.3006993</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.04</td>\n",
       "<td>0.7927500</td>\n",
       "<td>6.9930070</td>\n",
       "<td>6.9930070</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0699301</td>\n",
       "<td>0.2797203</td>\n",
       "<td>599.3006993</td>\n",
       "<td>599.3006993</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.05</td>\n",
       "<td>0.7630625</td>\n",
       "<td>6.9930070</td>\n",
       "<td>6.9930070</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0699301</td>\n",
       "<td>0.3496503</td>\n",
       "<td>599.3006993</td>\n",
       "<td>599.3006993</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1</td>\n",
       "<td>0.5127500</td>\n",
       "<td>6.4335664</td>\n",
       "<td>6.7132867</td>\n",
       "<td>0.92</td>\n",
       "<td>0.96</td>\n",
       "<td>0.3216783</td>\n",
       "<td>0.6713287</td>\n",
       "<td>543.3566434</td>\n",
       "<td>571.3286713</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.15</td>\n",
       "<td>0.2634844</td>\n",
       "<td>3.9160839</td>\n",
       "<td>5.7808858</td>\n",
       "<td>0.56</td>\n",
       "<td>0.8266667</td>\n",
       "<td>0.1958042</td>\n",
       "<td>0.8671329</td>\n",
       "<td>291.6083916</td>\n",
       "<td>478.0885781</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.1755025</td>\n",
       "<td>0.8391608</td>\n",
       "<td>4.5454545</td>\n",
       "<td>0.12</td>\n",
       "<td>0.65</td>\n",
       "<td>0.0419580</td>\n",
       "<td>0.9090909</td>\n",
       "<td>-16.0839161</td>\n",
       "<td>354.5454545</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3</td>\n",
       "<td>0.1100417</td>\n",
       "<td>0.1398601</td>\n",
       "<td>3.0769231</td>\n",
       "<td>0.02</td>\n",
       "<td>0.44</td>\n",
       "<td>0.0139860</td>\n",
       "<td>0.9230769</td>\n",
       "<td>-86.0139860</td>\n",
       "<td>207.6923077</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.0825485</td>\n",
       "<td>0.0</td>\n",
       "<td>2.3076923</td>\n",
       "<td>0.0</td>\n",
       "<td>0.33</td>\n",
       "<td>0.0</td>\n",
       "<td>0.9230769</td>\n",
       "<td>-100.0</td>\n",
       "<td>130.7692308</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.0586597</td>\n",
       "<td>0.0</td>\n",
       "<td>1.8461538</td>\n",
       "<td>0.0</td>\n",
       "<td>0.264</td>\n",
       "<td>0.0</td>\n",
       "<td>0.9230769</td>\n",
       "<td>-100.0</td>\n",
       "<td>84.6153846</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.0426986</td>\n",
       "<td>0.1398601</td>\n",
       "<td>1.5617716</td>\n",
       "<td>0.02</td>\n",
       "<td>0.2233333</td>\n",
       "<td>0.0139860</td>\n",
       "<td>0.9370629</td>\n",
       "<td>-86.0139860</td>\n",
       "<td>56.1771562</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.7</td>\n",
       "<td>0.0325182</td>\n",
       "<td>0.3496503</td>\n",
       "<td>1.3886114</td>\n",
       "<td>0.05</td>\n",
       "<td>0.1985714</td>\n",
       "<td>0.0349650</td>\n",
       "<td>0.9720280</td>\n",
       "<td>-65.0349650</td>\n",
       "<td>38.8611389</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0238202</td>\n",
       "<td>0.0699301</td>\n",
       "<td>1.2237762</td>\n",
       "<td>0.01</td>\n",
       "<td>0.175</td>\n",
       "<td>0.0069930</td>\n",
       "<td>0.9790210</td>\n",
       "<td>-93.0069930</td>\n",
       "<td>22.3776224</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.9</td>\n",
       "<td>0.0162297</td>\n",
       "<td>0.1398601</td>\n",
       "<td>1.1033411</td>\n",
       "<td>0.02</td>\n",
       "<td>0.1577778</td>\n",
       "<td>0.0139860</td>\n",
       "<td>0.9930070</td>\n",
       "<td>-86.0139860</td>\n",
       "<td>10.3341103</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0012500</td>\n",
       "<td>0.0699301</td>\n",
       "<td>1.0</td>\n",
       "<td>0.01</td>\n",
       "<td>0.143</td>\n",
       "<td>0.0069930</td>\n",
       "<td>1.0</td>\n",
       "<td>-93.0069930</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ---------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.011                       0.9075             6.99301    6.99301            1                1                           0.0769231       0.0769231                  599.301   599.301\n",
       "    2        0.02                        0.87755            6.99301    6.99301            1                1                           0.0629371       0.13986                    599.301   599.301\n",
       "    3        0.03                        0.840037           6.99301    6.99301            1                1                           0.0699301       0.20979                    599.301   599.301\n",
       "    4        0.04                        0.79275            6.99301    6.99301            1                1                           0.0699301       0.27972                    599.301   599.301\n",
       "    5        0.05                        0.763062           6.99301    6.99301            1                1                           0.0699301       0.34965                    599.301   599.301\n",
       "    6        0.1                         0.51275            6.43357    6.71329            0.92             0.96                        0.321678        0.671329                   543.357   571.329\n",
       "    7        0.15                        0.263484           3.91608    5.78089            0.56             0.826667                    0.195804        0.867133                   291.608   478.089\n",
       "    8        0.2                         0.175502           0.839161   4.54545            0.12             0.65                        0.041958        0.909091                   -16.0839  354.545\n",
       "    9        0.3                         0.110042           0.13986    3.07692            0.02             0.44                        0.013986        0.923077                   -86.014   207.692\n",
       "    10       0.4                         0.0825485          0          2.30769            0                0.33                        0               0.923077                   -100      130.769\n",
       "    11       0.5                         0.0586597          0          1.84615            0                0.264                       0               0.923077                   -100      84.6154\n",
       "    12       0.6                         0.0426986          0.13986    1.56177            0.02             0.223333                    0.013986        0.937063                   -86.014   56.1772\n",
       "    13       0.7                         0.0325182          0.34965    1.38861            0.05             0.198571                    0.034965        0.972028                   -65.035   38.8611\n",
       "    14       0.8                         0.0238202          0.0699301  1.22378            0.01             0.175                       0.00699301      0.979021                   -93.007   22.3776\n",
       "    15       0.9                         0.0162297          0.13986    1.10334            0.02             0.157778                    0.013986        0.993007                   -86.014   10.3341\n",
       "    16       1                           0.00125            0.0699301  1                  0.01             0.143                       0.00699301      1                          -93.007   0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scoring History: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>timestamp</b></td>\n",
       "<td><b>duration</b></td>\n",
       "<td><b>number_of_trees</b></td>\n",
       "<td><b>training_rmse</b></td>\n",
       "<td><b>training_logloss</b></td>\n",
       "<td><b>training_auc</b></td>\n",
       "<td><b>training_lift</b></td>\n",
       "<td><b>training_classification_error</b></td>\n",
       "<td><b>validation_rmse</b></td>\n",
       "<td><b>validation_logloss</b></td>\n",
       "<td><b>validation_auc</b></td>\n",
       "<td><b>validation_lift</b></td>\n",
       "<td><b>validation_classification_error</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>2018-04-03 21:56:17</td>\n",
       "<td> 0.048 sec</td>\n",
       "<td>0.0</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2018-04-03 21:56:17</td>\n",
       "<td> 0.243 sec</td>\n",
       "<td>1.0</td>\n",
       "<td>0.3299550</td>\n",
       "<td>3.7593817</td>\n",
       "<td>0.8308720</td>\n",
       "<td>4.0336049</td>\n",
       "<td>0.1088271</td>\n",
       "<td>0.3270305</td>\n",
       "<td>3.6650092</td>\n",
       "<td>0.7946936</td>\n",
       "<td>4.3246227</td>\n",
       "<td>0.107</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2018-04-03 21:56:17</td>\n",
       "<td> 0.312 sec</td>\n",
       "<td>2.0</td>\n",
       "<td>0.3199525</td>\n",
       "<td>3.3116478</td>\n",
       "<td>0.8127797</td>\n",
       "<td>4.3355429</td>\n",
       "<td>0.1018051</td>\n",
       "<td>0.2733811</td>\n",
       "<td>1.5744127</td>\n",
       "<td>0.8568025</td>\n",
       "<td>6.0808756</td>\n",
       "<td>0.075</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2018-04-03 21:56:17</td>\n",
       "<td> 0.352 sec</td>\n",
       "<td>3.0</td>\n",
       "<td>0.3209383</td>\n",
       "<td>3.0195030</td>\n",
       "<td>0.8090340</td>\n",
       "<td>4.5637209</td>\n",
       "<td>0.1284884</td>\n",
       "<td>0.2540667</td>\n",
       "<td>1.0172508</td>\n",
       "<td>0.8787240</td>\n",
       "<td>6.7476383</td>\n",
       "<td>0.069</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2018-04-03 21:56:18</td>\n",
       "<td> 0.390 sec</td>\n",
       "<td>4.0</td>\n",
       "<td>0.3150823</td>\n",
       "<td>2.7257284</td>\n",
       "<td>0.8089239</td>\n",
       "<td>4.7902886</td>\n",
       "<td>0.1039834</td>\n",
       "<td>0.2466377</td>\n",
       "<td>0.8890110</td>\n",
       "<td>0.8823225</td>\n",
       "<td>6.9930070</td>\n",
       "<td>0.062</td></tr>\n",
       "<tr><td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2018-04-03 21:56:20</td>\n",
       "<td> 3.102 sec</td>\n",
       "<td>81.0</td>\n",
       "<td>0.2222210</td>\n",
       "<td>0.3934846</td>\n",
       "<td>0.9033278</td>\n",
       "<td>6.8617647</td>\n",
       "<td>0.0467210</td>\n",
       "<td>0.2056091</td>\n",
       "<td>0.1763409</td>\n",
       "<td>0.9384991</td>\n",
       "<td>6.9930070</td>\n",
       "<td>0.041</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2018-04-03 21:56:21</td>\n",
       "<td> 3.494 sec</td>\n",
       "<td>82.0</td>\n",
       "<td>0.2220493</td>\n",
       "<td>0.3934773</td>\n",
       "<td>0.9030393</td>\n",
       "<td>6.8617647</td>\n",
       "<td>0.0467210</td>\n",
       "<td>0.2055255</td>\n",
       "<td>0.1764233</td>\n",
       "<td>0.9382910</td>\n",
       "<td>6.9930070</td>\n",
       "<td>0.043</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2018-04-03 21:56:21</td>\n",
       "<td> 3.789 sec</td>\n",
       "<td>83.0</td>\n",
       "<td>0.2220529</td>\n",
       "<td>0.3936901</td>\n",
       "<td>0.9025080</td>\n",
       "<td>6.8617647</td>\n",
       "<td>0.0467210</td>\n",
       "<td>0.2055025</td>\n",
       "<td>0.1765029</td>\n",
       "<td>0.9381972</td>\n",
       "<td>6.9930070</td>\n",
       "<td>0.044</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2018-04-03 21:56:26</td>\n",
       "<td> 8.561 sec</td>\n",
       "<td>709.0</td>\n",
       "<td>0.2169476</td>\n",
       "<td>0.2007973</td>\n",
       "<td>0.8998266</td>\n",
       "<td>6.8617647</td>\n",
       "<td>0.0424346</td>\n",
       "<td>0.2045717</td>\n",
       "<td>0.1738288</td>\n",
       "<td>0.9413754</td>\n",
       "<td>6.9930070</td>\n",
       "<td>0.039</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2018-04-03 21:56:27</td>\n",
       "<td> 9.409 sec</td>\n",
       "<td>800.0</td>\n",
       "<td>0.2164278</td>\n",
       "<td>0.2007403</td>\n",
       "<td>0.8994754</td>\n",
       "<td>6.8617647</td>\n",
       "<td>0.0420060</td>\n",
       "<td>0.2044818</td>\n",
       "<td>0.1737388</td>\n",
       "<td>0.9416814</td>\n",
       "<td>6.9930070</td>\n",
       "<td>0.04</td></tr></table></div>"
      ],
      "text/plain": [
       "     timestamp            duration    number_of_trees    training_rmse        training_logloss     training_auc        training_lift      training_classification_error    validation_rmse      validation_logloss    validation_auc      validation_lift     validation_classification_error\n",
       "---  -------------------  ----------  -----------------  -------------------  -------------------  ------------------  -----------------  -------------------------------  -------------------  --------------------  ------------------  ------------------  ---------------------------------\n",
       "     2018-04-03 21:56:17  0.048 sec   0.0                nan                  nan                  nan                 nan                nan                              nan                  nan                   nan                 nan                 nan\n",
       "     2018-04-03 21:56:17  0.243 sec   1.0                0.3299549533419618   3.759381658451759    0.8308720112517581  4.033604928457869  0.10882708585247884              0.32703054843001095  3.6650091908267997    0.7946936377508139  4.324622745675378   0.107\n",
       "     2018-04-03 21:56:17  0.312 sec   2.0                0.31995248719448316  3.3116478265664515   0.8127796965211596  4.335542873865965  0.10180505415162455              0.273381134867804    1.5744126828608989    0.856802474072019   6.0808756460930375  0.075\n",
       "     2018-04-03 21:56:17  0.352 sec   3.0                0.3209383181496875   3.0195030090134423   0.8090340035860656  4.563720865704773  0.12848837209302325              0.2540666971937434   1.0172508387501398    0.8787239598208093  6.747638326585696   0.069\n",
       "     2018-04-03 21:56:18  0.390 sec   4.0                0.31508233994601303  2.725728410467888    0.8089238889980269  4.790288568257491  0.10398344542162442              0.24663767870350212  0.8890110203449179    0.8823224616690194  6.993006993006993   0.062\n",
       "---  ---                  ---         ---                ---                  ---                  ---                 ---                ---                              ---                  ---                   ---                 ---                 ---\n",
       "     2018-04-03 21:56:20  3.102 sec   81.0               0.2222209614344215   0.3934845664628034   0.9033278238540775  6.861764705882353  0.046720960137162454             0.20560906318441863  0.17634086417814698   0.9384990738549666  6.993006993006993   0.041\n",
       "     2018-04-03 21:56:21  3.494 sec   82.0               0.22204928124644221  0.3934772592299307   0.9030393140698327  6.861764705882353  0.046720960137162454             0.20552547246576877  0.1764232708079174    0.938290997217485   6.993006993006993   0.043\n",
       "     2018-04-03 21:56:21  3.789 sec   83.0               0.2220528667432045   0.3936900972855122   0.9025080428558779  6.861764705882353  0.046720960137162454             0.20550254065985527  0.176502930668173     0.9381971587339148  6.993006993006993   0.044\n",
       "     2018-04-03 21:56:26  8.561 sec   709.0              0.2169475783137592   0.20079733990022758  0.8998265989787787  6.861764705882353  0.04243463351907415              0.2045716731482206   0.17382881484082147   0.9413754273730937  6.993006993006993   0.039\n",
       "     2018-04-03 21:56:27  9.409 sec   800.0              0.21642784657414169  0.20074033630931348  0.8994753696762197  6.861764705882353  0.04200600085726532              0.20448181196350304  0.17373883641178992   0.9416814224282135  6.993006993006993   0.04"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "See the whole table with table.as_data_frame()\n",
      "Variable Importances: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>variable</b></td>\n",
       "<td><b>relative_importance</b></td>\n",
       "<td><b>scaled_importance</b></td>\n",
       "<td><b>percentage</b></td></tr>\n",
       "<tr><td>Total day minutes</td>\n",
       "<td>20748.4902344</td>\n",
       "<td>1.0</td>\n",
       "<td>0.1363729</td></tr>\n",
       "<tr><td>Total day charge</td>\n",
       "<td>19548.7832031</td>\n",
       "<td>0.9421786</td>\n",
       "<td>0.1284876</td></tr>\n",
       "<tr><td>Customer service calls</td>\n",
       "<td>19157.3359375</td>\n",
       "<td>0.9233123</td>\n",
       "<td>0.1259148</td></tr>\n",
       "<tr><td>International plan</td>\n",
       "<td>14547.7001953</td>\n",
       "<td>0.7011450</td>\n",
       "<td>0.0956172</td></tr>\n",
       "<tr><td>Total eve charge</td>\n",
       "<td>9294.6621094</td>\n",
       "<td>0.4479681</td>\n",
       "<td>0.0610907</td></tr>\n",
       "<tr><td>Total eve minutes</td>\n",
       "<td>9159.7636719</td>\n",
       "<td>0.4414665</td>\n",
       "<td>0.0602041</td></tr>\n",
       "<tr><td>Total intl calls</td>\n",
       "<td>8102.6518555</td>\n",
       "<td>0.3905177</td>\n",
       "<td>0.0532560</td></tr>\n",
       "<tr><td>Total intl minutes</td>\n",
       "<td>6614.3745117</td>\n",
       "<td>0.3187882</td>\n",
       "<td>0.0434741</td></tr>\n",
       "<tr><td>Total intl charge</td>\n",
       "<td>6516.2104492</td>\n",
       "<td>0.3140571</td>\n",
       "<td>0.0428289</td></tr>\n",
       "<tr><td>Total night charge</td>\n",
       "<td>5165.4580078</td>\n",
       "<td>0.2489558</td>\n",
       "<td>0.0339508</td></tr>\n",
       "<tr><td>Total night minutes</td>\n",
       "<td>5084.6352539</td>\n",
       "<td>0.2450605</td>\n",
       "<td>0.0334196</td></tr>\n",
       "<tr><td>Total night calls</td>\n",
       "<td>5060.0112305</td>\n",
       "<td>0.2438737</td>\n",
       "<td>0.0332578</td></tr>\n",
       "<tr><td>Total day calls</td>\n",
       "<td>4819.5454102</td>\n",
       "<td>0.2322841</td>\n",
       "<td>0.0316773</td></tr>\n",
       "<tr><td>Number vmail messages</td>\n",
       "<td>4619.8515625</td>\n",
       "<td>0.2226596</td>\n",
       "<td>0.0303647</td></tr>\n",
       "<tr><td>Total eve calls</td>\n",
       "<td>4476.2211914</td>\n",
       "<td>0.2157372</td>\n",
       "<td>0.0294207</td></tr>\n",
       "<tr><td>Account length</td>\n",
       "<td>4363.5600586</td>\n",
       "<td>0.2103074</td>\n",
       "<td>0.0286802</td></tr>\n",
       "<tr><td>Voice mail plan</td>\n",
       "<td>3168.4050293</td>\n",
       "<td>0.1527053</td>\n",
       "<td>0.0208249</td></tr>\n",
       "<tr><td>Area code</td>\n",
       "<td>1697.5781250</td>\n",
       "<td>0.0818169</td>\n",
       "<td>0.0111576</td></tr></table></div>"
      ],
      "text/plain": [
       "variable                relative_importance    scaled_importance    percentage\n",
       "----------------------  ---------------------  -------------------  ------------\n",
       "Total day minutes       20748.5                1                    0.136373\n",
       "Total day charge        19548.8                0.942179             0.128488\n",
       "Customer service calls  19157.3                0.923312             0.125915\n",
       "International plan      14547.7                0.701145             0.0956172\n",
       "Total eve charge        9294.66                0.447968             0.0610907\n",
       "Total eve minutes       9159.76                0.441467             0.0602041\n",
       "Total intl calls        8102.65                0.390518             0.053256\n",
       "Total intl minutes      6614.37                0.318788             0.0434741\n",
       "Total intl charge       6516.21                0.314057             0.0428289\n",
       "Total night charge      5165.46                0.248956             0.0339508\n",
       "Total night minutes     5084.64                0.24506              0.0334196\n",
       "Total night calls       5060.01                0.243874             0.0332578\n",
       "Total day calls         4819.55                0.232284             0.0316773\n",
       "Number vmail messages   4619.85                0.22266              0.0303647\n",
       "Total eve calls         4476.22                0.215737             0.0294207\n",
       "Account length          4363.56                0.210307             0.0286802\n",
       "Voice mail plan         3168.41                0.152705             0.0208249\n",
       "Area code               1697.58                0.0818169            0.0111576"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = training.columns\n",
    "X.remove('Churn')\n",
    "y = 'Churn'\n",
    "\n",
    "rf1 = H2ORandomForestEstimator(model_id='tutorial1', ntrees=800, seed=152)\n",
    "rf1.train(X, y, training_frame=training, validation_frame=validation)\n",
    "rf1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получился большой вывод информации о модели.  \n",
    "Сразу следует обратить внимание на AUC при тех же заданных параметрах (ntrees=800)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9416814224282135\n"
     ]
    }
   ],
   "source": [
    "print(rf1.auc(valid=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вся информация идет сначала об обучающей, потом о валидационной выборке\n",
    "* Отображаются все метрики (MSE, RMSE, LogLoss, AUC, Gini и т.д.)\n",
    "* Строится матрица ошибок (confusion matrix), которая приводится для порогового значения спрогнозированной вероятности события, оптимального с точки зрения F1-меры _(для справки : F1- мера = 2 x точность x полнота/(точность + полнота))_\n",
    "* _Maximum Metrics:_ Рассчитываются на ней различные метрики и соответствующие пороговые значения\n",
    "* _Gains/Lift Table:_ Таблица выигрышей создается путем разбиения данных на группы по квантильным пороговым значениям спрогнозированной верроятности положительного класса\n",
    "* _Variable Importances:_ Информация о важностях предикторов. Информация выводится также в отмасштабированном и процентном видах. На сайте с документацией указано, что важность рассчитывается, как относительное влияние каждой переменной: была ли переменная выбрана при построении дерева и на как изменилась среднеквадратичная ошибка (рассчитывается на всех деревьях)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# можно нарисовать графиком \n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "variables = rf1._model_json['output']['variable_importances']['variable']\n",
    "y_pos = np.arange(len(variables))\n",
    "scaled_importance = rf1._model_json['output']['variable_importances']['scaled_importance']\n",
    "ax.barh(y_pos, scaled_importance, align='center', color='green', ecolor='black')\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(variables)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Scaled Importance')\n",
    "ax.set_title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Описание параметров вызова функции___  \n",
    "1. Параметры, определяющие задачу  \n",
    "    * __model_id:__ идентификатор модели\n",
    "    * __training_frame:__ датасет для построения модели\n",
    "    * __validation_frame:__ датасет для валидации\n",
    "    * __nfolds:__ количество фолдов для кросс-валидации (по умолчанию 0)\n",
    "    * __y:__ имена зависимой переменной\n",
    "    * __x:__ список названий предикторов\n",
    "    * __seed:__ random state <br><br>\n",
    "2. Параметры, задающие сложность дерева\n",
    "    * __ntrees__: количество деревьев\n",
    "    * __max_depth:__  максимальная глубина дерева\n",
    "    * __min_rows:__ минимальное количество наблюдений в терминальном листе\n",
    " <br><br>\n",
    "3. Параметры, определяющие формирование подвыборок\n",
    "    * __mtries:__ количество случайно отбираемых предикторов для разбиения узла. По умолчанию -1: для классификации корень квадратный из р, для регрессии р / 3, где р - количество предикторов\n",
    "    * __sample_rate:__ какую часть строк отбирать (от 0 до 1). По умолчанию 0.6320000291 \n",
    "    * __sample_rate_per_class:__ для построения модели из несбалансированного набора данных. Какую часть строк выбирать для каждого дерева (от 0 до 1) \n",
    "    * __col_sample_rate_per_tree:__ какую часть столбцов выбирать для каждого дерева (от 0 до 1, по умоланчанию 1)\n",
    "    * __col_sample_rate_change_per_level:__ задает изменение отбора столбцов для каждого уровня дерева (от 0 до 2, по умолчанию 1), например: (factor = col_sample_rate_change_per_level)\n",
    "        * level 1: col_sample_rate\n",
    "        * level 2: col_sample_rate * factor\n",
    "        * level 3: col_sample_rate * factor^2\n",
    "        * level 4: col_sample_rate * factor^3\n",
    " <br><br>\n",
    "4. Параметры, определяющие биннинг переменных\n",
    "    * __nbins__: (Numerical/real/int only) для каждой переменной строит по крайней мере n интервалов и затем рабивает по наилучшей точке расщепления\n",
    "    * __nbins_top_level:__  (Numerical/real/int only) определяет максимальное количество интервалов n на вершине дерева. При переходе на увроень ниже делит заданное число на 2 до тех пор, пока не дойдет до уровня nbins \n",
    "    * __nbins_cats:__ (Categorical/enums only) каждая переменная может быть разбита максимум на n интерваловю. Более высокие значения могут привести к переобучению.\n",
    "    * __categorical_encoding:__ схема кодировки для категориальных переменных\n",
    "        * auto: используется схема enum.\n",
    "        * enum: 1 столбец для каждого категориального признака (как есть)\n",
    "        * one_hot_explicit: N+1 новых столбцов для каждого признака с N уровнями\n",
    "        * binary: не более 32 столбцов для признака (используется хеширование)\n",
    "        * eigen: выполняет one-hot-encoding и оставляет k главных компонент\n",
    "        * label_encoder: все категории сортируются в лексикографическом порядке и каждому уровню присваивается целое число, начиная с 0 (например, level 0 -> 0, level 1 -> 1,  и т.д.)\n",
    "        * sort_by_response: все категории сортируются по среднему значению переменной и каждому уровню присваивается значение (например, для уровня с мин средним ответом -> 0, вторым наименьшим -> 1, и т.д.). \n",
    "    * __histogram_type:__ тип гистограммы для поиска оптимальных расщепляющих значений\n",
    "        * AUTO = UniformAdaptive\n",
    "        * UniformAdaptive: задаются интервалы одинаковой ширины (max-min)/N\n",
    "        * QuantilesGlobal: интервалы одинаквого размера (в каждом интервале одинаковое количество наблюдений)\n",
    "        * Random: задает построение Extremely Randomized Trees (XRT). Случайным образом отбирается N-1 точка расщепления и затем выбирается наилучшее разбиение\n",
    "        * RoundRobin: все типы гистограмм (по одному на каждое дерево) перебираются по кругу \n",
    " <br><br>\n",
    "5. Параметры остановки\n",
    "    * __stopping_rounds:__ задает количество шагов в течение которого должно произойти заданное улучшение(stopping_tolerance:) для заданной метрики (stopping_metric) \n",
    "    * __stopping_metric:__  метрика для ранней остановки (по умолчанию логлосс для классификации и дисперсия - для регрессии). Возможные значения: deviance, logloss, mse, rmse, mae, rmsle, auc, lift_top_group, misclassification, mean_per_class_error\n",
    "    * __stopping_tolerance:__ относительное улучшение для остановки обучения (если меньше, то остановка)\n",
    "\n",
    "  \n",
    "  \n",
    "  \n",
    "Полный список всех параметров с описанием можно найти на сайте с [официальной документацией](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/drf.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И в завершение этой части сделаем подбор параметров и попытаемся улучшить модель по разным параметрам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.grid.grid_search import H2OGridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drf Grid Build progress: |████████████████████████████████████████████████| 100%\n",
      "    max_depth                  model_ids                 auc\n",
      "0          14  rf_grid_max_depth_model_1  0.9433909148028168\n",
      "1          18  rf_grid_max_depth_model_2  0.9421669345823371\n",
      "2          10  rf_grid_max_depth_model_0  0.9416895822963501\n",
      "3          24  rf_grid_max_depth_model_3  0.9414692658566638\n",
      "\n",
      "0.9433909148028168\n",
      "Hyperparameters: [max_depth]\n",
      "[14]\n"
     ]
    }
   ],
   "source": [
    "rf_params = {'max_depth': [10, 14, 18, 24]}\n",
    "\n",
    "rf_grid = H2OGridSearch(model=H2ORandomForestEstimator(ntrees=800, seed=152),\n",
    "                          grid_id='rf_grid_max_depth',\n",
    "                          hyper_params=rf_params)\n",
    "rf_grid.train(X, y, training_frame=training, validation_frame=validation)\n",
    "\n",
    "# модели, отсортированные по AUC\n",
    "rf_gridperf = rf_grid.get_grid(sort_by='auc', decreasing=True)\n",
    "print(rf_gridperf)\n",
    "\n",
    "# выберем лучшую модель и выведем AUC на тесте \n",
    "best_rf = rf_gridperf.models[0]\n",
    "print(best_rf.auc(valid=True))\n",
    "print(rf_gridperf.get_hyperparams(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drf Grid Build progress: |████████████████████████████████████████████████| 100%\n",
      "    mtries                model_ids                 auc\n",
      "0        7  rf_grid_mtries1_model_2  0.9422444533296342\n",
      "1        5  rf_grid_mtries1_model_1  0.9417263017029645\n",
      "2        3  rf_grid_mtries1_model_0  0.9406940783836933\n",
      "3        9  rf_grid_mtries1_model_3  0.9390417050860458\n",
      "\n",
      "0.9422444533296342\n",
      "Hyperparameters: [mtries]\n",
      "[7]\n"
     ]
    }
   ],
   "source": [
    "rf_params1 = {'mtries': [3, 5, 7, 9]}\n",
    "\n",
    "rf_grid1 = H2OGridSearch(model=H2ORandomForestEstimator(ntrees=800, seed=152, max_depth=14),\n",
    "                          grid_id='rf_grid_mtries1',\n",
    "                          hyper_params=rf_params1)\n",
    "rf_grid1.train(X, y, training_frame=training, validation_frame=validation)\n",
    "\n",
    "# модели, отсортированные по AUC\n",
    "rf_gridperf1 = rf_grid1.get_grid(sort_by='auc', decreasing=True)\n",
    "print(rf_gridperf1)\n",
    "\n",
    "# выберем лучшую модель и выведем AUC на тесте \n",
    "best_rf1 = rf_gridperf1.models[0]\n",
    "print(best_rf1.auc(valid=True))\n",
    "print(rf_gridperf1.get_hyperparams(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "теперь к настройке более специфических параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drf Grid Build progress: |████████████████████████████████████████████████| 100%\n",
      "      histogram_type                  model_ids                 auc\n",
      "0         RoundRobin  rf_grid_hist_type_model_3  0.9433868348687484\n",
      "1    UniformAdaptive  rf_grid_hist_type_model_0  0.9422444533296342\n",
      "2             Random  rf_grid_hist_type_model_1  0.9391192238333429\n",
      "3    QuantilesGlobal  rf_grid_hist_type_model_2  0.9380339613711842\n",
      "\n",
      "AUC for the best model:  0.9433868348687484\n",
      "Hyperparameters: [histogram_type]\n",
      "['RoundRobin']\n"
     ]
    }
   ],
   "source": [
    "rf_params2 = {'histogram_type': ['UniformAdaptive', 'Random', 'QuantilesGlobal', 'RoundRobin']}\n",
    "\n",
    "rf_grid2 = H2OGridSearch(model=H2ORandomForestEstimator(ntrees=800, seed=152, max_depth=14, mtries=7),\n",
    "                          grid_id='rf_grid_hist_type',\n",
    "                          hyper_params=rf_params2)\n",
    "rf_grid2.train(X, y, training_frame=training, validation_frame=validation)\n",
    "\n",
    "# модели, отсортированные по AUC\n",
    "rf_gridperf2 = rf_grid2.get_grid(sort_by='auc', decreasing=True)\n",
    "print(rf_gridperf2)\n",
    "\n",
    "# выберем лучшую модель и выведем AUC на тесте \n",
    "best_rf2 = rf_gridperf2.models[0]\n",
    "print('AUC for the best model: ', best_rf2.auc(valid=True))\n",
    "print(rf_gridperf2.get_hyperparams(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По типу гистограммы RoundRobin оказался самым оптимальным"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drf Grid Build progress: |████████████████████████████████████████████████| 100%\n",
      "     col_sample_rate_per_tree sample_rate          model_ids  \\\n",
      "0                         0.9         0.6   rf_grid3_model_9   \n",
      "1                         0.7         0.7  rf_grid3_model_12   \n",
      "2                         0.9         0.7  rf_grid3_model_14   \n",
      "3                         0.7         0.5   rf_grid3_model_2   \n",
      "4                         0.8         0.6   rf_grid3_model_8   \n",
      "5                         0.9         0.8  rf_grid3_model_19   \n",
      "6                         0.9         0.9  rf_grid3_model_24   \n",
      "7                         0.9         0.5   rf_grid3_model_4   \n",
      "8                         0.8         0.5   rf_grid3_model_3   \n",
      "9                         0.6         0.5   rf_grid3_model_1   \n",
      "10                        0.7         0.6   rf_grid3_model_7   \n",
      "11                        0.7         0.8  rf_grid3_model_17   \n",
      "12                        0.8         0.7  rf_grid3_model_13   \n",
      "13                        0.6         0.8  rf_grid3_model_16   \n",
      "14                        0.5         0.7  rf_grid3_model_10   \n",
      "15                        0.8         0.8  rf_grid3_model_18   \n",
      "16                        0.5         0.5   rf_grid3_model_0   \n",
      "17                        0.6         0.7  rf_grid3_model_11   \n",
      "18                        0.8         0.9  rf_grid3_model_23   \n",
      "19                        0.7         0.9  rf_grid3_model_22   \n",
      "20                        0.6         0.6   rf_grid3_model_6   \n",
      "21                        0.6         0.9  rf_grid3_model_21   \n",
      "22                        0.5         0.6   rf_grid3_model_5   \n",
      "23                        0.5         0.8  rf_grid3_model_15   \n",
      "24                        0.5         0.9  rf_grid3_model_20   \n",
      "\n",
      "                   auc  \n",
      "0   0.9416855023622818  \n",
      "1   0.9415875839446435  \n",
      "2    0.941518225065483  \n",
      "3   0.9405920800319867  \n",
      "4   0.9405472007572357  \n",
      "5   0.9403554438560272  \n",
      "6   0.9402412057021159  \n",
      "7   0.9402412057021159  \n",
      "8   0.9401188076800678  \n",
      "9   0.9398862514381767  \n",
      "10     0.9396904146029  \n",
      "11  0.9396659349984905  \n",
      "12  0.9393640198774388  \n",
      "13  0.9392212221850494  \n",
      "14  0.9385194735253078  \n",
      "15  0.9375321294807876  \n",
      "16  0.9374015715906031  \n",
      "17  0.9373322127114426  \n",
      "18  0.9373281327773744  \n",
      "19  0.9370751768651419  \n",
      "20  0.9361816713041917  \n",
      "21  0.9354268835015626  \n",
      "22  0.9351494479849205  \n",
      "23   0.935018890094736  \n",
      "24  0.9342763420943118  \n",
      "\n",
      "AUC for the best model:  0.9416855023622818\n",
      "Hyperparameters: [col_sample_rate_per_tree, sample_rate]\n",
      "[0.9, 0.6]\n"
     ]
    }
   ],
   "source": [
    "rf_params3 = {'col_sample_rate_per_tree': [0.5, 0.6, 0.7, 0.8, 0.9], \n",
    "             'sample_rate': [0.5, 0.6, 0.7, 0.8, 0.9] }\n",
    "\n",
    "rf_grid3 = H2OGridSearch(model=H2ORandomForestEstimator(ntrees=800, seed=152, max_depth=14, mtries=7,\n",
    "                                                       histogram_type='RoundRobin'),\n",
    "                          grid_id='rf_grid3',\n",
    "                          hyper_params=rf_params3)\n",
    "rf_grid3.train(X, y, training_frame=training, validation_frame=validation)\n",
    "\n",
    "# модели, отсортированные по AUC\n",
    "rf_gridperf3 = rf_grid3.get_grid(sort_by='auc', decreasing=True)\n",
    "print(rf_gridperf3)\n",
    "\n",
    "# выберем лучшую модель и выведем AUC на тесте \n",
    "best_rf3 = rf_gridperf3.models[0]\n",
    "print('AUC for the best model: ', best_rf3.auc(valid=True))\n",
    "print(rf_gridperf3.get_hyperparams(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sklearn LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC для sklearn LogisticRegression: 0.8271\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression().fit(X_train, y_train)\n",
    "print('AUC для sklearn LogisticRegression: {:.4f}'.format(roc_auc_score(y_test, logreg.predict_proba(X_test)[:, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "проверим на отмасштабированных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC для sklearn LogisticRegression Scaled: 0.8283\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "logreg = LogisticRegression().fit(X_train_scaled, y_train)\n",
    "print('AUC для sklearn LogisticRegression Scaled: {:.4f}'.format(\n",
    "    roc_auc_score(y_test, logreg.predict_proba(X_test_scaled)[:, 1])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "результаты хуже, чем в лесу  \n",
    "займемся подбором параметров в H2O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H2OGeneralizedLinearEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.estimators.glm import H2OGeneralizedLinearEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glm Model Build progress: |███████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "# создаем экземпляр класса H2OGeneralizedLinearEstimator\n",
    "glm_model = H2OGeneralizedLinearEstimator(family= \"binomial\", seed=1000000)\n",
    "# обучаем модель\n",
    "glm_model.train(X, y, training_frame= training, validation_frame=validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ModelMetricsBinomialGLM: glm\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.09918009133349871\n",
      "RMSE: 0.3149287083349162\n",
      "LogLoss: 0.3263867097273659\n",
      "Null degrees of freedom: 2332\n",
      "Residual degrees of freedom: 2312\n",
      "Null deviance: 1937.5065769088035\n",
      "Residual deviance: 1522.9203875878893\n",
      "AIC: 1564.9203875878893\n",
      "AUC: 0.8190335291166141\n",
      "Gini: 0.6380670582332282\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.24799189912357045: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>1773.0</td>\n",
       "<td>220.0</td>\n",
       "<td>0.1104</td>\n",
       "<td> (220.0/1993.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>149.0</td>\n",
       "<td>191.0</td>\n",
       "<td>0.4382</td>\n",
       "<td> (149.0/340.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>1922.0</td>\n",
       "<td>411.0</td>\n",
       "<td>0.1582</td>\n",
       "<td> (369.0/2333.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0     1    Error    Rate\n",
       "-----  ----  ---  -------  --------------\n",
       "0      1773  220  0.1104   (220.0/1993.0)\n",
       "1      149   191  0.4382   (149.0/340.0)\n",
       "Total  1922  411  0.1582   (369.0/2333.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.2479919</td>\n",
       "<td>0.5086551</td>\n",
       "<td>178.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1228917</td>\n",
       "<td>0.6103074</td>\n",
       "<td>263.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.3054631</td>\n",
       "<td>0.5099502</td>\n",
       "<td>150.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.6080174</td>\n",
       "<td>0.8649807</td>\n",
       "<td>47.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9870110</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0069831</td>\n",
       "<td>1.0</td>\n",
       "<td>396.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9870110</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.2479919</td>\n",
       "<td>0.4180577</td>\n",
       "<td>178.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1400732</td>\n",
       "<td>0.75</td>\n",
       "<td>248.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1557006</td>\n",
       "<td>0.7547866</td>\n",
       "<td>237.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.247992     0.508655  178\n",
       "max f2                       0.122892     0.610307  263\n",
       "max f0point5                 0.305463     0.50995   150\n",
       "max accuracy                 0.608017     0.864981  47\n",
       "max precision                0.987011     1         0\n",
       "max recall                   0.00698315   1         396\n",
       "max specificity              0.987011     1         0\n",
       "max absolute_mcc             0.247992     0.418058  178\n",
       "max min_per_class_accuracy   0.140073     0.75      248\n",
       "max mean_per_class_accuracy  0.155701     0.754787  237"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 14,57 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0102872</td>\n",
       "<td>0.7456359</td>\n",
       "<td>5.4322304</td>\n",
       "<td>5.4322304</td>\n",
       "<td>0.7916667</td>\n",
       "<td>0.7916667</td>\n",
       "<td>0.0558824</td>\n",
       "<td>0.0558824</td>\n",
       "<td>443.2230392</td>\n",
       "<td>443.2230392</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0201457</td>\n",
       "<td>0.6584274</td>\n",
       "<td>4.1767263</td>\n",
       "<td>4.8178348</td>\n",
       "<td>0.6086957</td>\n",
       "<td>0.7021277</td>\n",
       "<td>0.0411765</td>\n",
       "<td>0.0970588</td>\n",
       "<td>317.6726343</td>\n",
       "<td>381.7834793</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300043</td>\n",
       "<td>0.5984535</td>\n",
       "<td>4.1767263</td>\n",
       "<td>4.6071849</td>\n",
       "<td>0.6086957</td>\n",
       "<td>0.6714286</td>\n",
       "<td>0.0411765</td>\n",
       "<td>0.1382353</td>\n",
       "<td>317.6726343</td>\n",
       "<td>360.7184874</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0402915</td>\n",
       "<td>0.5507832</td>\n",
       "<td>2.8590686</td>\n",
       "<td>4.1608573</td>\n",
       "<td>0.4166667</td>\n",
       "<td>0.6063830</td>\n",
       "<td>0.0294118</td>\n",
       "<td>0.1676471</td>\n",
       "<td>185.9068627</td>\n",
       "<td>316.0857322</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0501500</td>\n",
       "<td>0.5157454</td>\n",
       "<td>2.9833760</td>\n",
       "<td>3.9293866</td>\n",
       "<td>0.4347826</td>\n",
       "<td>0.5726496</td>\n",
       "<td>0.0294118</td>\n",
       "<td>0.1970588</td>\n",
       "<td>198.3375959</td>\n",
       "<td>292.9386626</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1003000</td>\n",
       "<td>0.3692585</td>\n",
       "<td>3.1083208</td>\n",
       "<td>3.5188537</td>\n",
       "<td>0.4529915</td>\n",
       "<td>0.5128205</td>\n",
       "<td>0.1558824</td>\n",
       "<td>0.3529412</td>\n",
       "<td>210.8320764</td>\n",
       "<td>251.8853695</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500214</td>\n",
       "<td>0.2860657</td>\n",
       "<td>2.9576572</td>\n",
       "<td>3.3328571</td>\n",
       "<td>0.4310345</td>\n",
       "<td>0.4857143</td>\n",
       "<td>0.1470588</td>\n",
       "<td>0.5</td>\n",
       "<td>195.7657201</td>\n",
       "<td>233.2857143</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2001715</td>\n",
       "<td>0.2245553</td>\n",
       "<td>1.7594268</td>\n",
       "<td>2.9386573</td>\n",
       "<td>0.2564103</td>\n",
       "<td>0.4282655</td>\n",
       "<td>0.0882353</td>\n",
       "<td>0.5882353</td>\n",
       "<td>75.9426848</td>\n",
       "<td>193.8657262</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000429</td>\n",
       "<td>0.1520522</td>\n",
       "<td>1.4430321</td>\n",
       "<td>2.4408277</td>\n",
       "<td>0.2103004</td>\n",
       "<td>0.3557143</td>\n",
       "<td>0.1441176</td>\n",
       "<td>0.7323529</td>\n",
       "<td>44.3032063</td>\n",
       "<td>144.0827731</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.3999143</td>\n",
       "<td>0.1133559</td>\n",
       "<td>0.8540394</td>\n",
       "<td>2.0445558</td>\n",
       "<td>0.1244635</td>\n",
       "<td>0.2979636</td>\n",
       "<td>0.0852941</td>\n",
       "<td>0.8176471</td>\n",
       "<td>-14.5960616</td>\n",
       "<td>104.4555829</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5002143</td>\n",
       "<td>0.0863671</td>\n",
       "<td>0.6157994</td>\n",
       "<td>1.7580700</td>\n",
       "<td>0.0897436</td>\n",
       "<td>0.2562125</td>\n",
       "<td>0.0617647</td>\n",
       "<td>0.8794118</td>\n",
       "<td>-38.4200603</td>\n",
       "<td>75.8069963</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6000857</td>\n",
       "<td>0.0648764</td>\n",
       "<td>0.3239460</td>\n",
       "<td>1.5193908</td>\n",
       "<td>0.0472103</td>\n",
       "<td>0.2214286</td>\n",
       "<td>0.0323529</td>\n",
       "<td>0.9117647</td>\n",
       "<td>-67.6054027</td>\n",
       "<td>51.9390756</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999571</td>\n",
       "<td>0.0473250</td>\n",
       "<td>0.2944963</td>\n",
       "<td>1.3446202</td>\n",
       "<td>0.0429185</td>\n",
       "<td>0.1959584</td>\n",
       "<td>0.0294118</td>\n",
       "<td>0.9411765</td>\n",
       "<td>-70.5503661</td>\n",
       "<td>34.4620151</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7998285</td>\n",
       "<td>0.0337193</td>\n",
       "<td>0.2061474</td>\n",
       "<td>1.2024636</td>\n",
       "<td>0.0300429</td>\n",
       "<td>0.1752412</td>\n",
       "<td>0.0205882</td>\n",
       "<td>0.9617647</td>\n",
       "<td>-79.3852562</td>\n",
       "<td>20.2463590</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8997000</td>\n",
       "<td>0.0205791</td>\n",
       "<td>0.2650467</td>\n",
       "<td>1.0984054</td>\n",
       "<td>0.0386266</td>\n",
       "<td>0.1600762</td>\n",
       "<td>0.0264706</td>\n",
       "<td>0.9882353</td>\n",
       "<td>-73.4953295</td>\n",
       "<td>9.8405403</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0017895</td>\n",
       "<td>0.1172951</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0170940</td>\n",
       "<td>0.1457351</td>\n",
       "<td>0.0117647</td>\n",
       "<td>1.0</td>\n",
       "<td>-88.2704877</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0102872                   0.745636           5.43223   5.43223            0.791667         0.791667                    0.0558824       0.0558824                  443.223   443.223\n",
       "    2        0.0201457                   0.658427           4.17673   4.81783            0.608696         0.702128                    0.0411765       0.0970588                  317.673   381.783\n",
       "    3        0.0300043                   0.598454           4.17673   4.60718            0.608696         0.671429                    0.0411765       0.138235                   317.673   360.718\n",
       "    4        0.0402915                   0.550783           2.85907   4.16086            0.416667         0.606383                    0.0294118       0.167647                   185.907   316.086\n",
       "    5        0.05015                     0.515745           2.98338   3.92939            0.434783         0.57265                     0.0294118       0.197059                   198.338   292.939\n",
       "    6        0.1003                      0.369259           3.10832   3.51885            0.452991         0.512821                    0.155882        0.352941                   210.832   251.885\n",
       "    7        0.150021                    0.286066           2.95766   3.33286            0.431034         0.485714                    0.147059        0.5                        195.766   233.286\n",
       "    8        0.200171                    0.224555           1.75943   2.93866            0.25641          0.428266                    0.0882353       0.588235                   75.9427   193.866\n",
       "    9        0.300043                    0.152052           1.44303   2.44083            0.2103           0.355714                    0.144118        0.732353                   44.3032   144.083\n",
       "    10       0.399914                    0.113356           0.854039  2.04456            0.124464         0.297964                    0.0852941       0.817647                   -14.5961  104.456\n",
       "    11       0.500214                    0.0863671          0.615799  1.75807            0.0897436        0.256213                    0.0617647       0.879412                   -38.4201  75.807\n",
       "    12       0.600086                    0.0648764          0.323946  1.51939            0.0472103        0.221429                    0.0323529       0.911765                   -67.6054  51.9391\n",
       "    13       0.699957                    0.047325           0.294496  1.34462            0.0429185        0.195958                    0.0294118       0.941176                   -70.5504  34.462\n",
       "    14       0.799829                    0.0337193          0.206147  1.20246            0.0300429        0.175241                    0.0205882       0.961765                   -79.3853  20.2464\n",
       "    15       0.8997                      0.0205791          0.265047  1.09841            0.0386266        0.160076                    0.0264706       0.988235                   -73.4953  9.84054\n",
       "    16       1                           0.00178946         0.117295  1                  0.017094         0.145735                    0.0117647       1                          -88.2705  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "glm_model.model_performance().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.8190335291166141'"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(glm_model.model_performance().auc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Описание параметров вызова функции___  \n",
    "1. Параметры, определяющие задачу  \n",
    "    * __model_id:__ идентификатор \n",
    "    * __training_frame:__ датасет для построения модели\n",
    "    * __validation_frame:__ датасет для валидации\n",
    "    * __nfolds:__ количество фолдов для кросс-валидации (по умолчанию 0)\n",
    "    * __y:__ имена зависимой переменной\n",
    "    * __x:__ список названий предикторов\n",
    "    * __seed:__ random state \n",
    "    * __family:__ тип модели (gaussian, binomial, multinomial, ordinal, quasibinomial, poisson, gamma, tweedie   \n",
    "    * __solver:__ \n",
    "        * IRLSM: Iteratively Reweighted Least Squares Method - используется с небольшим количеством предикторов и для l1-регулярзации\n",
    "        * L_BFGS: Limited-memory Broyden-Fletcher-Goldfarb-Shanno algorithm - используется для данных в большим числом колонок\n",
    "        * COORDINATE_DESCENT, COORDINATE_DESCENT_NAIVE - экспериментальные\n",
    "        * AUTO: Sets the solver based on given data and parameters (default)\n",
    "        * GRADIENT_DESCENT_LH, GRADIENT_DESCENT_SQERR: используется только для family=Ordinal<br><br>\n",
    "2. Параметры, определяющие регуляризацию   \n",
    "для справки формула ElasticNet, объединяющая $L_1$ и $L_2$ регуляризацию\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\lambda R\\left(\\textbf W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\lambda \\left(\\alpha \\sum_{k=1}^K\\sum_{i=1}^M w_{ki}^2 + \\left(1 - \\alpha\\right) \\sum_{k=1}^K\\sum_{i=1}^M \\left|w_{ki}\\right| \\right)\n",
    "\\end{array}$$ где $\\alpha \\in \\left[0, 1\\right]$\n",
    "\n",
    "    * __alpha:__  распределение между $L_1$ и $L_2$ регуляризацией. (1 - $L_1$, 0 - $L_2$)\n",
    "    * __lambda:__ сила регуляризации\n",
    "    * __lambda_search:__ True / False. Определяет стоит ли начинать поиск $\\lambda$, начиная с максимального значения\n",
    "    * __lambda_min_ratio:__ минимальное значение $\\lambda$, используемое при поиске $\\lambda$\n",
    "    * __nlambdas:__ количество шагов при поиске $\\lambda$ (по умолчанию 100)\n",
    " <br><br>\n",
    "3. Параметры, влияющие на предобработку предикторов\n",
    "    * __standardize:__ использовать ли масштабирование\n",
    "    * __missing_values_handling:__ как работать с пропцщенными значениями (пропускать или испутировать средним)\n",
    "    * __remove_collinear_columns:__ удалять ли автоматически коллинеарные столбцы при построении модели \n",
    "    * __interactions:__ список колонок, из которых буду составлены все возможные пары и использованы для построения модели \n",
    "    * __interaction_pairs:__ список уже готовых пар для модели \n",
    " <br><br>\n",
    "\n",
    "Полный список всех параметров с описанием можно найти на сайте с [официальной документацией](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/glm.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glm Grid Build progress: |████████████████████████████████████████████████| 100%\n",
      "                      alpha             model_ids                 auc\n",
      "0                     [0.0]   gridresults_model_0  0.8284550921657106\n",
      "1                     [1.0]  gridresults_model_20  0.8283938931546866\n",
      "2      [0.9500000000000001]  gridresults_model_19  0.8283490138799356\n",
      "3                     [0.9]  gridresults_model_18  0.8282755750667069\n",
      "4      [0.8500000000000001]  gridresults_model_17  0.8282388556600926\n",
      "5                     [0.8]  gridresults_model_16  0.8282347757260242\n",
      "6      [0.7000000000000001]  gridresults_model_14  0.8282102961216147\n",
      "7                    [0.75]  gridresults_model_15   0.828202136253478\n",
      "8                    [0.65]  gridresults_model_13  0.8281613369127955\n",
      "9      [0.6000000000000001]  gridresults_model_12  0.8280185392204062\n",
      "10                   [0.55]  gridresults_model_11  0.8278186224510612\n",
      "11                    [0.5]  gridresults_model_10   0.827785982978515\n",
      "12                   [0.45]   gridresults_model_9   0.827753343505969\n",
      "13    [0.35000000000000003]   gridresults_model_7  0.8275615866047604\n",
      "14                    [0.4]   gridresults_model_8  0.8275493468025557\n",
      "15    [0.30000000000000004]   gridresults_model_6  0.8274432685167807\n",
      "16                   [0.25]   gridresults_model_5  0.8272963908903233\n",
      "17                    [0.2]   gridresults_model_4  0.8272392718133674\n",
      "18    [0.15000000000000002]   gridresults_model_3  0.8270801543847051\n",
      "19                   [0.05]   gridresults_model_1  0.8270679145825003\n",
      "20                    [0.1]   gridresults_model_2  0.8270107955055446\n",
      "\n",
      "AUC for the best model:  0.8284550921657106\n",
      "['Ridge ( lambda = 3.014E-5 )']\n"
     ]
    }
   ],
   "source": [
    "hyper_parameters = {'alpha': np.arange(0, 1.05, 0.05).tolist()}\n",
    "\n",
    "gridsearch = H2OGridSearch(H2OGeneralizedLinearEstimator(family='binomial', lambda_search=True, standardize=True),\n",
    "                           grid_id=\"gridresults\", hyper_params=hyper_parameters)\n",
    "gridsearch.train(X, y, training_frame= training, validation_frame=validation)\n",
    "\n",
    "gridperf = gridsearch.get_grid(sort_by=\"auc\", decreasing=True)\n",
    "best_model = gridperf.models[0]\n",
    "print(gridperf)\n",
    "\n",
    "print('AUC for the best model: ', best_model.auc(valid=True))\n",
    "print(best_model.summary()['regularization'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sklearn GradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC для sklearn GradientBoostingClassifier: 0.9357\n"
     ]
    }
   ],
   "source": [
    "grb = GradientBoostingClassifier().fit(X_train, y_train)\n",
    "print('AUC для sklearn GradientBoostingClassifier: {:.4f}'.format(\n",
    "    roc_auc_score(y_test, grb.predict_proba(X_test)[:, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbm Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "\n",
      "ModelMetricsBinomial: gbm\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 4.97643761962381e-09\n",
      "RMSE: 7.05438701775272e-05\n",
      "LogLoss: 2.387073099550759e-05\n",
      "Mean Per-Class Error: 0.0\n",
      "AUC: 1.0\n",
      "Gini: 1.0\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.9991932827883621: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>1993.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td> (0.0/1993.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>0.0</td>\n",
       "<td>340.0</td>\n",
       "<td>0.0</td>\n",
       "<td> (0.0/340.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>1993.0</td>\n",
       "<td>340.0</td>\n",
       "<td>0.0</td>\n",
       "<td> (0.0/2333.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0     1    Error    Rate\n",
       "-----  ----  ---  -------  ------------\n",
       "0      1993  0    0        (0.0/1993.0)\n",
       "1      0     340  0        (0.0/340.0)\n",
       "Total  1993  340  0        (0.0/2333.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.9991933</td>\n",
       "<td>1.0</td>\n",
       "<td>158.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.9991933</td>\n",
       "<td>1.0</td>\n",
       "<td>158.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.9991933</td>\n",
       "<td>1.0</td>\n",
       "<td>158.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.9991933</td>\n",
       "<td>1.0</td>\n",
       "<td>158.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>1.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.9991933</td>\n",
       "<td>1.0</td>\n",
       "<td>158.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>1.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.9991933</td>\n",
       "<td>1.0</td>\n",
       "<td>158.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.9991933</td>\n",
       "<td>1.0</td>\n",
       "<td>158.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.9991933</td>\n",
       "<td>1.0</td>\n",
       "<td>158.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value    idx\n",
       "---------------------------  -----------  -------  -----\n",
       "max f1                       0.999193     1        158\n",
       "max f2                       0.999193     1        158\n",
       "max f0point5                 0.999193     1        158\n",
       "max accuracy                 0.999193     1        158\n",
       "max precision                1            1        0\n",
       "max recall                   0.999193     1        158\n",
       "max specificity              1            1        0\n",
       "max absolute_mcc             0.999193     1        158\n",
       "max min_per_class_accuracy   0.999193     1        158\n",
       "max mean_per_class_accuracy  0.999193     1        158"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 14,57 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0102872</td>\n",
       "<td>0.9999999</td>\n",
       "<td>6.8617647</td>\n",
       "<td>6.8617647</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0705882</td>\n",
       "<td>0.0705882</td>\n",
       "<td>586.1764706</td>\n",
       "<td>586.1764706</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0201457</td>\n",
       "<td>0.9999990</td>\n",
       "<td>6.8617647</td>\n",
       "<td>6.8617647</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0676471</td>\n",
       "<td>0.1382353</td>\n",
       "<td>586.1764706</td>\n",
       "<td>586.1764706</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300043</td>\n",
       "<td>0.9999975</td>\n",
       "<td>6.8617647</td>\n",
       "<td>6.8617647</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0676471</td>\n",
       "<td>0.2058824</td>\n",
       "<td>586.1764706</td>\n",
       "<td>586.1764706</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0402915</td>\n",
       "<td>0.9999958</td>\n",
       "<td>6.8617647</td>\n",
       "<td>6.8617647</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0705882</td>\n",
       "<td>0.2764706</td>\n",
       "<td>586.1764706</td>\n",
       "<td>586.1764706</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0501500</td>\n",
       "<td>0.9999929</td>\n",
       "<td>6.8617647</td>\n",
       "<td>6.8617647</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0676471</td>\n",
       "<td>0.3441176</td>\n",
       "<td>586.1764706</td>\n",
       "<td>586.1764706</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1003000</td>\n",
       "<td>0.9999602</td>\n",
       "<td>6.8617647</td>\n",
       "<td>6.8617647</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.3441176</td>\n",
       "<td>0.6882353</td>\n",
       "<td>586.1764706</td>\n",
       "<td>586.1764706</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500214</td>\n",
       "<td>0.0001398</td>\n",
       "<td>6.2702333</td>\n",
       "<td>6.6657143</td>\n",
       "<td>0.9137931</td>\n",
       "<td>0.9714286</td>\n",
       "<td>0.3117647</td>\n",
       "<td>1.0</td>\n",
       "<td>527.0233266</td>\n",
       "<td>566.5714286</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2001715</td>\n",
       "<td>0.0000579</td>\n",
       "<td>0.0</td>\n",
       "<td>4.9957173</td>\n",
       "<td>0.0</td>\n",
       "<td>0.7280514</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>399.5717345</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000429</td>\n",
       "<td>0.0000241</td>\n",
       "<td>0.0</td>\n",
       "<td>3.3328571</td>\n",
       "<td>0.0</td>\n",
       "<td>0.4857143</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>233.2857143</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.3999143</td>\n",
       "<td>0.0000117</td>\n",
       "<td>0.0</td>\n",
       "<td>2.5005359</td>\n",
       "<td>0.0</td>\n",
       "<td>0.3644159</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>150.0535906</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5002143</td>\n",
       "<td>0.0000058</td>\n",
       "<td>0.0</td>\n",
       "<td>1.9991431</td>\n",
       "<td>0.0</td>\n",
       "<td>0.2913453</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>99.9143102</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6000857</td>\n",
       "<td>0.0000030</td>\n",
       "<td>0.0</td>\n",
       "<td>1.6664286</td>\n",
       "<td>0.0</td>\n",
       "<td>0.2428571</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>66.6428571</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999571</td>\n",
       "<td>0.0000013</td>\n",
       "<td>0.0</td>\n",
       "<td>1.4286589</td>\n",
       "<td>0.0</td>\n",
       "<td>0.2082058</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>42.8658910</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7998285</td>\n",
       "<td>0.0000006</td>\n",
       "<td>0.0</td>\n",
       "<td>1.2502680</td>\n",
       "<td>0.0</td>\n",
       "<td>0.1822079</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>25.0267953</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8997000</td>\n",
       "<td>0.0000002</td>\n",
       "<td>0.0</td>\n",
       "<td>1.1114817</td>\n",
       "<td>0.0</td>\n",
       "<td>0.1619819</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>11.1481658</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.1457351</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift     cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain     cumulative_gain\n",
       "--  -------  --------------------------  -----------------  -------  -----------------  ---------------  --------------------------  --------------  -------------------------  -------  -----------------\n",
       "    1        0.0102872                   1                  6.86176  6.86176            1                1                           0.0705882       0.0705882                  586.176  586.176\n",
       "    2        0.0201457                   0.999999           6.86176  6.86176            1                1                           0.0676471       0.138235                   586.176  586.176\n",
       "    3        0.0300043                   0.999997           6.86176  6.86176            1                1                           0.0676471       0.205882                   586.176  586.176\n",
       "    4        0.0402915                   0.999996           6.86176  6.86176            1                1                           0.0705882       0.276471                   586.176  586.176\n",
       "    5        0.05015                     0.999993           6.86176  6.86176            1                1                           0.0676471       0.344118                   586.176  586.176\n",
       "    6        0.1003                      0.99996            6.86176  6.86176            1                1                           0.344118        0.688235                   586.176  586.176\n",
       "    7        0.150021                    0.000139845        6.27023  6.66571            0.913793         0.971429                    0.311765        1                          527.023  566.571\n",
       "    8        0.200171                    5.79219e-05        0        4.99572            0                0.728051                    0               1                          -100     399.572\n",
       "    9        0.300043                    2.41348e-05        0        3.33286            0                0.485714                    0               1                          -100     233.286\n",
       "    10       0.399914                    1.17004e-05        0        2.50054            0                0.364416                    0               1                          -100     150.054\n",
       "    11       0.500214                    5.77469e-06        0        1.99914            0                0.291345                    0               1                          -100     99.9143\n",
       "    12       0.600086                    2.98531e-06        0        1.66643            0                0.242857                    0               1                          -100     66.6429\n",
       "    13       0.699957                    1.30511e-06        0        1.42866            0                0.208206                    0               1                          -100     42.8659\n",
       "    14       0.799829                    6.00854e-07        0        1.25027            0                0.182208                    0               1                          -100     25.0268\n",
       "    15       0.8997                      1.69372e-07        0        1.11148            0                0.161982                    0               1                          -100     11.1482\n",
       "    16       1                           2.73715e-10        0        1                  0                0.145735                    0               1                          -100     0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gbm = H2OGradientBoostingEstimator(ntrees = 800, learn_rate = 0.1,seed = 1234)\n",
    "gbm.train(X, y, training_frame= training, validation_frame=validation)\n",
    "gbm.model_performance().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC на валидационной выборке:  0.9146600190940914\n"
     ]
    }
   ],
   "source": [
    "print('AUC на валидационной выборке: ',gbm.auc(valid=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Описание параметров вызова функции___  \n",
    "только специфические, отличаюшиеся от RandomForest\n",
    "1. Параметры для GBM\n",
    "    * __learn_rate:__ скорость обучения (от 0 до 1)\n",
    "    * __learn_rate_annealing:__ уменьшает скорость обучения после построения каждого дерева\n",
    "    \n",
    "Для настройки параметров бустинга есть следующие советы от экспертов (Марк Лэндри, Дмитрий Ларько и [github](https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/tutorials/gbm/gbmTuning.ipynb)): \n",
    "* заификисровать количество деревьев константой, подобрать leraning rate (например, learn_rate=0.02, learn_rate_annealing=0.995) \n",
    "* в самом конце можно снова вернуться к настройке количества деревьев\n",
    "* потом необходимо настроить глубину деревьев (max_depth) (чаще всего это 4-10)\n",
    "* попробовать изменить тип гистрограммы, nbins/nbins_cat\n",
    "* изменить настройки, определяющие подвыборки (sample_rate, col_sample_rate), чаще всего это 70-80%\n",
    "* для несбалансированных наборов необходимо настроить параметры, отвечающие за баланс классов (sample_rate_per_class)\n",
    "* определить критерии остановки "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "начинаем настройку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbm Grid Build progress: |████████████████████████████████████████████████| 100%\n",
      "    learn_rate                    model_ids                 auc\n",
      "0        0.001  gbm_grid_learn_rate_model_1  0.9319711793457418\n",
      "1         0.01  gbm_grid_learn_rate_model_2  0.9310858336529282\n",
      "2          1.0  gbm_grid_learn_rate_model_4  0.9299924113226331\n",
      "3       1.0E-4  gbm_grid_learn_rate_model_0  0.9189194702613606\n",
      "4          0.1  gbm_grid_learn_rate_model_3  0.9146600190940914\n",
      "\n",
      "0.9319711793457418\n",
      "Hyperparameters: [learn_rate]\n",
      "[0.001]\n"
     ]
    }
   ],
   "source": [
    "gbm_params = {'learn_rate': [0.0001, 0.001, 0.01, 0.1, 1]}\n",
    "\n",
    "gbm_grid = H2OGridSearch(model=H2OGradientBoostingEstimator(ntrees = 800, seed = 1234),\n",
    "                          grid_id='gbm_grid_learn_rate',\n",
    "                          hyper_params=gbm_params)\n",
    "gbm_grid.train(X, y, training_frame=training, validation_frame=validation)\n",
    "\n",
    "# модели, отсортированные по AUC\n",
    "gbm_gridperf = gbm_grid.get_grid(sort_by='auc', decreasing=True)\n",
    "print(gbm_gridperf)\n",
    "\n",
    "# выберем лучшую модель и выведем AUC на тесте \n",
    "best_gbm = gbm_gridperf.models[0]\n",
    "print(best_gbm.auc(valid=True))\n",
    "print(gbm_gridperf.get_hyperparams(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbm Grid Build progress: |████████████████████████████████████████████████| 100%\n",
      "    max_depth                   model_ids                 auc\n",
      "0           6  gbm_grid_max_depth_model_1  0.9400943280756583\n",
      "1           8  gbm_grid_max_depth_model_2  0.9301229692128176\n",
      "2          10  gbm_grid_max_depth_model_3  0.9272466156946904\n",
      "3           4  gbm_grid_max_depth_model_0  0.9173772551835563\n",
      "\n",
      "0.9400943280756583\n",
      "Hyperparameters: [max_depth]\n",
      "[6]\n"
     ]
    }
   ],
   "source": [
    "gbm_params1 = {'max_depth': [4, 6, 8, 10]}\n",
    "\n",
    "gbm_grid1 = H2OGridSearch(model=H2OGradientBoostingEstimator(ntrees = 800, seed = 1234, learn_rate=0.001),\n",
    "                          grid_id='gbm_grid_max_depth',\n",
    "                          hyper_params=gbm_params1)\n",
    "gbm_grid1.train(X, y, training_frame=training, validation_frame=validation)\n",
    "\n",
    "# модели, отсортированные по AUC\n",
    "gbm_gridperf1 = gbm_grid1.get_grid(sort_by='auc', decreasing=True)\n",
    "print(gbm_gridperf1)\n",
    "\n",
    "# выберем лучшую модель и выведем AUC на тесте \n",
    "best_gbm1 = gbm_gridperf1.models[0]\n",
    "print(best_gbm1.auc(valid=True))\n",
    "print(gbm_gridperf1.get_hyperparams(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbm Grid Build progress: |████████████████████████████████████████████████| 100%\n",
      "       histogram_type nbins               model_ids                 auc\n",
      "0          RoundRobin    16  gbm_grid_hist_model_11  0.9450596078367374\n",
      "1          RoundRobin     8   gbm_grid_hist_model_7  0.9447291331772079\n",
      "2          RoundRobin    32  gbm_grid_hist_model_15  0.9445985752870233\n",
      "3     UniformAdaptive    32  gbm_grid_hist_model_12   0.943439874011636\n",
      "4     UniformAdaptive     8   gbm_grid_hist_model_4  0.9427993243629184\n",
      "5     UniformAdaptive    16   gbm_grid_hist_model_8  0.9425912477254368\n",
      "6              Random    16   gbm_grid_hist_model_9  0.9421424549779276\n",
      "7              Random    32  gbm_grid_hist_model_13  0.9394864178994867\n",
      "8              Random     8   gbm_grid_hist_model_5  0.9377973251952249\n",
      "9     QuantilesGlobal    32  gbm_grid_hist_model_14  0.9370343775244592\n",
      "10    QuantilesGlobal    16  gbm_grid_hist_model_10  0.9347373746440257\n",
      "11    QuantilesGlobal     8   gbm_grid_hist_model_6  0.9315958254114614\n",
      "12         RoundRobin     2   gbm_grid_hist_model_3  0.9300209708611109\n",
      "13             Random     2   gbm_grid_hist_model_1  0.9293069823991644\n",
      "14    UniformAdaptive     2   gbm_grid_hist_model_0     0.9261654331666\n",
      "15    QuantilesGlobal     2   gbm_grid_hist_model_2  0.8943541872363342\n",
      "\n",
      "0.9450596078367374\n",
      "Hyperparameters: [nbins, histogram_type]\n",
      "[16, 'RoundRobin']\n"
     ]
    }
   ],
   "source": [
    "gbm_params2 = {'nbins': [2, 8, 16, 32],\n",
    "               'histogram_type': ['UniformAdaptive', 'Random', 'QuantilesGlobal', 'RoundRobin']}\n",
    "\n",
    "gbm_grid2 = H2OGridSearch(model=H2OGradientBoostingEstimator(ntrees = 800, seed = 1234, \n",
    "                                                             max_depth=6, learn_rate=0.001),\n",
    "                          grid_id='gbm_grid_hist',\n",
    "                          hyper_params=gbm_params2)\n",
    "gbm_grid2.train(X, y, training_frame=training, validation_frame=validation)\n",
    "\n",
    "# модели, отсортированные по AUC\n",
    "gbm_gridperf2 = gbm_grid2.get_grid(sort_by='auc', decreasing=True)\n",
    "print(gbm_gridperf2)\n",
    "\n",
    "# выберем лучшую модель и выведем AUC на тесте \n",
    "best_gbm2 = gbm_gridperf2.models[0]\n",
    "print(best_gbm2.auc(valid=True))\n",
    "print(gbm_gridperf2.get_hyperparams(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В итоге AUC = 0.9450 на валидационной выборке получился немного выше для GradientBoostingMachine, чем для RandomForestClassifier AUC = 0.941686"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще одним из алгоритмов, доступных в H20, из семейства Gradient Boosting Machine является XGBoost. Эта библиотека - очень популярный инструмент для решения Kaggle-задач. Он позволяет строить деервья параллельно, что позволяет решать проблемы скорости обучения. \n",
    "\n",
    "__Особенности модели__\n",
    "\n",
    "Реализация модели поддерживает особенности реализации scikit-learn и R с новыми дополнениями, такими как регуляризация. Поддерживаются три основные формы повышения градиента:\n",
    "\n",
    "* Алгоритм Gradient Boosting также называется градиентной машиной повышения, включая скорость обучения.\n",
    "* Stochastic Gradient Boosting с суб-выборкой в строке, столбце и столбце на каждый уровень разделения.\n",
    "* Регулярное усиление градиента с регуляцией L1 и L2.\n",
    "\n",
    "__Системные функции__\n",
    "\n",
    "Библиотека предоставляет систему для использования в различных вычислительных средах, не в последнюю очередь:\n",
    "\n",
    "* Параллелизация построения дерева с использованием всех ваших ядер процессора во время обучения.\n",
    "* Распределенные вычисления для обучения очень крупных моделей с использованием кластера машин.\n",
    "* Внекорпоративные вычисления для очень больших наборов данных, которые не вписываются в память.\n",
    "* Кэш Оптимизация структуры данных и алгоритма для наилучшего использования аппаратного обеспечения.\n",
    "\n",
    "__Особенности алгоритма__\n",
    "\n",
    "Реализация алгоритма была разработана для эффективности вычислительных ресурсов времени и памяти. Цель проекта заключалась в том, чтобы наилучшим образом использовать имеющиеся ресурсы для обучения модели. Некоторые ключевые функции реализации алгоритма включают:\n",
    "\n",
    "* Редкая реализация Aware с автоматической обработкой отсутствующих значений данных.\n",
    "* Блочная структура для поддержки распараллеливания конструкции дерева.\n",
    "* Продолжение обучения, чтобы вы могли еще больше повысить уже установленную модель для новых данных. \n",
    "\n",
    "В реальности в H2O используется нативный алгоритм [XGBoost](http://xgboost.readthedocs.io/en/latest/get_started/index.html), поэтому не буду пытатьтся подобрать параметры, покажу только, как запускается"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from h2o.estimators import H2OXGBoostEstimator\n",
    "# библиотека доступна не для всех платформ, сначала надо проверить ее доступность\n",
    "is_xgboost_available = H2OXGBoostEstimator.available()\n",
    "print(is_xgboost_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost Model Build progress: |███████████████████████████████████████████| 100%\n",
      "\n",
      "ModelMetricsBinomial: xgboost\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.04233021547018061\n",
      "RMSE: 0.2057430812206831\n",
      "LogLoss: 0.1893648656423194\n",
      "Mean Per-Class Error: 0.0834117942209498\n",
      "AUC: 0.9795984475074526\n",
      "Gini: 0.9591968950149052\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.38141971826553345: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>1960.0</td>\n",
       "<td>33.0</td>\n",
       "<td>0.0166</td>\n",
       "<td> (33.0/1993.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>65.0</td>\n",
       "<td>275.0</td>\n",
       "<td>0.1912</td>\n",
       "<td> (65.0/340.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>2025.0</td>\n",
       "<td>308.0</td>\n",
       "<td>0.042</td>\n",
       "<td> (98.0/2333.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0     1    Error    Rate\n",
       "-----  ----  ---  -------  -------------\n",
       "0      1960  33   0.0166   (33.0/1993.0)\n",
       "1      65    275  0.1912   (65.0/340.0)\n",
       "Total  2025  308  0.042    (98.0/2333.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.3814197</td>\n",
       "<td>0.8487654</td>\n",
       "<td>163.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1430533</td>\n",
       "<td>0.8405723</td>\n",
       "<td>274.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.6040332</td>\n",
       "<td>0.9194529</td>\n",
       "<td>118.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4644568</td>\n",
       "<td>0.9597085</td>\n",
       "<td>143.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9170414</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.1096087</td>\n",
       "<td>1.0</td>\n",
       "<td>316.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9170414</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.4626352</td>\n",
       "<td>0.8304986</td>\n",
       "<td>144.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1715132</td>\n",
       "<td>0.8966382</td>\n",
       "<td>251.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1430533</td>\n",
       "<td>0.9165882</td>\n",
       "<td>274.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.38142      0.848765  163\n",
       "max f2                       0.143053     0.840572  274\n",
       "max f0point5                 0.604033     0.919453  118\n",
       "max accuracy                 0.464457     0.959709  143\n",
       "max precision                0.917041     1         0\n",
       "max recall                   0.109609     1         316\n",
       "max specificity              0.917041     1         0\n",
       "max absolute_mcc             0.462635     0.830499  144\n",
       "max min_per_class_accuracy   0.171513     0.896638  251\n",
       "max mean_per_class_accuracy  0.143053     0.916588  274"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 14,57 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0102872</td>\n",
       "<td>0.8952582</td>\n",
       "<td>6.8617647</td>\n",
       "<td>6.8617647</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0705882</td>\n",
       "<td>0.0705882</td>\n",
       "<td>586.1764706</td>\n",
       "<td>586.1764706</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0201457</td>\n",
       "<td>0.8631429</td>\n",
       "<td>6.8617647</td>\n",
       "<td>6.8617647</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0676471</td>\n",
       "<td>0.1382353</td>\n",
       "<td>586.1764706</td>\n",
       "<td>586.1764706</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300043</td>\n",
       "<td>0.8510975</td>\n",
       "<td>6.8617647</td>\n",
       "<td>6.8617647</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0676471</td>\n",
       "<td>0.2058824</td>\n",
       "<td>586.1764706</td>\n",
       "<td>586.1764706</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0402915</td>\n",
       "<td>0.8373678</td>\n",
       "<td>6.8617647</td>\n",
       "<td>6.8617647</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0705882</td>\n",
       "<td>0.2764706</td>\n",
       "<td>586.1764706</td>\n",
       "<td>586.1764706</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0501500</td>\n",
       "<td>0.8046734</td>\n",
       "<td>6.8617647</td>\n",
       "<td>6.8617647</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0676471</td>\n",
       "<td>0.3441176</td>\n",
       "<td>586.1764706</td>\n",
       "<td>586.1764706</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1003000</td>\n",
       "<td>0.6370200</td>\n",
       "<td>6.8031171</td>\n",
       "<td>6.8324409</td>\n",
       "<td>0.9914530</td>\n",
       "<td>0.9957265</td>\n",
       "<td>0.3411765</td>\n",
       "<td>0.6852941</td>\n",
       "<td>580.3117144</td>\n",
       "<td>583.2440925</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500214</td>\n",
       "<td>0.2845466</td>\n",
       "<td>3.0759635</td>\n",
       "<td>5.5874370</td>\n",
       "<td>0.4482759</td>\n",
       "<td>0.8142857</td>\n",
       "<td>0.1529412</td>\n",
       "<td>0.8382353</td>\n",
       "<td>207.5963489</td>\n",
       "<td>458.7436975</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2001715</td>\n",
       "<td>0.1906352</td>\n",
       "<td>0.5864756</td>\n",
       "<td>4.3345195</td>\n",
       "<td>0.0854701</td>\n",
       "<td>0.6316916</td>\n",
       "<td>0.0294118</td>\n",
       "<td>0.8676471</td>\n",
       "<td>-41.3524384</td>\n",
       "<td>333.4519461</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000429</td>\n",
       "<td>0.1235101</td>\n",
       "<td>1.1779854</td>\n",
       "<td>3.2838445</td>\n",
       "<td>0.1716738</td>\n",
       "<td>0.4785714</td>\n",
       "<td>0.1176471</td>\n",
       "<td>0.9852941</td>\n",
       "<td>17.7985357</td>\n",
       "<td>228.3844538</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.3999143</td>\n",
       "<td>0.1064968</td>\n",
       "<td>0.1472482</td>\n",
       "<td>2.5005359</td>\n",
       "<td>0.0214592</td>\n",
       "<td>0.3644159</td>\n",
       "<td>0.0147059</td>\n",
       "<td>1.0</td>\n",
       "<td>-85.2751830</td>\n",
       "<td>150.0535906</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5002143</td>\n",
       "<td>0.0975546</td>\n",
       "<td>0.0</td>\n",
       "<td>1.9991431</td>\n",
       "<td>0.0</td>\n",
       "<td>0.2913453</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>99.9143102</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6000857</td>\n",
       "<td>0.0918106</td>\n",
       "<td>0.0</td>\n",
       "<td>1.6664286</td>\n",
       "<td>0.0</td>\n",
       "<td>0.2428571</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>66.6428571</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999571</td>\n",
       "<td>0.0864580</td>\n",
       "<td>0.0</td>\n",
       "<td>1.4286589</td>\n",
       "<td>0.0</td>\n",
       "<td>0.2082058</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>42.8658910</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7998285</td>\n",
       "<td>0.0819798</td>\n",
       "<td>0.0</td>\n",
       "<td>1.2502680</td>\n",
       "<td>0.0</td>\n",
       "<td>0.1822079</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>25.0267953</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8997000</td>\n",
       "<td>0.0772951</td>\n",
       "<td>0.0</td>\n",
       "<td>1.1114817</td>\n",
       "<td>0.0</td>\n",
       "<td>0.1619819</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>11.1481658</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0684245</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.1457351</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0102872                   0.895258           6.86176   6.86176            1                1                           0.0705882       0.0705882                  586.176   586.176\n",
       "    2        0.0201457                   0.863143           6.86176   6.86176            1                1                           0.0676471       0.138235                   586.176   586.176\n",
       "    3        0.0300043                   0.851098           6.86176   6.86176            1                1                           0.0676471       0.205882                   586.176   586.176\n",
       "    4        0.0402915                   0.837368           6.86176   6.86176            1                1                           0.0705882       0.276471                   586.176   586.176\n",
       "    5        0.05015                     0.804673           6.86176   6.86176            1                1                           0.0676471       0.344118                   586.176   586.176\n",
       "    6        0.1003                      0.63702            6.80312   6.83244            0.991453         0.995726                    0.341176        0.685294                   580.312   583.244\n",
       "    7        0.150021                    0.284547           3.07596   5.58744            0.448276         0.814286                    0.152941        0.838235                   207.596   458.744\n",
       "    8        0.200171                    0.190635           0.586476  4.33452            0.0854701        0.631692                    0.0294118       0.867647                   -41.3524  333.452\n",
       "    9        0.300043                    0.12351            1.17799   3.28384            0.171674         0.478571                    0.117647        0.985294                   17.7985   228.384\n",
       "    10       0.399914                    0.106497           0.147248  2.50054            0.0214592        0.364416                    0.0147059       1                          -85.2752  150.054\n",
       "    11       0.500214                    0.0975546          0         1.99914            0                0.291345                    0               1                          -100      99.9143\n",
       "    12       0.600086                    0.0918106          0         1.66643            0                0.242857                    0               1                          -100      66.6429\n",
       "    13       0.699957                    0.086458           0         1.42866            0                0.208206                    0               1                          -100      42.8659\n",
       "    14       0.799829                    0.0819798          0         1.25027            0                0.182208                    0               1                          -100      25.0268\n",
       "    15       0.8997                      0.0772951          0         1.11148            0                0.161982                    0               1                          -100      11.1482\n",
       "    16       1                           0.0684245          0         1                  0                0.145735                    0               1                          -100      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AUC на валидационной выборке GradientBoostingMachine:  0.9146600190940914\n",
      "AUC на валидационной выборке XGBoost:  0.9335786733686384\n"
     ]
    }
   ],
   "source": [
    "param = {\n",
    "      \"ntrees\" : 100\n",
    "    , \"max_depth\" : 10\n",
    "    , \"learn_rate\" : 0.02\n",
    "    , \"sample_rate\" : 0.7\n",
    "    , \"col_sample_rate_per_tree\" : 0.9\n",
    "    , \"min_rows\" : 5\n",
    "    , \"seed\": 4241\n",
    "    , \"score_tree_interval\": 100\n",
    "}\n",
    "\n",
    "model = H2OXGBoostEstimator(**param)\n",
    "model.train(X, y, training_frame=training, validation_frame=validation)\n",
    "model.model_performance().show()\n",
    "\n",
    "print('AUC на валидационной выборке GradientBoostingMachine: ',gbm.auc(valid=True))\n",
    "print('AUC на валидационной выборке XGBoost: ',model.auc(valid=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LightGBM](https://github.com/Microsoft/LightGBM) строит глубокие асимметричные деревья, повторно разбивая один лист вместо разбиения всех листьев одного уровня до достижения максимальной глубины. H2O не интегрирован с LightGBM, но предоставляет метод для эмуляции LightGBM алгоритма, используя определенный набор параметров:\n",
    "\n",
    "tree_method=\"hist\"\n",
    "grow_policy=\"lossguide\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H2O session _sid_addf closed.\n"
     ]
    }
   ],
   "source": [
    "h2o.cluster().shutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Список литературы \n",
    "\n",
    "https://www.h2o.ai/  \n",
    "http://statistica.ru/local-portals/actuaries/obobshchennye-lineynye-modeli-glm/  \n",
    "https://en.wikipedia.org/wiki/Generalized_linear_model  \n",
    "https://alexanderdyakonov.files.wordpress.com/2017/06/book_boosting_pdf.pdf  \n",
    "https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/tutorials/gbm/gbmTuning.ipynb  \n",
    "https://ru.bmstu.wiki/XGBoost  \n",
    "Артем Груздев. Прогнозное моделирование в IBM SPSS Statistics, R и Python    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
