{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Math, Latex\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в теорию или как связаны подгузники и пиво?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Обучение на ассоциативных правилах (далее Associations rules learning - ARL) представляет из себя с одной стороны, простой, с другой - довольно часто применимый в реальной жизни метод поиска взаимосвязей (ассоциаций) в датасетах или, если точнее, айтемсетах (itemsests). Именно ARL лежит в основе многих рекомендательных систем, работающих в интернет-магазинах. \n",
    "   Впервые подробно об этом заговорил Piatesky-Shapiro G [1] в работе “Discovery, Analysis, and Presentation of Strong Rules.” (1991) Более подробо тему развивали Agrawal R, Imielinski T, Swami A в работах “Mining Association Rules between Sets of Items in Large Databases” (1993) [2] и “Fast Algorithms for Mining Association Rules.” (1994) [3].\n",
    "\n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "  В общем виде ARL можно опсиать как \"Who bought x also bought y\" (\"Кто купил x, также купил y\"). В основе лежит анализ транзакций (transactions), внутри каждой из которых лежит свой уникальный itemset из набора items. При помощи ARL алогритмов нахоядтся те самые \"правила\" совпадения items внутри одной транзакции, которые потом сортируются по их силе. \n",
    "\n",
    "   Да, вот все так просто и логично."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   За этой простотой, однако, могут скрываться поразительные вещи, о которых common sense даже не подозревал:) \n",
    "\n",
    "   Классический случай такого когнитивного диссонанса описан в статье D.J. Power \"Ask Dan!\", опубликованной в DSSResources.com [4]: \n",
    "\n",
    "   В 1992 году, группа по консалтингу в области ритейла компании Teradata под руководством Томаса Блишока провела исследование 1.2 миллиона транзакций в 25 магазинах для ритейлера Osco Drug (нет, там продавали не наркотики и даже не лекарства, точнее, не только лекартсва. Drug Store в стране веротяного противника - формат разнокалиберных магазинов у дома). \n",
    "   После анализа всех этих транзакций самым сильным правилом получилось \"Между 17:00 и 19:00 чаще всего пиво и подгузники покупают вместе\". \n",
    "   К сожалению такое правило показалось руководству Osco Drug настолкьо контринтуитивным, что ставить подгузники на полках рядом с пивом они не стали:) Хотя объяснение паре пиво-подгузники вполне себе нашлось: когда оба члена молодой семьи возвращались с работы домой (как раз часам к 5 вечера), жены обычно отправляли мужей за подгузниками в ближайшей магазин. И мужья, не долго думая, совмещали приятное с полезным - покупали подгузники по заданию жены и пиво для собственного вечернего времяпрепровождения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Описание Association rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Итак мы выяснили, что пиво и подгузники - хороший джентельменский набор, а что дальше? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть у нас имеется некий датасет (или коллекция) D, такой, что D = {d_0 ... d_j}, где d - уникальная транзакция-itemset (например, кассовый чек).\n",
    "Внутри каждой d представлен набор items (i - item), причем в идеальном случае он представлен в бинарном виде: d1 = [{Пиво: 1}, {Вода: 0}, {Кола: 1}, {...}], d2 = [{Пиво : 0}, {Вода: 1}, {Кола : 1}, {...}]. Принято каждый itemset описывать через количество ненулевых значений (k-itemset), например, [{Пиво: 1}, {Вода: 0}, {Кола: 1}] являестя 2-itemset.\n",
    "\n",
    "Если в изначальном виде не представлен, можно при желании руками его преобразовать (OHE и pd.get_dummies вам в помощь).\n",
    "\n",
    "Таким образом, датасет представляет собой разреженную матрицу со значениями {1,0}. Это будет бинарный датасет. Существуют и другие виды записи - вертикальный датасет (показывает для каждого отдельного item вектор транзакций, где он присутствует) и транзакционный датасет (примерно как в кассовом чеке)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ОК, данные преобразовали, как найти правила?\n",
    "\n",
    "Существует целый ряд ключевых (если хотите - базовых) понятий в ARL, которые нам помогут эти правила вывести."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Support (поддержка)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первое понятие  в ARL - support:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$supp(X) = \\frac{\\{t\\in T;\\ X \\in t\\}}{|T|}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ", где X - itemset, содержащий в себе i-items, а T - количество транзакций. Т.е. в общем виде это показатель \"частотности\" данного itemset во всех анализируемых транзакциях. Но это касается только X. Нам же интересен скорее вариант, когда у нас в одном itemset встречаются x1 и x2 (например).\n",
    "Ну тут тоже все просто. Пусть x1 = {Пиво}, а x2 = {Подгузники}, значит нам нужно посчитать, во скольких транзакциях втречается эта парочка. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$supp(x_1\\cup x_2) = \\frac{\\sigma(x_1 \\cup x_2)}{|T|}$, где $\\sigma $ - количество транзакций, содержащих $x_1$ и $x_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confidence (достоверность)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующее ключевое понятие - confidence. Это показатель того, как часто наше правило срабатывает для всего датасета. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$conf(x_1\\cup x_2) = \\frac{supp(x_1 \\cup x_2)}{supp(x_1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приведем пример: мы хотим посчитать confidence для правила \"кто покупает пиво, тот покупает и подгузники\".\n",
    "\n",
    "Для этого сначала посчитаем, какой support у правила \"покупает пиво\", потом посчитаем support у правила \"покупает пиво и подгузники\", и просто поделим одно на другое. Т.е. мы посчитаем в скольких случаях (транзакциях) срабатывает правило \"купил пиво $supp(X)$, купил подгузники и пиво $supp(X \\cup Y)$\"\n",
    "Ничего не напоминает? Байес смотрит на все это несколько недоуменно и с презрением:)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/thomas-bayes.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Lift (поддержка)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующее понятие в нашем списке - lift. Грубо говоря, lift - это отношение \"зависимости\" items к их \"независимости\". Lift показывает, насколько items зависят друг от друга. Это очевидно из формулы:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$lift(x_1\\cup x_2) = \\frac{supp(x_1 \\cup x_2)}{supp(x_1) \\times supp(x_2)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, мы хотим понять зависимость покупки пива и покупки подгузников. Для этого считаем support правила \"купил пиво и подгузники\" и делим его на произведение правил \"купил пиво\" и \"купил подгузники\". В случае, если lift = 1, мы говорим, что items независимы и правил совместной покупки тут нет. Если же lift > 1, то величина, на которую lift, собственно, больше этой самой единицы, и покажет нам \"силу\" правила. Чем больше единицы, тем круче."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conviction (убедительность)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В общем виде Conviction - это \"частотность ошибок\" нашего правила. Т.е., например, как часто покупали пиво без подгузников и наоборот."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$conv(x_1\\cup x_2) = \\frac{1 - supp(x_2)}{1 - conf(x_1 \\cup x_2)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чем результат по формуле выше ближе к 1, тем лучше. Например, если conviction покупки пива и подгузников вместе равен 1.2, это значит, что правило \"купил пиво и подгузники\" было бы в 1.2 раза (на 20%) более верным, чем если бы совпадение этих items в одной транзакции было бы чисто случайным. Немного неинтуитивное понятие, но оно и используестя н етак часто, как предыдущие три. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует ряд часто используемых классических алгоритмов, позволяющих находить правила в itemsets согласно перечисленным выше понятиям - Наивный или брутфорс-алгоритм, Apriori- алгоритм, ECLAT-алгоритм, FP-growth алгоритм и другие."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Брутфорс"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найти правила в itemsets в общем не сложно, сложно сделать это эффективно. Брутфорс-алгоритм самый простой и, в то же время, самый неэффективный способ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В псевдо-коде алгоритм выглядит так:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ВХОД**: Датасет $D$, содержащий список транзакций"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ВЫХОД**: Наборы itemsets $F_1, F_2, F_3, ... F_q$, где $F_i$ - набор itemsets размера $I$, которые встречаются как минимум $s$ раз в $D$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ПОДХОД:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $R \\leftarrow$  целочисленный array, содержащий в себе все комбинации items в $D$, размера $2^{|D|}$\n",
    "2. **for** $n \\leftarrow  [1, |D|]$ **do**:\n",
    "<br>\n",
    "$F \\leftarrow$  все возможные комбинации из $D_n$\n",
    "<br>\n",
    "Увеличить каждое значение в $R$ согласно значениям в каждом $F[]$\n",
    "5. **return** Все itemsets в $R \\geq s$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сложность брутфорс-алгоритма очевидна:\n",
    "Для того чтобы найти все возможные Association rules применяя брутфорс-алгоритм нам необходимо перечислить все подмножества $X$ из набора $I$ и для каждого подмножества $X$ рассчитать $supp(X)$. Данный подход будет состоять из следующих шагов:\n",
    "-  генерация **кандидатов**. Данный шаг состоит из перебора всех возможных подмножеств $X$ подмножества $I$. Данные подмножества называются **кандидатами**, поскольку каждое подмножество теоретически может являться часто встречаемым подмножеством, то множество потенциальных кандидатов будет состоять из $2^{|D|}$ элементов (здесь $|D|$ обозначается число элементов множества $I$, а $2^{|D|}$ часто называется булеаном множества $I$, то есть множество всех подмножеств $I$)\n",
    "-  расчет **support**. На данном шаге рассчитывается $supp(X)$ каждого кандидата $X$\n",
    "\n",
    "Таким образом, сложность нашего алгоритма будет: $O(|I|*|D|*2^{|I|})$, где\n",
    "<br>$O(2^{|I|})$ - количество возможных кандидатов</br>\n",
    "<br>$O(|I|*|D|)$ - сложность расчета $supp(X)$. Поскольку для расчета  $supp(X)$ нам необходимо перебрать все элементы в $I$ в каждой транзакции $t \\in T$</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нотации O-большое нам понадобится $O(N)$ операций, где $N$ - количество itemsets.\n",
    "Таким образом, для применения брутфорс-алгоритма нам понадобится $2^i$ ячеек памяти, где $i$ - индивидуальный itemset. Т.е. для хранения и обсчета 34 items нам понадобится 128GB RAM (для 64-битных целосчисленных ячеек). \n",
    "<br>Таким образом брутфорс поможет нам в обсчете транзакций в палатке с шаурмой у вокзала, но для чего-то более серьезного он совершенно не пригоден. Реализацию в Питоне и R делать нет смысла."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apriori Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Теория"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Используемые понятия:\n",
    "<br> -  Множество объектов (**itemset**): $X \\subseteq I = \\{x_1, x_2, ..., x_n\\}$</br>\n",
    "<br> -  Множество идентификаторов транзакций (**tidset**): $T = \\{t_1, t_2, ..., t_m\\}$</br>\n",
    "<br> -  Множество транзакций (**transactions**): $\\{(t,\\ X):\\ t\\in T,\\ X \\in I\\}$</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Введем дополнительно еще несколько понятий.\n",
    "<br>Будем рассматривать дерево префиксов (prefix tree), где 2 элемента $X$ и $Y$ соединены, если $X$ является прямым подмножеством $Y$. Таким образом мы можем пронумеровать все подмножества множества $I$. Рисунок приведен ниже</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/prefix-based search.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, рассмотрим Apriori алгоритм.\n",
    "<br>\n",
    "Apriori алгоритм использует следующее утверждение: если $X \\subseteq Y$, то $supp(X) \\geq supp(Y)$. Откуда следуют следующие 2 свойства:\n",
    " -  если $Y$ встречается часто, то любое подмножество $X: X \\subseteq Y$  так же встречается часто\n",
    " -  если $X$ встречается редко, то любое супермножество $Y: Y \\supseteq X$ так же встречается редко\n",
    " </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apriori алгоритм по-уровнево проходит по префиксному дереву и рассчитывает частоту встречаемости подмножеств $X$ в $D$. Таким образом, в соответствии с алгоритмом:\n",
    " -  исключаются редкие подмножества и все их супермножества\n",
    " -  рассчитывается $supp(X)$ для кадого подходящего кандидата $X$ размера $k$ на уровне $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/Intersections.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В псевдо-код нотации Априори-алгоритм выглядит следующим образом:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ВХОД**: Датасет $D$, содержащий список транзакций, и $\\sigma$ - задаваемый пользователем порог $supp$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ВЫХОД**: Список itemsets $F(D, \\sigma)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ПОДХОД**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $C_1$ $\\leftarrow$ $[{i}|i \\in J]$\n",
    "2. $k \\leftarrow 1$\n",
    "3. **while** $C_k \\neq 1$ **do**:\n",
    "4. #Считаем все support для всех кандидатов\n",
    "<br>\n",
    "**for all** транзакций (tid, I) $\\in D$ **do**:\n",
    "<br>\n",
    "    **for all** кандидатов $X \\in C_k$ **do**:\n",
    "<br>\n",
    "**if** $X \\subseteq I$:\n",
    "<br>\n",
    "$X.support++$\n",
    "5. #Вытаскиваем все частые itemsets:\n",
    "<br>\n",
    "$F_k = $ {X|X.support > $\\sigma$}\n",
    "6. #Генерируем новых кандидатов\n",
    "<br>\n",
    "**for all** $X,Y \\in F_i, X[i] = Y[i]$ **for** $1 \\leq i \\leq k-1$ **and** $X[k] \\leq Y[k]$ **do**:\n",
    "<br>\n",
    "$I = X \\cup$ {$Y$|$k$|}\n",
    "<br>\n",
    "**if** $\\forall J \\subset I,|J| = k: J \\in F_k$ **then**\n",
    "<br>\n",
    "$C_k+1$ $\\leftarrow C_k+1 \\cup I$\n",
    "<br>\n",
    "$k++$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, Apriori-алгоритм сначала ищет все единичные (содержащие 1 элемент) itemsets, удовлетворяющие заданному пользователем $supp$, затем составляет из них пары по принципу иерархической монотонности, т.е. если $x_1$ встречается часто и $x_2$ встречается часто, то и $[x_1, x_2]$ встречается часто."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Явным минусом такого подхода является то, что необходимо \"просканировать\" весь датасет, посчитать все $supp$ на всех уровнях breadth-first search (поиск в ширину - подробнее [тут](https://ru.wikipedia.org/wiki/%D0%9F%D0%BE%D0%B8%D1%81%D0%BA_%D0%B2_%D1%88%D0%B8%D1%80%D0%B8%D0%BD%D1%83))\n",
    "Это также может подъесть RAM на больших датасетах, хотя и намного эффективнее брутфорса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Реализация в Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from sklearn import... эммм... а импортировать-то и нечего:) На данный момент модулей для ALR в sklearn нет. Ну ничего, погуглим или напишем свои, правда?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "По сети гуляет целый ряд реализаций, например [вот](https://github.com/asaini/Apriori), [вот](http://adataanalyst.com/machine-learning/apriori-algorithm-python-3-0/), и даже [вот](https://codereview.stackexchange.com/questions/38101/apriori-algorithm-using-pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы же на практике придерживаемся алгоритма apyori, написанного Ю Мочиузки (Yu Mochizuki). Полный код приводить не будем, желающие могут посмотреть [тут](https://github.com/ymoch/apyori) , а вот архитектуру решения и пример использования покажем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Условно решение Мочизуки можно разделить на 4 части: Структура данных, Внутренние функции, API и Прикладные функции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первая часть модуля (Структура данных) работает с изначальным датасетом. Реализуется класс TransactionManager, методы которого объединяют транзакции в матрицу, формируют список правил-кандидатов и считают support для каждого правила. Внутренние функции дополнительно по support'у формируют списки правил и соответственно их ранжируют. API логично позволяет работать напрямую с датасетами, а Прикладные функции позволяют обрабатывать транзакции и выводить результат в читаемый вид. Никакого rocketscience.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как использовать модуль на реальном (ну, в данном случае - игрушечном) датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# подгрузим модули\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# загрузим данные\n",
    "dataset = pd.read_csv('data/Market_Basket_Optimisation.csv', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shrimp</td>\n",
       "      <td>almonds</td>\n",
       "      <td>avocado</td>\n",
       "      <td>vegetables mix</td>\n",
       "      <td>green grapes</td>\n",
       "      <td>whole weat flour</td>\n",
       "      <td>yams</td>\n",
       "      <td>cottage cheese</td>\n",
       "      <td>energy drink</td>\n",
       "      <td>tomato juice</td>\n",
       "      <td>low fat yogurt</td>\n",
       "      <td>green tea</td>\n",
       "      <td>honey</td>\n",
       "      <td>salad</td>\n",
       "      <td>mineral water</td>\n",
       "      <td>salmon</td>\n",
       "      <td>antioxydant juice</td>\n",
       "      <td>frozen smoothie</td>\n",
       "      <td>spinach</td>\n",
       "      <td>olive oil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>burgers</td>\n",
       "      <td>meatballs</td>\n",
       "      <td>eggs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chutney</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>turkey</td>\n",
       "      <td>avocado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mineral water</td>\n",
       "      <td>milk</td>\n",
       "      <td>energy bar</td>\n",
       "      <td>whole wheat rice</td>\n",
       "      <td>green tea</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0          1           2                 3             4  \\\n",
       "0  shrimp         almonds    avocado     vegetables mix    green grapes   \n",
       "1  burgers        meatballs  eggs        NaN               NaN            \n",
       "2  chutney        NaN        NaN         NaN               NaN            \n",
       "3  turkey         avocado    NaN         NaN               NaN            \n",
       "4  mineral water  milk       energy bar  whole wheat rice  green tea      \n",
       "\n",
       "                  5     6               7             8             9  \\\n",
       "0  whole weat flour  yams  cottage cheese  energy drink  tomato juice   \n",
       "1  NaN               NaN   NaN             NaN           NaN            \n",
       "2  NaN               NaN   NaN             NaN           NaN            \n",
       "3  NaN               NaN   NaN             NaN           NaN            \n",
       "4  NaN               NaN   NaN             NaN           NaN            \n",
       "\n",
       "               10         11     12     13             14      15  \\\n",
       "0  low fat yogurt  green tea  honey  salad  mineral water  salmon   \n",
       "1  NaN             NaN        NaN    NaN    NaN            NaN      \n",
       "2  NaN             NaN        NaN    NaN    NaN            NaN      \n",
       "3  NaN             NaN        NaN    NaN    NaN            NaN      \n",
       "4  NaN             NaN        NaN    NaN    NaN            NaN      \n",
       "\n",
       "                  16               17       18         19  \n",
       "0  antioxydant juice  frozen smoothie  spinach  olive oil  \n",
       "1  NaN                NaN              NaN      NaN        \n",
       "2  NaN                NaN              NaN      NaN        \n",
       "3  NaN                NaN              NaN      NaN        \n",
       "4  NaN                NaN              NaN      NaN        "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# посомтрим на датасет\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.fillna(method = 'ffill',axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что датасет у нас представляет разреженную матрицу, где в строках у нас набор items в каждой транзакции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shrimp</td>\n",
       "      <td>almonds</td>\n",
       "      <td>avocado</td>\n",
       "      <td>vegetables mix</td>\n",
       "      <td>green grapes</td>\n",
       "      <td>whole weat flour</td>\n",
       "      <td>yams</td>\n",
       "      <td>cottage cheese</td>\n",
       "      <td>energy drink</td>\n",
       "      <td>tomato juice</td>\n",
       "      <td>low fat yogurt</td>\n",
       "      <td>green tea</td>\n",
       "      <td>honey</td>\n",
       "      <td>salad</td>\n",
       "      <td>mineral water</td>\n",
       "      <td>salmon</td>\n",
       "      <td>antioxydant juice</td>\n",
       "      <td>frozen smoothie</td>\n",
       "      <td>spinach</td>\n",
       "      <td>olive oil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>burgers</td>\n",
       "      <td>meatballs</td>\n",
       "      <td>eggs</td>\n",
       "      <td>eggs</td>\n",
       "      <td>eggs</td>\n",
       "      <td>eggs</td>\n",
       "      <td>eggs</td>\n",
       "      <td>eggs</td>\n",
       "      <td>eggs</td>\n",
       "      <td>eggs</td>\n",
       "      <td>eggs</td>\n",
       "      <td>eggs</td>\n",
       "      <td>eggs</td>\n",
       "      <td>eggs</td>\n",
       "      <td>eggs</td>\n",
       "      <td>eggs</td>\n",
       "      <td>eggs</td>\n",
       "      <td>eggs</td>\n",
       "      <td>eggs</td>\n",
       "      <td>eggs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chutney</td>\n",
       "      <td>chutney</td>\n",
       "      <td>chutney</td>\n",
       "      <td>chutney</td>\n",
       "      <td>chutney</td>\n",
       "      <td>chutney</td>\n",
       "      <td>chutney</td>\n",
       "      <td>chutney</td>\n",
       "      <td>chutney</td>\n",
       "      <td>chutney</td>\n",
       "      <td>chutney</td>\n",
       "      <td>chutney</td>\n",
       "      <td>chutney</td>\n",
       "      <td>chutney</td>\n",
       "      <td>chutney</td>\n",
       "      <td>chutney</td>\n",
       "      <td>chutney</td>\n",
       "      <td>chutney</td>\n",
       "      <td>chutney</td>\n",
       "      <td>chutney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>turkey</td>\n",
       "      <td>avocado</td>\n",
       "      <td>avocado</td>\n",
       "      <td>avocado</td>\n",
       "      <td>avocado</td>\n",
       "      <td>avocado</td>\n",
       "      <td>avocado</td>\n",
       "      <td>avocado</td>\n",
       "      <td>avocado</td>\n",
       "      <td>avocado</td>\n",
       "      <td>avocado</td>\n",
       "      <td>avocado</td>\n",
       "      <td>avocado</td>\n",
       "      <td>avocado</td>\n",
       "      <td>avocado</td>\n",
       "      <td>avocado</td>\n",
       "      <td>avocado</td>\n",
       "      <td>avocado</td>\n",
       "      <td>avocado</td>\n",
       "      <td>avocado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mineral water</td>\n",
       "      <td>milk</td>\n",
       "      <td>energy bar</td>\n",
       "      <td>whole wheat rice</td>\n",
       "      <td>green tea</td>\n",
       "      <td>green tea</td>\n",
       "      <td>green tea</td>\n",
       "      <td>green tea</td>\n",
       "      <td>green tea</td>\n",
       "      <td>green tea</td>\n",
       "      <td>green tea</td>\n",
       "      <td>green tea</td>\n",
       "      <td>green tea</td>\n",
       "      <td>green tea</td>\n",
       "      <td>green tea</td>\n",
       "      <td>green tea</td>\n",
       "      <td>green tea</td>\n",
       "      <td>green tea</td>\n",
       "      <td>green tea</td>\n",
       "      <td>green tea</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0          1           2                 3             4  \\\n",
       "0  shrimp         almonds    avocado     vegetables mix    green grapes   \n",
       "1  burgers        meatballs  eggs        eggs              eggs           \n",
       "2  chutney        chutney    chutney     chutney           chutney        \n",
       "3  turkey         avocado    avocado     avocado           avocado        \n",
       "4  mineral water  milk       energy bar  whole wheat rice  green tea      \n",
       "\n",
       "                  5          6               7             8             9  \\\n",
       "0  whole weat flour  yams       cottage cheese  energy drink  tomato juice   \n",
       "1  eggs              eggs       eggs            eggs          eggs           \n",
       "2  chutney           chutney    chutney         chutney       chutney        \n",
       "3  avocado           avocado    avocado         avocado       avocado        \n",
       "4  green tea         green tea  green tea       green tea     green tea      \n",
       "\n",
       "               10         11         12         13             14         15  \\\n",
       "0  low fat yogurt  green tea  honey      salad      mineral water  salmon      \n",
       "1  eggs            eggs       eggs       eggs       eggs           eggs        \n",
       "2  chutney         chutney    chutney    chutney    chutney        chutney     \n",
       "3  avocado         avocado    avocado    avocado    avocado        avocado     \n",
       "4  green tea       green tea  green tea  green tea  green tea      green tea   \n",
       "\n",
       "                  16               17         18         19  \n",
       "0  antioxydant juice  frozen smoothie  spinach    olive oil  \n",
       "1  eggs               eggs             eggs       eggs       \n",
       "2  chutney            chutney          chutney    chutney    \n",
       "3  avocado            avocado          avocado    avocado    \n",
       "4  green tea          green tea        green tea  green tea  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#создаим из них матрицу\n",
    "transactions = []\n",
    "for i in range(0, 7501): \n",
    "    transactions.append([str(dataset.values[i,j]) for j in range(0, 20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#загружаем apriori\n",
    "import apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 664 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# и обучимся правилам. Обратите внимание, что пороговые значения мы вибираем сами в зависимости от того, /\n",
    "# насколкьо \"сильные\" правила мы хотим получить\n",
    "# min_support -- минимальный support для правил (dtype = float).\n",
    "# min_confidence -- минимальное значение confidence для правил (dtype = float)\n",
    "# min_lift -- минимальный lift (dtype = float)\n",
    "# max_length -- максимальная длина itemset (вспоминаем про k-itemset)  (dtype = integer)\n",
    "\n",
    "result = list(apriori.apriori(transactions, min_support = 0.003, min_confidence = 0.2, min_lift = 4, min_length = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Визуализируем выход"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shutil, os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from StringIO import StringIO\n",
    "except ImportError:\n",
    "    from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json #преобразовывать будем в json, используя встроенные в модуль методы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = []\n",
    "for RelationRecord in result:\n",
    "    o = StringIO()\n",
    "    apyori.dump_as_json(RelationRecord, o)\n",
    "    output.append(json.loads(o.getvalue()))\n",
    "data_df = pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>items</th>\n",
       "      <th>ordered_statistics</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[chicken, light cream]</td>\n",
       "      <td>[{'items_base': ['light cream'], 'items_add': ['chicken'], 'confidence': 0.29059829059829057, 'lift': 4.84395061728395}]</td>\n",
       "      <td>0.004533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[escalope, pasta]</td>\n",
       "      <td>[{'items_base': ['pasta'], 'items_add': ['escalope'], 'confidence': 0.3728813559322034, 'lift': 4.700811850163794}]</td>\n",
       "      <td>0.005866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[fromage blanc, honey]</td>\n",
       "      <td>[{'items_base': ['fromage blanc'], 'items_add': ['honey'], 'confidence': 0.2450980392156863, 'lift': 5.164270764485569}]</td>\n",
       "      <td>0.003333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[olive oil, whole wheat pasta]</td>\n",
       "      <td>[{'items_base': ['whole wheat pasta'], 'items_add': ['olive oil'], 'confidence': 0.2714932126696833, 'lift': 4.122410097642296}]</td>\n",
       "      <td>0.007999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[pasta, shrimp]</td>\n",
       "      <td>[{'items_base': ['pasta'], 'items_add': ['shrimp'], 'confidence': 0.3220338983050847, 'lift': 4.506672147735896}]</td>\n",
       "      <td>0.005066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[cake, frozen vegetables, tomatoes]</td>\n",
       "      <td>[{'items_base': ['cake', 'frozen vegetables'], 'items_add': ['tomatoes'], 'confidence': 0.2987012987012987, 'lift': 4.367560314928736}]</td>\n",
       "      <td>0.003066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[cereals, ground beef, spaghetti]</td>\n",
       "      <td>[{'items_base': ['cereals', 'spaghetti'], 'items_add': ['ground beef'], 'confidence': 0.45999999999999996, 'lift': 4.681763907734057}]</td>\n",
       "      <td>0.003066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[chocolate, ground beef, herb &amp; pepper]</td>\n",
       "      <td>[{'items_base': ['chocolate', 'herb &amp; pepper'], 'items_add': ['ground beef'], 'confidence': 0.4411764705882354, 'lift': 4.4901827759597746}]</td>\n",
       "      <td>0.003999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[eggs, ground beef, herb &amp; pepper]</td>\n",
       "      <td>[{'items_base': ['eggs', 'ground beef'], 'items_add': ['herb &amp; pepper'], 'confidence': 0.2066666666666667, 'lift': 4.178454627133872}]</td>\n",
       "      <td>0.004133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[french fries, ground beef, herb &amp; pepper]</td>\n",
       "      <td>[{'items_base': ['french fries', 'ground beef'], 'items_add': ['herb &amp; pepper'], 'confidence': 0.23076923076923078, 'lift': 4.665768194070081}, {'items_base': ['french fries', 'herb &amp; pepper'], 'items_add': ['ground beef'], 'confidence': 0.46153846153846156, 'lift': 4.697421981004071}]</td>\n",
       "      <td>0.003200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[ground beef, herb &amp; pepper, spaghetti]</td>\n",
       "      <td>[{'items_base': ['herb &amp; pepper', 'spaghetti'], 'items_add': ['ground beef'], 'confidence': 0.3934426229508197, 'lift': 4.004359721511667}]</td>\n",
       "      <td>0.006399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[ground beef, spaghetti, tomato sauce]</td>\n",
       "      <td>[{'items_base': ['spaghetti', 'tomato sauce'], 'items_add': ['ground beef'], 'confidence': 0.4893617021276596, 'lift': 4.980599901844742}]</td>\n",
       "      <td>0.003066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[milk, olive oil, soup]</td>\n",
       "      <td>[{'items_base': ['milk', 'olive oil'], 'items_add': ['soup'], 'confidence': 0.2109375, 'lift': 4.174781497361478}]</td>\n",
       "      <td>0.003600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[milk, soup, tomatoes]</td>\n",
       "      <td>[{'items_base': ['milk', 'tomatoes'], 'items_add': ['soup'], 'confidence': 0.21904761904761905, 'lift': 4.335293378565146}]</td>\n",
       "      <td>0.003066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[mineral water, olive oil, whole wheat pasta]</td>\n",
       "      <td>[{'items_base': ['mineral water', 'whole wheat pasta'], 'items_add': ['olive oil'], 'confidence': 0.4027777777777778, 'lift': 6.115862573099416}]</td>\n",
       "      <td>0.003866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[chocolate, frozen vegetables, mineral water, shrimp]</td>\n",
       "      <td>[{'items_base': ['chocolate', 'frozen vegetables', 'mineral water'], 'items_add': ['shrimp'], 'confidence': 0.32876712328767127, 'lift': 4.600899611531385}, {'items_base': ['chocolate', 'mineral water', 'shrimp'], 'items_add': ['frozen vegetables'], 'confidence': 0.4210526315789474, 'lift': 4.417224880382776}]</td>\n",
       "      <td>0.003200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[frozen vegetables, milk, mineral water, olive oil]</td>\n",
       "      <td>[{'items_base': ['frozen vegetables', 'milk', 'mineral water'], 'items_add': ['olive oil'], 'confidence': 0.30120481927710846, 'lift': 4.573557387444516}, {'items_base': ['milk', 'mineral water', 'olive oil'], 'items_add': ['frozen vegetables'], 'confidence': 0.39062500000000006, 'lift': 4.098011363636364}]</td>\n",
       "      <td>0.003333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[frozen vegetables, milk, mineral water, soup]</td>\n",
       "      <td>[{'items_base': ['frozen vegetables', 'milk', 'mineral water'], 'items_add': ['soup'], 'confidence': 0.27710843373493976, 'lift': 5.484407286136631}, {'items_base': ['frozen vegetables', 'mineral water', 'soup'], 'items_add': ['milk'], 'confidence': 0.6052631578947368, 'lift': 4.670863114576565}]</td>\n",
       "      <td>0.003066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[frozen vegetables, mineral water, shrimp, spaghetti]</td>\n",
       "      <td>[{'items_base': ['mineral water', 'shrimp', 'spaghetti'], 'items_add': ['frozen vegetables'], 'confidence': 0.39062500000000006, 'lift': 4.098011363636364}]</td>\n",
       "      <td>0.003333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# и взгялнем на итоги\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(data_df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итого мы видим:\n",
    "\n",
    "1. Пары items\n",
    "2. items_base - первый элемент пары\n",
    "3. items_add - второй (добавленный алгоритмом) элемент пары\n",
    "4. confidence - значение confidence для пары\n",
    "5. lift - значение lift для пары\n",
    "6. support - начение support для пары. При желании, по нему можно отсортировать \n",
    "\n",
    "\n",
    "Результаты логичные: эскалоп и макароны, эскалоп и сливочно-грибной соус, курица и нежирная сметана, мягкий сыр и мед и т.д. - все это вполне логичные и, главное, вкусные сочетания:)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Реализация в R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ARL тот случай,скогда R-фаги могут злорадно похихикать. В R реализована библиотека arules, где присутствует и apriori, и другие алгоритмы. Официальную доку можно посмотреть [тут](https://cran.r-project.org/web/packages/arules/arules.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Посмотрим на нее в действии:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала установим ее (если еще не установили):\n",
    "<br>\n",
    "install.packages('arules')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем данные и преобразуем их в матрицу транзакций:\n",
    "<BR>\n",
    "<br>\n",
    "$library(arules)$\n",
    "<br>\n",
    "$dataset = read.csv('Market_Basket_Optimisation.csv', header = FALSE)$\n",
    "<br>\n",
    "$dataset = read.transactions('Market_Basket_Optimisation.csv', sep = ',', rm.duplicates = TRUE)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на данные:\n",
    "<BR>\n",
    "$summary(dataset)$\n",
    "<br>\n",
    "$itemFrequencyPlot(dataset, topN = 10)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выучим наши правила:\n",
    "<br>\n",
    "В общем виде фнкция вызова apriori выглядит так:  $apriori(data, parameter = NULL, appearance = NULL, control = NULL)$, где\n",
    "<br>\n",
    "\n",
    "$data$ - наш датасет\n",
    "<br>\n",
    "$paramter$ - список (list) параметров для модели: минимальные support, confidence и lift\n",
    "<br>\n",
    "$appearance$ - отвечает за отображение данных. Может принимать значения lhs, rhs, both, items, none, которые определяют положение items в output\n",
    "<br>\n",
    "$control$ - отвечает за сортировку вывода (ascending, descending, без сортировки), а также за то, отображать ли прогрессбар или нет (параметр verbose)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$rules = apriori(data = dataset, parameter = list(support = 0.004, confidence = 0.2))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И посмотрим на результаты:\n",
    "<br>\n",
    "$inspect(sort(rules, by = 'lift')[1:10])$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убедимся, что на выходе имеем примерно те же результаты, что при использовании модуля apyori в Python:\n",
    "<br>\n",
    "\n",
    "1. {light cream}                               => {chicken}       0.004532729\n",
    "2. {pasta}                                     => {escalope}      0.005865885\n",
    "3. {pasta}                                     => {shrimp}        0.005065991\n",
    "4. {eggs,ground beef}                          => {herb & pepper} 0.004132782\n",
    "5. {whole wheat pasta}                         => {olive oil}     0.007998933\n",
    "6. {herb & pepper,spaghetti}                   => {ground beef}   0.006399147\n",
    "7. {herb & pepper,mineral water}               => {ground beef}   0.006665778\n",
    "8. {tomato sauce}                              => {ground beef}   0.005332622\n",
    "9. {mushroom cream sauce}                      => {escalope}      0.005732569\n",
    "10. {frozen vegetables,mineral water,spaghetti} => {ground beef}   0.004399413"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, в R apriori использовать на данный момент намного удобнее, чем в Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ECLAT Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Теория"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Идея алгоритма ECLAT (Equivalence CLAss Transformation) заключается в ускорении подсчета $supp(X)$. Для этого нам необходимо проиндексировать нашу базу данных $D$ так, чтобы это позволило быстро рассчитывать $supp(X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Легко заметить, что если $t(X)$ обозначает множество всех транзакций, где встречается подмножество $X$, то\n",
    "<br>$t(XY) = t(X) \\cap t(Y)$</br>\n",
    "<br>и</br>\n",
    "<br>$supp(XY) = |t(XY)|$</br>\n",
    "<br> то есть $supp(XY)$ равен кардинальности (размеру) множества $t(XY)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/Pruning.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный подход может быть значительно усовершенствован путем уменьшения размера промежуточных множеств идентификаторов транзакций (tidsets). А именно, мы можем хранить не все множество транзакций на промежуточном уровне, а только множество различий этих транзакций. Предположим, что\n",
    "<br>$X_a = \\{x_1, x_2,..., x_{n-1}, a\\}$</br>\n",
    "<br>$X_b = \\{x_1, x_2,..., x_{n-1}, b\\}$</br>\n",
    "<br>Тогда, мы получим: </br>\n",
    "<br>$X_{ab} = \\{x_1, x_2,..., x_{n-1}, a, b\\}$</br>\n",
    "<br>$diffset(X_{ab})$ это множество всех id транзакций, которые содержат префикс $X_a$ но не содержат элемент $b$: </br>\n",
    "<br>$d(X_{ab}) =t(X_a)/t(X_{ab})=t(X_a)/t(X_{b})$</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"img/Diffsets.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В отличие от Apriori-алгоритма, ECLAT производите поиск в глубину (DFS, [подробнее тут](https://ru.wikipedia.org/wiki/%D0%9F%D0%BE%D0%B8%D1%81%D0%BA_%D0%B2_%D0%B3%D0%BB%D1%83%D0%B1%D0%B8%D0%BD%D1%83)). Иногда его называют \"вертикальным\" (в отличие от \"горизонтального\" для Apriori)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ВХОД**: Датасет $D$, содержащий список транзакций, $\\sigma$ - задаваемый пользователем порог $supp$ и новый элемент префикс $I \\subseteq J$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ВЫХОД**: Список itemsets $F[I](D, \\sigma)$ для соответсвующего префикса $I$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ПОДХОД:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $F[i] \\leftarrow$ {}\n",
    "<br>\n",
    "2. **for** all $i \\in J$ in $D$ **do**:\n",
    "<br>\n",
    "$F[I] := F[I] \\cup$ {I $\\cup$ {i}} \n",
    "<br>\n",
    "3. #Создаем $D_i$\n",
    "<br>\n",
    "$D_i \\leftarrow$ {}\n",
    "<br>\n",
    "**for** all $j \\in J$ in $D$ таких, что $j > i$ **do:**\n",
    "<br>\n",
    "$C \\leftarrow$ покрывает ({$i$} $\\cap$ покрывает {$j$})\n",
    "<br>\n",
    "**if** $|C| \\geq \\sigma$ **then**\n",
    "<br>\n",
    "$D_i \\leftarrow D_i \\cup$ {$C$,$i$}\n",
    "<br>\n",
    "4. #DFS - рекурсия:\n",
    "<br>\n",
    "Считаем $F|I \\cup$ {$i$}| $(D_i, \\sigma)$\n",
    "<br>\n",
    "$F[I]: F[I] \\cup F[I \\cup i]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ключевым понятием для ECLAT-алгоритма является I-префикс. В началае генерируется пустое множество I, это позволяет нам на первом проходе выделить все частотные itemsets. Затем алгоритм будет вызывать сам себя и увеличивать I на 1 на каждом шаге до тех пор, пока не будет достигнута заданная пользователем длина I. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для хранения значений используется префиксное дерево (trie (не tree:)), [тут подробнее](https://ru.wikipedia.org/wiki/%D0%9F%D1%80%D0%B5%D1%84%D0%B8%D0%BA%D1%81%D0%BD%D0%BE%D0%B5_%D0%B4%D0%B5%D1%80%D0%B5%D0%B2%D0%BE)). Вначале строится нулевой корень дерева (то самое пустое множество I), затем по мере прохода по itemsets алгоритм прописывает содержащиеся в каждом itesmsets items, при этом самая левая ветвь является child нулевого корня и далее вниз. При этом ветвей столько, сколкьо items встречаестя в itemsets. Такой подход позволяет записывать itemset в памяти только один раз, что делает ECALT быстрее Apriori. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Реализация в Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует несколько реализаций данного алгоритма в Python, желающие могут погуглить и даже попытаться их заставить работать, но мы же напишем свой, максимально простой и, главное, рабочий:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\"\"\"\n",
    "Класс инициируется 3мя параметрами:\n",
    "- min_supp - минимальный support  который мы рассматриваем для ItemSet. Рассчитывается как % от количества транзакций\n",
    "- max_items - максимальное количество елементов в нашем ItemSet\n",
    "- min_items - минимальное количество элементов ItemSet\n",
    "\"\"\"\n",
    "class Eclat:\n",
    "    #инициализация объекта класса\n",
    "    def __init__(self, min_support = 0.01, max_items = 5, min_items = 2):\n",
    "        self.min_support = min_support\n",
    "        self.max_items = max_items\n",
    "        self.min_items = min_items\n",
    "        self.item_lst = list()\n",
    "        self.item_len = 0\n",
    "        self.item_dict = dict()\n",
    "        self.final_dict = dict()\n",
    "        self.data_size = 0\n",
    "    \n",
    "    #создание словаря из ненулевых объектов из всех транзакций (вертикальный датасет)\n",
    "    def read_data(self, dataset):\n",
    "        for index, row in dataset.iterrows():\n",
    "            row_wo_na = row.dropna().unique()\n",
    "            for item in row_wo_na:\n",
    "                item = item.strip()\n",
    "                if item in self.item_dict:\n",
    "                    self.item_dict[item][0] += 1\n",
    "                else:\n",
    "                    self.item_dict.setdefault(item, []).append(1)\n",
    "                self.item_dict[item].append(index)\n",
    "        #задаем переменные экземпляра (instance variables)\n",
    "        self.data_size = dataset.shape[0]\n",
    "        self.item_lst = list(self.item_dict.keys())\n",
    "        self.item_len = len(self.item_lst)\n",
    "        self.min_support = self.min_support * self.data_size\n",
    "        #print (\"min_supp\", self.min_support)\n",
    "        \n",
    "    #рекурсивный метод для поиска всех ItemSet по алгоритму Eclat\n",
    "    #структура данных: {Item: [Supp number, tid1, tid2, tid3, ...]}\n",
    "    def recur_eclat(self, item_name, tids_array, minsupp, num_items, k_start):\n",
    "        if tids_array[0] >= minsupp and num_items <= self.max_items:\n",
    "            for k in range(k_start+1, self.item_len):\n",
    "                if self.item_dict[self.item_lst[k]][0] >= minsupp:\n",
    "                    new_item = item_name + \" | \" + self.item_lst[k]\n",
    "                    new_tids = np.intersect1d(tids_array[1:], self.item_dict[self.item_lst[k]][1:])\n",
    "                    new_tids_size = new_tids.size\n",
    "                    new_tids = np.insert(new_tids, 0, new_tids_size)\n",
    "                    if new_tids_size >= minsupp:\n",
    "                        if num_items >= self.min_items: self.final_dict.update({new_item: new_tids})\n",
    "                        self.recur_eclat(new_item, new_tids, minsupp, num_items+1, k)\n",
    "    \n",
    "    #последовательный вызов функций определенных выше\n",
    "    def fit(self, dataset):\n",
    "        i = 0\n",
    "        self.read_data(dataset)\n",
    "        for w in self.item_lst:\n",
    "            self.recur_eclat(w, self.item_dict[w], self.min_support, 2, i)\n",
    "            i+=1\n",
    "        return self\n",
    "        \n",
    "    #вывод в форме словаря {ItemSet: support(ItemSet)}\n",
    "    def transform(self):\n",
    "        return {k: \"{0:.4f}%\".format((v[0]+0.0)/self.data_size*100) for k, v in self.final_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Потестируем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#создадим экземпляр класса с нужными нам параметрами\n",
    "model = Eclat(min_support = 0.01, max_items = 4, min_items = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Eclat at 0x224ff5bd240>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#обучим\n",
    "model.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eggs | spaghetti | chocolate': '1.0532%',\n",
       " 'milk | spaghetti | chocolate': '1.0932%',\n",
       " 'mineral water | chocolate | ground beef': '1.0932%',\n",
       " 'mineral water | eggs | chocolate': '1.3465%',\n",
       " 'mineral water | eggs | ground beef': '1.0132%',\n",
       " 'mineral water | eggs | milk': '1.3065%',\n",
       " 'mineral water | eggs | spaghetti': '1.4265%',\n",
       " 'mineral water | french fries | spaghetti': '1.0132%',\n",
       " 'mineral water | frozen vegetables | spaghetti': '1.1998%',\n",
       " 'mineral water | milk | chocolate': '1.3998%',\n",
       " 'mineral water | milk | frozen vegetables': '1.1065%',\n",
       " 'mineral water | milk | ground beef': '1.1065%',\n",
       " 'mineral water | milk | spaghetti': '1.5731%',\n",
       " 'mineral water | olive oil | spaghetti': '1.0265%',\n",
       " 'mineral water | spaghetti | chocolate': '1.5865%',\n",
       " 'mineral water | spaghetti | ground beef': '1.7064%',\n",
       " 'mineral water | spaghetti | pancakes': '1.1465%'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#и визуализируем результаты\n",
    "model.transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, реализовать алгоритм своими силами довольно просто, хотя с эффективностью стоит поработать:)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Реализация в R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "И вновь пользователи R ликуют, для них никаких танцев с бубном делать не надо, все по аналогии с apriori. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Запускаем библиотеку и читаем данные:\n",
    "\n",
    "$library(arules)$\n",
    "<br>\n",
    "$dataset = read.csv('Market_Basket_Optimisation.csv')$\n",
    "<br>\n",
    "$dataset = read.transactions('Market_Basket_Optimisation.csv', sep = ',', rm.duplicates = TRUE)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Быстрый взгляд на датасет:\n",
    "<br>\n",
    "$summary(dataset)$\n",
    "<br>\n",
    "$itemFrequencyPlot(dataset, topN = 10)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Правила:\n",
    "<br>\n",
    "$rules = eclat(data = dataset, parameter = list(support = 0.003, minlen = 2))$ \n",
    "<br>\n",
    "Обратите внимание, настраиваем толкьо support и минимальную длину (k в k-itemset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И смотрим на результаты:\n",
    "<br>\n",
    "$inspect(sort(rules, by = 'support')[1:10])$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP-Growth Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Теория"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "FP-Growth (Frequent Pattern Growth) алгоритм самый молодой из нашей троицы, впервые он описан в 2000 году в [7].\n",
    "FP-Growth предлагает радикальную вещь - отказаться от генерации **кандидатов** (напомним, генерация кандидатов лежит в основе Apriori и ECLAT). Теоретчиески, такой подход позволит еще больше увеличить скорость алгоритма и использовать еще меньше памяти."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Это достигается за счет хранения в памяти префиксного дерева (trie) не из комбинаций кандидатов, а из самих транзакций. \n",
    "При этом FP-Growth генерирует таблицу заголовков для каждого item, чей $supp$ выше заданного пользователем. Эта таблица заголовков хранит связанный список всех однотипных узлов префиксного дерева. Таким образом, алгоритм сочетает в себе плюсы **BFS** за счет таблицы заголовков и **DFS** за счет построения trie. Псевдокод алгоритма схож с ECALT, за некоторыми исключениями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ВХОД**: Датасет $D$, содержащий список транзакций, $\\sigma$ - задаваемый пользователем порог $supp$ и префикс $I \\subseteq J$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ВЫХОД**: Список itemsets $F[I](D, \\sigma)$ для соответсвующего префикса $I$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ПОДХОД:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $F[i] \\leftarrow$ {}\n",
    "<br>\n",
    "2. **for** all $i \\in J$ in $D$ **do**:\n",
    "<br>\n",
    "$F[I] := F[I] \\cup$ {I $\\cup$ {i}} \n",
    "<br>\n",
    "3. #Создаем $D_i$\n",
    "<br>\n",
    "$D_i \\leftarrow$ {}\n",
    "<br> \n",
    "**$H_i \\leftarrow$ {}**\n",
    "<br>\n",
    "**for** all $j \\in J$ in $D$ таких, что $j > i$ **do:**\n",
    "<br>\n",
    "**if** $supp$ ($I \\cup$ {$i$,$j$}) $\\geq \\sigma$ **then**:\n",
    "<br>\n",
    "$H \\leftarrow H \\cup$ {$j$}\n",
    "<br>\n",
    "**for** all $(tid, X) \\in D$ при $I \\in X$ **do**:\n",
    "<br>\n",
    "$D_i \\leftarrow D_i \\cup$ ({$tid,X \\cap H$})\n",
    "<br>\n",
    "4. #DFS - рекурсия:\n",
    "<br>\n",
    "Считаем $F|I \\cup$ {$i$}| $(D_i, \\sigma)$\n",
    "<br>\n",
    "$F[I] \\leftarrow F[I] \\cup F[I \\cup i]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Реализация в Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализации FP-Growth в Питоне повезло не больше, чем другим ALR-алгоритмам. Стандартных библиотек под него нет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Неплохо FP_Growth представлен в pyspark, смотреть [тут](http://spark.apache.org/docs/2.2.0/mllib-frequent-pattern-mining.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На gitHub тоже можно найти несколкьо решений эпохи неолита, например [тут](https://github.com/enaeseth/python-fp-growth) и [тут](https://github.com/evandempsey/fp-growth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Потестим второй вариант"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyfpgrowth - установка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyfpgrowth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "#Сгенериуем паттерны\n",
    "patterns = pyfpgrowth.find_frequent_patterns(transactions, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Выучим правила\n",
    "rules = pyfpgrowth.generate_association_rules(patterns, 30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Покажем\n",
    "rules;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Реализация в R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном случае R не отстает от Питона: в такой удобной и родной arules библиотеке FP-Growth отсутствует. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "В то же время, как и для Питона, реализация сущетсвует в Spark - https://spark.apache.org/docs/2.2.0/api/R/spark.fpGrowth.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Заключение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В заключении давайте эмпирически сравним **эффектвиность** метрикой *runtime* в зависимости от плотности датасета и длин транзакций датасета[9]. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Под *плотностью* датасета подразумеваестя отношение транзакций, содержащих в себе частые items к общему количеству транзакций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/density.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из графика очевидно, что эффективность (чем меньше runtime, тем эффективнее) Apriori-алгоритма падает при увеличении плотности датасета."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Под *длинной транзакции* понимается количество в items в itemset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/size.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очевидно, что при увеличении длины транзакции Apriori также справляется гораздо хуже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Итоги"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Итак, мы познакомились с базовой теорией ARL (\"кто купил х, также купил y\") и основными понятиями и метриками (support, confidence, lift и conviction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Посмотрели 3 самых популярных алгоритма (Apriori, ECLAT, FP-Growth), позавидовали пользователям R и библиотеки arules, попробовали сами реализовать ECALT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основные моменты:\n",
    "1. ARL лежат в основе рекомендательных систем\n",
    "2. ARL широко применимы - от традиционного ритейла и онлайн ритейла (от Ozon до Steam), обычных закупок ТМЦ до банков и телекома (подключаемые сервисы и услуги)\n",
    "3. ARL относительно легко использовать, существуют реализации разного уровня проработки для разных задач.\n",
    "4. ARL хорошо интепретируются и не требуют специальных навыков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо рассмотренных бызовых алгоритмов существет модификации и ответвления:\n",
    "\n",
    "1. Алгоритм CHARM для поиска закрытых itemsets (пример: допустим, у нас есть itemset (1,2,3,4), открытый - (1,2), закрытый - (1,2,3,4). Он не очень подходит для прикладных задач в ритейле, т.к. нам чаще всего интересны сочетания индивидуальных items, а не их наборов. \n",
    "1. AprioriDP (Deep Programming) - позволяет хранить $supp$ в специальной структруе данных, работает немного быстрее классичсекого Apriori\n",
    "2. FP Bonsai - улучшенный FP-Growth с обрезкой префиксного дерева (пример алгоритма с **ограничениями**)\n",
    "3. Серия Multi-Relations алгоритмов, которые определают несколкьо свзяей в itemsets.\n",
    "4. и другие"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В заключении не можем не упомянуть о сумрачном гении ARL докторе Кристиане Боргельте из Университета Констанца (http://www.borgelt.net/software.html).\n",
    "Кристиан реализовывал упомянутые нами алгоритмы на С, Python, Java и R. Ну или почти все. Существует даже GUI за его авторством, где в пару кликов можно загрузить датасет, выбрать нужный алгоритм и найти правила. Это при условии, что оно у вас заработает:)\n",
    "Для простых же задач достаточно и того, что мы рассмотрели в этой статье. А если недостаточно - призываем писать реализацию самим! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Использованная литература:**\n",
    "<br>\n",
    "[1] Discovery, analysis and presentation of strong rules. G. Piatetsky-Shapiro. Knowledge Discovery in Databases, AAAI Press, (1991)\n",
    "<br>\n",
    "[2] Mining Association Rules between Sets of Items in Large Databases http://arbor.ee.ntu.edu.tw/~chyun/dmpaper/agrama93.pdf\n",
    "<br>\n",
    "[3] Fast Algorithms for Mining Association Rules http://www.vldb.org/conf/1994/P487.PDF\n",
    "<br>\n",
    "[4] Ask Dan! http://www.dssresources.com/newsletters/66.php\n",
    "<br>\n",
    "[5] Introduction to arul)es – A computational environment for mining association rules and frequent item sets http://www.lsi.upc.edu/~belanche/Docencia/mineria/Practiques/R/arules.pdf\n",
    "<br>\n",
    "[6] Публикации Д-ра Боргельта - http://www.borgelt.net/publications.html\n",
    "<br>\n",
    "[7] J. Han, J. Pei, and Y. Yin, “Mining frequent patterns without candidate generation,” in ACM SIGMOD Record, vol. 29, no. 2. ACM, 2000, pp. 1–12.\n",
    "<br>\n",
    "[8] Shimon, Sh. Improving Data mining algorithms using constraints. The Open University of Israel, 2012.\n",
    "<br>\n",
    "[9] Jeff Heaton. Comparing Dataset Characteristics that Favor the Apriori, Eclat or FP-Growth Frequent Itemset Mining Algorithms (https://arxiv.org/pdf/1701.09042.pdf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
