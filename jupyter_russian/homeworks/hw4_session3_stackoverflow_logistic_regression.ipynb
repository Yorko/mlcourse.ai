{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\">\n",
    "## Открытый курс по машинному обучению. Сессия № 3\n",
    "Автор материала: Павел Нестеров (@mephistopheies). Материал распространяется на условиях лицензии [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Можно использовать в любых целях (редактировать, поправлять и брать за основу), кроме коммерческих, но с обязательным упоминанием автора материала."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Домашняя работа №4\n",
    "## <center> Логистическая регрессия в задаче тегирования вопросов StackOverflow\n",
    "\n",
    "**Надо вывести формулы, где это просится (да, ручка и бумажка), заполнить код в клетках и выбрать ответы в [веб-форме](https://docs.google.com/forms/d/100c3Ek94UL-VRwXrN4lxCSnGjfJrl6Gc96G21DNCh4w).**\n",
    "\n",
    "## 0. Описание задачи\n",
    "\n",
    "В этой домашней работе мы с вами изучим и запрограммируем модель для прогнозирования тегов по тексту вопроса на базе многоклассовой логистической регрессии. В отличие от обычной постановки задачи классификации (multiclass), в данном случае один пример может принадлежать одновременно к нескольким классам (multilabel). Мы будем реализовывать онлайн-версию алгоритма multilabel-классификации.\n",
    "\n",
    "Мы будем использовать небольшую выборку из протеггированных вопросов с сайта StackOverflow размером в 125 тысяч примеров (около 150 Мб, скачайте по [этой](https://drive.google.com/open?id=0B4bl7YMqDnViYVo0V2FubFVhMFE) ссылке).\n",
    "\n",
    "PS: Можно показать, что такая реализация совсем не эффективная и проще было бы использовать векторизированные вычисления. Для данного датасета так и есть. Но на самом деле подобные реализации используются в жизни, но естественно, написаны они не на Python. Например, в онлайн-моделях прогнозирования [CTR](https://en.wikipedia.org/wiki/Click-through_rate) юзеру показывается баннер, затем в зависимости от наличия клика происходит обновление параметров модели. В реальной жизни параметров модели может быть несколько сотен миллионов, а у юзера из этих ста миллионов от силы сто или тысяча параметров отличны от нуля, векторизировать такие вычисления не очень эффективно. Обычно все это хранится в огромных кластерах в in-memory базах данных, а обработка пользователей происходит распределенно.\n",
    "\n",
    "PS2:\n",
    "- в процессе решения домашней работы вам придется работать с текстом, и у вас может возникнуть желание сделать очевидный препроцессинг, например привести все слова в нижний регистр, в-общем **этого делать не нужно, если не оговорено заранее в задании**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n"
     ]
    }
   ],
   "source": [
    "#!pip install watermark\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем версии используемых библиотек. Совпадут ли ответы в случае других версий - не гарантируется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPython 3.6.3\n",
      "IPython 6.1.0\n",
      "\n",
      "numpy 1.13.3\n",
      "scipy 0.19.1\n",
      "pandas 0.20.3\n",
      "matplotlib 2.1.0\n",
      "sklearn 0.19.1\n",
      "\n",
      "compiler   : GCC 7.2.0\n",
      "system     : Linux\n",
      "release    : 4.13.0-1011-gcp\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 8\n",
      "interpreter: 64bit\n",
      "Git hash   : 2a020a559578444040250c1174e3e2175b962da8\n"
     ]
    }
   ],
   "source": [
    "%watermark -v -m -p numpy,scipy,pandas,matplotlib,sklearn -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"dark\")\n",
    "plt.rcParams['figure.figsize'] = 16, 12\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# поменяйте на свой путь\n",
    "DS_FILE_NAME = '../../../mlco_data/stackoverflow_sample_125k.tsv'\n",
    "TAGS_FILE_NAME = '../../../mlco_data/top10_tags.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'jquery', 'php', 'android', 'ios', 'javascript', 'c#', 'html', 'python', 'c++', 'java'}\n"
     ]
    }
   ],
   "source": [
    "top_tags = []\n",
    "with open(TAGS_FILE_NAME, 'r') as f:\n",
    "    for line in f:\n",
    "        top_tags.append(line.strip())\n",
    "top_tags = set(top_tags)\n",
    "print(top_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Многоклассовая логистическая регрессия\n",
    "\n",
    "Вспомним, как получается логистическая регрессия для двух классов $\\left\\{0, 1\\right\\}$, вероятность принадлежности объекта к классу $1$ выписывается по теореме Байеса:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "p\\left(c = 1 \\mid \\vec{x}\\right) &=& \\dfrac{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right)}{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right) + p\\left(\\vec{x} \\mid c = 0\\right)p\\left(c = 0\\right)} \\\\\n",
    "&=& \\dfrac{1}{1 + e^{-a}} \\\\\n",
    "&=& \\sigma\\left(a\\right)\n",
    "\\end{array}$$\n",
    "где:\n",
    "- $\\vec{x}$ – вектор признаков объекта\n",
    "- $\\sigma$ – обозначение функции логистического сигмоида при скалярном аргументе\n",
    "- $a = \\log \\frac{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right)}{p\\left(\\vec{x} \\mid c = 0\\right)p\\left(c = 0\\right)} = \\sum_{i=0}^M w_i x_i$ – это отношение мы моделируем линейной функцией от признаков объекта и параметров модели\n",
    "\n",
    "Данное выражение легко обобщить до множества из $K$ классов, изменится только знаменатель в формуле Байеса. Запишем вероятность принадлежности объекта к классу $k$:\n",
    "$$\\large \\begin{array}{rcl}\n",
    "p\\left(c = k \\mid \\vec{x}\\right) &=& \\dfrac{p\\left(\\vec{x} \\mid c = k\\right)p\\left(c = k\\right)}{\\sum_{i=1}^K p\\left(\\vec{x} \\mid c = i\\right)p\\left(c = i\\right)} \\\\\n",
    "&=& \\dfrac{e^{z_k}}{\\sum_{i=1}^{K}e^{z_i}} \\\\\n",
    "&=& \\sigma_k\\left(\\vec{z}\\right)\n",
    "\\end{array}$$\n",
    "где:\n",
    "- $\\sigma_k$ – обозначение функции softmax при векторном аргументе\n",
    "- $z_k = \\log p\\left(\\vec{x} \\mid c = k\\right)p\\left(c = k\\right) = \\sum_{i=0}^M w_{ki} x_i$ – это выражение моделируется линейной функцией от признаков объекта и параметров модели для класса $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для моделирования полного правдоподобия примера мы используем [категориальное распределение](https://en.wikipedia.org/wiki/Categorical_distribution), а лучше его логарифм (для удобства):\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\mathcal{L} = \\log p\\left({\\vec{x}}\\right) &=& \\log \\prod_{i=1}^K \\sigma_i\\left(\\vec{z}\\right)^{y_i} \\\\\n",
    "&=& \\sum_{i=1}^K y_i \\log \\sigma_i\\left(\\vec{z}\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "Получается хорошо знакомая нам функция [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) (если домножить на $-1$). Правдоподобие нужно максимизировать, а, соответственно, перекрестную энтропию нужно минимизировать. Продифференцировав по параметрам модели, мы _легко_ получим правила обновления весов для градиентного спуска, **проделайте этот вывод, если вы его не делали** (если вы вдруг сдались, то на [этом](https://www.youtube.com/watch?v=-WiR16raQf4) видео есть разбор вывода, понимание этого вам понадобится для дальнейшего выполнения задания; если предпочитаете текст, то и он есть [тут](https://www.ics.uci.edu/~pjsadows/notes.pdf) и [тут](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/)):\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} &=& x_m \\left(y_k - \\sigma_k\\left(\\vec{z}\\right)\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "В стандартной формулировке получается, что вектор $\\left(\\sigma_1, \\sigma_2, \\ldots, \\sigma_K\\right)$ образует дискретное вероятностное распределение, т.е. $\\sum_{i=1}^K \\sigma_i = 1$. Но в нашей постановке задачи каждый пример может иметь несколько тегов или одновременно принадлежать к нескольким классам. Для этого мы немного изменим модель:\n",
    "- будем считать, что все теги независимы друг от друга, т.е. каждый исход – это логистическая регрессия на два класса (либо есть тег, либо его нет), тогда вероятность наличия тега у примера запишется следующим образом (каждый тег/класс как и в многоклассовой логрегрессии имеет свой набор параметров):\n",
    "$$\\large p\\left(\\text{tag}_k \\mid \\vec{x}\\right) = \\sigma\\left(z_k\\right) = \\sigma\\left(\\sum_{i=1}^M w_{ki} x^i \\right)$$\n",
    "- наличие каждого тега мы будем моделировать с помощью <a href=\"https://en.wikipedia.org/wiki/Bernoulli_distribution\">распределения Бернулли</a>\n",
    "\n",
    "<font color=\"red\">Вопрос 1.</font> Ваше первое задание –  записать упрощенное выражение логарифма правдоподобия примера с признаками $\\vec{x}$. Как правило, многие алгоритмы оптимизации имеют интерфейс для минимизации функции, мы последуем этой же традиции и домножим полученное выражение на $-1$, а во второй части выведем формулы для минимизации полученного выражения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. <font color=\"red\">$\\large -\\mathcal{L} = -\\sum_{i=1}^M y_i \\log \\sigma\\left(z_i\\right) + \\left(1 - y_i\\right) \\log \\left(1 - \\sigma\\left(z_i\\right)\\right)$</font>\n",
    "2. $\\large -\\mathcal{L} = -\\sum_{i=1}^K y_i \\log \\sigma\\left(z_i\\right) + \\left(1 - y_i\\right) \\log \\left(1 - \\sigma\\left(z_i\\right)\\right)$\n",
    "3. $\\large -\\mathcal{L} = -\\sum_{i=1}^K z_i \\log \\sigma\\left(y_i\\right) + \\left(1 - z_i\\right) \\log \\left(1 - \\sigma\\left(y_i\\right)\\right)$\n",
    "4. $\\large -\\mathcal{L} = -\\sum_{i=1}^M z_i \\log \\sigma\\left(y_i\\right) + \\left(1 - z_i\\right) \\log \\left(1 - \\sigma\\left(y_i\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Вывод формулы обновления весов\n",
    "\n",
    "<font color=\"red\">Вопрос 2.</font>В качестве второго задания вам предоставляется возможность вывести формулу градиента для $-\\mathcal{L}$. Какой вид она будет иметь?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = -x_m \\left(\\sigma\\left(z_k\\right) - y_k\\right)$\n",
    "2. <font color=\"red\">$\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = -x_m \\left(y_k - \\sigma\\left(z_k\\right)\\right)$</font>\n",
    "3. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = \\left(\\sigma\\left(z_k\\right)x_m - y_k\\right)$\n",
    "4. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = \\left(y_k - \\sigma\\left(z_k\\right)x_m\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Реализация базовой модели\n",
    "\n",
    "Вам предлагается каркас класса модели, разберите его внимательно, обращайте внимание на комментарии. Затем заполните пропуски, запустите полученную модель и ответьте на проверочный вопрос.\n",
    "\n",
    "Как вы могли уже заметить, при обновлении веса $w_{km}$ используется значение признака $x_m$, который равен $0$, если слова с индексом $m$ нет в предложении, и больше нуля, если такое слово есть. В нашем случае, чтобы не пересчитывать [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model) самим или с помощью [sklearn.feature_extraction.text.CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer), мы будем идти по словам предложения в порядке их следования. Если какое-то слово встречается несколько раз, то мы добавляем его в аккумулятор со своим весом. В итоге получится то же самое, как если сначала посчитать количество одинаковых слов и домножить на соответствующий вес. Соответственно, при вычислении линейной комбинации $z$ весов модели и признаков примера необходимо учитывать только ненулевые признаки объекта.\n",
    "\n",
    "Подсказка:\n",
    "- если реализовывать вычисление сигмоида так же, как в формуле, то при большом отрицательном значении $z$ вычисление $e^{-z}$ превратится в очень большое число, которое вылетит за допустимые пределы\n",
    "- в то же время $e^{-z}$ от большого положительного $z$ будет нулем\n",
    "- воспользуйтесь свойствами функции $\\sigma$ для того, чтобы пофиксить эту ошибку и реализовать $\\sigma$ без риска overflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    \n",
    "    \"\"\"Конструктор\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    tags : list of string, default=top_tags\n",
    "        список тегов\n",
    "    \"\"\"\n",
    "    def __init__(self, tags=top_tags):      \n",
    "        # словарь который содержит мапинг слов предложений и тегов в индексы (для экономии памяти)\n",
    "        # пример: self._vocab['exception'] = 17 означает что у слова exception индекс равен 17\n",
    "        self._vocab = {}\n",
    "        \n",
    "        # параметры модели: веса\n",
    "        # для каждого класса/тега нам необходимо хранить собственный вектор весов\n",
    "        # по умолчанию у нас все веса будут равны нулю\n",
    "        # мы заранее не знаем сколько весов нам понадобится\n",
    "        # поэтому для каждого класса мы сосздаем словарь изменяемого размера со значением по умолчанию 0\n",
    "        # пример: self._w['java'][self._vocab['exception']]  содержит вес для слова exception тега java\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        \n",
    "        # параметры модели: смещения или вес w_0\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        \n",
    "        self._tags = set(tags)\n",
    "    \n",
    "    \"\"\"Один прогон по датасету\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    fname : string, default=DS_FILE_NAME\n",
    "        имя файла с данными\n",
    "        \n",
    "    top_n_train : int\n",
    "        первые top_n_train строк будут использоваться для обучения, остальные для тестирования\n",
    "        \n",
    "    total : int, default=10000000\n",
    "        информация о количестве строк в файле для вывода прогресс бара\n",
    "    \n",
    "    learning_rate : float, default=0.1\n",
    "        скорость обучения для градиентного спуска\n",
    "        \n",
    "    tolerance : float, default=1e-16\n",
    "        используем для ограничения значений аргумента логарифмов\n",
    "    \"\"\"\n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16):\n",
    "        \n",
    "        self._loss = []\n",
    "        n = 0\n",
    "        \n",
    "        # откроем файл\n",
    "        with open(fname, 'r') as f:            \n",
    "            \n",
    "            # прогуляемся по строкам файла\n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # слова вопроса, это как раз признаки x\n",
    "                sentence = sentence.split(' ')\n",
    "                # теги вопроса, это y\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                # значение функции потерь для текущего примера\n",
    "                sample_loss = 0\n",
    "\n",
    "                # прокидываем градиенты для каждого тега\n",
    "                for tag in self._tags:\n",
    "                    # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                    # инициализируем z\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    z = self._b[tag]\n",
    "   \n",
    "                    for word in sentence:\n",
    "                        # если в режиме тестирования появляется слово которого нет в словаре, то мы его игнорируем\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        z += self._w[tag][self._vocab[word]]\n",
    "    \n",
    "                    # вычисляем вероятность наличия тега\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    #sigma = expit(z)\n",
    "                    sigma = 1.0/(1.0 + np.exp(-z)) if z >= 0 else np.exp(z)/(1.0 + np.exp(z))\n",
    "    \n",
    "                    \n",
    "                    # обновляем значение функции потерь для текущего примера\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    sample_loss += - y * np.log(sigma + tolerance) - (1 - y) * np.log((1 - sigma + tolerance))\n",
    "                 \n",
    "                    \n",
    "                    # если мы все еще в тренировочной части, то обновим параметры\n",
    "                    if n < top_n_train:\n",
    "                        # вычисляем производную логарифмического правдоподобия по весу\n",
    "                        # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                        dLdw = y - sigma\n",
    "\n",
    "                        # делаем градиентный шаг\n",
    "                        # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                        # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                        for word in sentence:                        \n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate*dLdw\n",
    "                        self._b[tag] -= -learning_rate*dLdw\n",
    "                    \n",
    "                n += 1\n",
    "                        \n",
    "                self._loss.append(sample_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b80d97c3c60547e3bb5538a70f835e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# создадим эксемпляр модели и пройдемся по датасету\n",
    "model = LogRegressor()\n",
    "model.iterate_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, действительно ли значение отрицательного логарифмического правдоподобия уменьшалось. Так как мы используем стохастический градентный спуск, не стоит ожидать плавного падения функции ошибки. Мы воспользуемся скользящим средним с окном в 10 тысяч примеров, чтобы хоть как-то сгладить график."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5sAAAKoCAYAAAD58uunAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6c2e5da4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the loss function on the last 10k train samples: 19.85\n"
     ]
    }
   ],
   "source": [
    "print('Mean of the loss function on the last 10k train samples: %0.2f' % np.mean(model._loss[-35000:-25000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 3.</font>\n",
    "Вычислите среднее значение функции стоимости на последних 10 000 примеров тренировочного набора, к какому из значений ваш ответ ближе всего?\n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 17.54\n",
    "2. 18.64\n",
    "3. <font color='red'><b>19.74</b></font>\n",
    "4. 20.84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Тестирование модели\n",
    "\n",
    "В базовой модели первые 100 000 строк используются для обучения, а оставшиеся – для тестирования. Как вы можете заметить, значение отрицательного логарифмического правдоподобия не очень информативно, хоть и позволяет сравнивать разные модели. В качестве четвертого задания вам необходимо модифицировать базовую модель таким образом, чтобы метод `iterate_file` возвращал значение _точности_ на тестовой части набора данных. \n",
    "\n",
    "Точность определим следующим образом:\n",
    "- считаем, что тег у вопроса присутствует, если спрогнозированная вероятность тега больше 0.9\n",
    "- точность одного примера расчитывается как [коэффициент Жаккара](https://ru.wikipedia.org/wiki/Коэффициент_Жаккара) между множеством настоящих тегов и предсказанных моделью\n",
    "  - например, если у примера настоящие теги ['html', 'jquery'], а по версии модели ['ios', 'html', 'java'], то коэффициент Жаккара будет равен |['html', 'jquery'] $\\cap$ ['ios', 'html', 'java']| / |['html', 'jquery'] $\\cup$ ['ios', 'html', 'java']| = |['html']| / |['jquery', 'ios', 'html', 'java']| = 1/4\n",
    "- метод `iterate_file` возвращает **среднюю** точность на тестовом наборе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "# выведем полученное значение с точностью до двух знаков\n",
    "print('%0.2f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Вопрос 4.</font> К какому значению ближе всего полученное значение точности?\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.39\n",
    "2. 0.49\n",
    "3. 0.59\n",
    "4. 0.69"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. $L_2$-регуляризация\n",
    "\n",
    "В качестве пятого задания вам необходимо добавить в класс `LogRegressor` поддержку $L_2$-регуляризации. В методе `iterate_file` должен появиться параметр `lmbda=0.01` со значением по умолчанию. С учетом регуляризации новая функция стоимости примет вид:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\frac{\\lambda}{2} R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\frac{\\lambda}{2} \\sum_{k=1}^K\\sum_{i=1}^M w_{ki}^2\n",
    "\\end{array}$$\n",
    "\n",
    "Градиент первого члена суммы мы уже вывели, а для второго он имеет вид:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\frac{\\partial}{\\partial w_{ki}} \\frac{\\lambda}{2} R\\left(W\\right) &=& \\lambda w_{ki}\n",
    "\\end{array}$$\n",
    "\n",
    "Если мы на каждом примере будем делать честное обновление всех весов, то все очень замедлится, ведь нам придется на каждой итерации пробегать по всем словам словаря. В ущерб теоретической корректности мы используем грязный трюк: будем регуляризировать только те слова, которые присутствуют в текущем предложении. Не забывайте, что смещение (bias) не регуляризируется. `sample_loss` тоже должен остаться без изменений.\n",
    "\n",
    "Замечание:\n",
    "- не забудьте, что нужно учитывать регуляризацию слова в градиентном шаге только один раз\n",
    "- условимся, что учитываем регуляризацию только при первой встрече слова\n",
    "- если бы мы считали сначала bag-of-words, то мы бы в цикле шли по уникальным словам, но т.к. мы этого не делаем, приходится выкручиваться (еще одна жертва богу online-моделей)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 5.</font> К какому значению ближе всего полученное значение точности?\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.3\n",
    "2. 0.35\n",
    "3. 0.4\n",
    "4. 0.52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ElasticNet регуляризация, вывод\n",
    "Помимо $L_2$ регуляризации, часто используется $L_1$ регуляризация.\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\frac{\\lambda}{2} R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\lambda \\sum_{k=1}^K\\sum_{i=1}^M \\left|w_{ki}\\right|\n",
    "\\end{array}$$\n",
    "\n",
    "Если линейно объединить $L_1$ и $L_2$ регуляризацию, то полученный тип регуляризации называется ElasticNet:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\lambda R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\lambda \\left(\\gamma \\sum_{k=1}^K\\sum_{i=1}^M w_{ki}^2 + \\left(1 - \\gamma\\right) \\sum_{k=1}^K\\sum_{i=1}^M \\left|w_{ki}\\right| \\right)\n",
    "\\end{array}$$\n",
    "- где $\\gamma \\in \\left[0, 1\\right]$\n",
    "\n",
    "В качестве шестого вопроса вам предлагается вывести формулу градиента ElasticNet регуляризации (не учитывая $-\\mathcal{L}$). \n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma w_{ki} + \\left(1 - \\gamma\\right) w_{ki}\\right)$ \n",
    "2. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma \\left|w_{ki}\\right| + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$\n",
    "3. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma w_{ki} + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$\n",
    "4. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(\\gamma w_{ki} + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Регуляризация ElasticNet , реализация\n",
    "\n",
    "В качестве седьмой задачи вам предлается изменить класс `LogRegressor` таким образом, чтобы метод `iterate_file` принимал два параметра со значениями по умолчанию `lmbda=0.0002` и `gamma=0.1`. Сделайте один проход по датасету с включенной `ElasticNet`-регуляризацией и заданными значениями по умолчанию и ответьте на вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Вопрос 7.</font> К какому значению ближе всего полученное значение точности:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.59\n",
    "2. 0.69\n",
    "3. 0.79\n",
    "4. 0.82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Самые важные слова для тега\n",
    "\n",
    "Прелесть линейных моделей в том, что они легко интерпретируемы. Вам предлагается вычислить, какие слова вносят наибольший вклад в вероятность появления каждого из тегов. А затем ответьте на контрольный вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model._vocab_inv = dict([(v, k) for (k, v) in model._vocab.items()])\n",
    "\n",
    "for tag in model._tags:\n",
    "    print(tag, ':', ', '.join([model._vocab_inv[k] for (k, v) in \n",
    "                               sorted(model._w[tag].items(), \n",
    "                                      key=lambda t: t[1], \n",
    "                                      reverse=True)[:5]]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 8.</font> Для многих тегов наличие самого тега в предложении является важным сигналом, у многих сам тег является самым сильным сигналом, что не удивительно. Для каких из тегов само название тега не входит в топ-5 самых важных?\n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. c# \n",
    "2. javascript\n",
    "3. jquery\n",
    "4. android"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 9. Сокращаем размер словаря\n",
    "Сейчас количество слов в словаре – 519290, если бы это была выборка из 10 миллионов вопросов с сайта StackOverflow, то размер словаря был бы миллионов 10. Регуляризировать модель можно не только изящно математически, но и топорно, например, ограничить размер словаря. Вам предоставляется возможность внести следующие изменения в класс `LogRegressor`:\n",
    "- добавить в метод `iterate_file` еще один аргумент со значением по умолчанию `update_vocab=True`\n",
    "- при `update_vocab=True` разрешать добавлять слова в словарь в режиме обучения\n",
    "- при `update_vocab=False` игнорировать слова не из словаря\n",
    "- добавить в класс метод `filter_vocab(n=10000)`, который оставит в словаре только топ-n самых популярных слов, используя данные из ``train``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file(update_vocab=True)\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# оставим только топ 10 000 слов\n",
    "model.filter_vocab(n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# сделаем еще одну итерацию по датасету, уменьшив скорость обучения в 10 раз\n",
    "acc = model.iterate_file(update_vocab=False, learning_rate=0.01)\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Вопрос 9.</font> К какому значению ближе всего полученное значение точности:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.48\n",
    "2. 0.58\n",
    "3. 0.68\n",
    "4. 0.78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Прогнозирование тегов для новых вопросов\n",
    "\n",
    "В завершение этого задания вам предлагается реализовать метод `predict_proba`, который принимает строку, содержащую вопрос, а возвращает список предсказанных тегов вопроса с их вероятностями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file(update_vocab=True)\n",
    "print('%0.2f' % acc)\n",
    "model.filter_vocab(n=10000)\n",
    "acc = model.iterate_file(update_vocab=False, learning_rate=0.01)\n",
    "print('%0.2f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = (\"I want to improve my coding skills, so I have planned write \" +\n",
    "            \"a Mobile Application.need to choose between Apple's iOS or Google's Android.\" +\n",
    "            \" my background: I have done basic programming in .Net,C/C++,Python and PHP \" +\n",
    "            \"in college, so got OOP concepts covered. about my skill level, I just know \" +\n",
    "            \"concepts and basic syntax. But can't write complex applications, if asked :(\" +\n",
    "            \" So decided to hone my skills, And I wanted to know which is easier to \" +\n",
    "            \"learn for a programming n00b. A) iOS which uses Objective C B) Android \" + \n",
    "            \"which uses Java. I want to decide based on difficulty \" + \n",
    "            \"level\").lower().replace(',', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted(model.predict_proba(sentence).items(), \n",
    "       key=lambda t: t[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 10.</font> Отметьте все теги, ассоциирующиеся с данным вопросом, если порог принятия равен $0.9$. То есть считаем, что вопросу надо поставить некоторый тег, если вероятность его появления, предсказанная моделью, больше или равна 0.9. \n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. android\n",
    "2. ios\n",
    "3. php\n",
    "4. java"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
