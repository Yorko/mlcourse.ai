{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\">\n",
    "## Открытый курс по машинному обучению\n",
    "<center>Автор материала: Юрий Кашницкий\n",
    "    \n",
    "Материал распространяется на условиях лицензии [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Можно использовать в любых целях (редактировать, поправлять и брать за основу), кроме коммерческих, но с обязательным упоминанием автора материала."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Тема 4. Линейные модели классификации и регрессии\n",
    "## <center>Часть 2. Логистическая регрессия и метод максимального правдоподобия "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Линейный классификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основная идея линейного классификатора заключается в том, что признаковое пространство может быть разделено гиперплоскостью на две полуплоскости, в каждой из которых прогнозируется одно из двух значений целевого класса. \n",
    "Если это можно сделать без ошибок, то обучающая выборка называется *линейно разделимой*.\n",
    "\n",
    "<img src=\"../../img/logit.png\">\n",
    "\n",
    "Мы уже знакомы с линейной регрессией и методом наименьших квадратов. Рассмотрим задачу бинарной классификации, причем метки целевого класса обозначим \"+1\" (положительные примеры) и \"-1\" (отрицательные примеры).\n",
    "Один из самых простых линейных классификаторов получается на основе регрессии вот таким образом:\n",
    "\n",
    "$$\\Large a(\\textbf{x}) = \\text{sign}(\\textbf{w}^{\\text{T}}\\textbf x),$$\n",
    "\n",
    "где\n",
    " - $\\textbf{x}$ – вектор признаков примера (вместе с единицей);\n",
    " - $\\textbf{w}$ – веса в линейной модели (вместе со смещением $w_0$);\n",
    " - $\\text{sign}(\\bullet)$ – функция \"сигнум\", возвращающая знак своего аргумента;\n",
    " - $a(\\textbf{x})$ – ответ классификатора на примере $\\textbf{x}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Логистическая регрессия как линейный классификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия является частным случаем линейного классификатора, но она обладает хорошим \"умением\" – прогнозировать вероятность $p_+$ отнесения примера $\\textbf{x}_\\text{i}$ к классу \"+\":\n",
    "$$\\Large p_+ = \\text P\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) $$\n",
    "\n",
    "Прогнозирование не просто ответа (\"+1\" или \"-1\"), а именно *вероятности* отнесения к классу \"+1\" во многих задачах является очень важным бизнес-требованием. Например, в задаче кредитного скоринга, где традиционно применяется логистическая регрессия, часто прогнозируют вероятность невозврата кредита ($p_+$). Клиентов, обратившихся за кредитом, сортируют по этой предсказанной вероятности (по убыванию), и получается скоркарта — по сути, рейтинг клиентов от плохих к хорошим. Ниже приведен игрушечный пример такой скоркарты. \n",
    "    <img src='../../img/toy_scorecard.png' width=60%>\n",
    "\n",
    "Банк выбирает для себя порог $p_*$ предсказанной вероятности невозврата кредита (на картинке – $0.15$) и начиная с этого значения уже не выдает кредит. Более того, можно умножить предсказнную вероятность на выданную сумму и получить матожидание потерь с клиента, что тоже будет хорошей бизнес-метрикой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, мы хотим прогнозировать вероятность $p_+ \\in [0,1]$, а пока умеем строить линейный прогноз с помощью МНК: $b(\\textbf{x}) = \\textbf{w}^\\text{T} \\textbf{x} \\in \\mathbb{R}$. Каким образом преобразовать полученное значение в вероятность, пределы которой – [0, 1]? Очевидно, для этого нужна некоторая функция $f: \\mathbb{R} \\rightarrow [0,1].$ В модели логистической регрессии для этого берется конкретная функция: $\\sigma(z) = \\frac{1}{1 + \\exp^{-z}}$. И сейчас разберемся, каковы для этого предпосылки. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# отключим всякие предупреждения Anaconda\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(z):\n",
    "    return 1. / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xx = np.linspace(-10, 10, 1000)\n",
    "plt.plot(xx, [sigma(x) for x in xx]);\n",
    "plt.xlabel('z');\n",
    "plt.ylabel('sigmoid(z)')\n",
    "plt.title('Sigmoid function');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обозначим $P(X)$ вероятностью происходящего события $X$. Тогда отношение вероятностей $OR(X)$ определяется из $\\frac{P(X)}{1-P(X)}$, а это — отношение вероятностей того, произойдет ли событие или не произойдет. Очевидно, что вероятность и отношение шансов содержат одинаковую информацию. Но в то время как $P(X)$ находится в пределах от 0 до 1, $OR(X)$ находится в пределах от 0 до $\\infty$.\n",
    "\n",
    "Если вычислить логарифм $OR(X)$ (то есть называется логарифм шансов, или логарифм отношения вероятностей), то легко заметить, что $\\log{OR(X)} \\in \\mathbb{R}$. Его то мы и будем прогнозировать с помощью МНК.\n",
    "\n",
    "Посмотрим, как логистическая регрессия будет делать прогноз $p_+ = \\text{P}\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right)$ (пока считаем, что веса $\\textbf{w}$ мы как-то получили (т.е. обучили модель), далее разберемся, как именно). \n",
    "\n",
    "**Шаг 1.** Вычислить значение $w_{0}+w_{1}x_1 + w_{2}x_2 + ... = \\textbf{w}^\\text{T}\\textbf{x}$. (уравнение $\\textbf{w}^\\text{T}\\textbf{x} = 0$ задает гиперплоскость, разделяющую примеры на 2 класса);\n",
    "\n",
    "\n",
    "**Шаг 2.** Вычислить логарифм отношения шансов: $ \\log(OR_{+}) =  \\textbf{w}^\\text{T}\\textbf{x}$.\n",
    "\n",
    "**Шаг 3.** Имея прогноз шансов на отнесение к классу \"+\" – $OR_{+}$, вычислить $p_{+}$ с помощью простой зависимости:\n",
    "\n",
    "$$\\Large p_{+} = \\frac{OR_{+}}{1 + OR_{+}} = \\frac{\\exp^{\\textbf{w}^\\text{T}\\textbf{x}}}{1 + \\exp^{\\textbf{w}^\\text{T}\\textbf{x}}} =  \\frac{1}{1 + \\exp^{-\\textbf{w}^\\text{T}\\textbf{x}}} = \\sigma(\\textbf{w}^\\text{T}\\textbf{x})$$\n",
    "\n",
    "\n",
    "В правой части мы получили как раз сигмоид-функцию.\n",
    "\n",
    "Итак, логистическая регрессия прогнозирует вероятность отнесения примера к классу \"+\" (при условии, что мы знаем его признаки и веса модели) как сигмоид-преобразование линейной комбинации вектора весов модели и вектора признаков примера:\n",
    "\n",
    "$$\\Large p_+(x_i) = \\text{P}\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\sigma(\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}). $$\n",
    "\n",
    "Следующий вопрос: как модель обучается. Тут мы опять обращаемся к принципу максимального правдоподобия."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Принцип максимального правдоподобия и логистическая регрессия\n",
    "Теперь посмотрим, как из принципа максимального правдоподобия получается оптимизационная задача, которую решает логистическая регрессия, а именно, – минимизация *логистической* функции потерь. \n",
    "Только что мы увидели, что логистическая регрессия моделирует вероятность отнесения примера к классу \"+\" как \n",
    "\n",
    "$$\\Large p_+(\\textbf{x}_\\text{i}) = \\text{P}\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\sigma(\\textbf{w}^\\text{T}\\textbf{x}_\\text{i})$$\n",
    "\n",
    "Тогда для класса \"-\" аналогичная вероятность:\n",
    "$$\\Large p_-(\\textbf{x}_\\text{i})  = \\text{P}\\left(y_i = -1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right)  = 1 - \\sigma(\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}) = \\sigma(-\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}) $$\n",
    "\n",
    "Оба этих выражения можно ловко объединить в одно (следите за моими руками – не обманывают ли вас):\n",
    "\n",
    "$$\\Large \\text{P}\\left(y = y_i \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\sigma(y_i\\textbf{w}^\\text{T}\\textbf{x}_\\text{i})$$\n",
    "\n",
    "Выражение $M(\\textbf{x}_\\text{i}) = y_i\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}$ называется *отступом* (*margin*) классификации на объекте $\\textbf{x}_\\text{i}$ (не путать с зазором (тоже margin), про который чаще всего говорят в контексте SVM). Если он неотрицателен, модель не ошибается на объекте $\\textbf{x}_\\text{i}$, если же отрицателен – значит, класс для $\\textbf{x}_\\text{i}$  спрогнозирован неправильно. \n",
    "Заметим, что отступ определен для объектов именно обучающей выборки, для которых известны реальные метки целевого класса $y_i$.\n",
    "\n",
    "Чтобы понять, почему это мы сделали такие выводы, обратимся к геометрической интерпретации линейного классификатора. Подробно про это можно почитать в материалах Евгения Соколова – [тут](https://github.com/esokolov/ml-course-msu/blob/master/ML16/lecture-notes/Sem09_linear.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Рекомендую решить почти классическую задачу из начального курса линейной алгебры: найти расстояние от точки с радиус-вектором $\\textbf{x}_A$ до плоскости, которая задается уравнением $\\textbf{w}^\\text{T}\\textbf{x} = 0.$\n",
    "\n",
    "\n",
    "Ответ: \n",
    "$\\Large \\rho(\\textbf{x}_A, \\textbf{w}^\\text{T}\\textbf{x} = 0) = \\frac{\\textbf{w}^\\text{T}\\textbf{x}_A}{||\\textbf{w}||}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = '../../img/simple_linal_task.png' width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда получим (или посмотрим) ответ, то поймем, что чем больше по модулю выражение $\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}$, тем дальше точка $\\textbf{x}_\\text{i}$ находится от плоскости $\\textbf{w}^{\\text{T}}\\textbf{x} = 0.$\n",
    "\n",
    "Значит, выражение $M(\\textbf{x}_\\text{i}) = y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}$ – это своего рода \"уверенность\" модели в классификации объекта $\\textbf{x}_\\text{i}$: \n",
    "\n",
    "- если отступ большой (по модулю) и положительный, это значит, что метка класса поставлена правильно, а объект находится далеко от разделяющей гиперплоскости (такой объект классифицируется уверенно). На рисунке – $x_3$.\n",
    "- если отступ большой (по модулю) и отрицательный, значит метка класса поставлена неправильно, а объект находится далеко от разделюящей гиперплоскости (скорее всего такой объект – аномалия, например, его метка в обучающей выборке поставлена неправильно). На рисунке – $x_1$.\n",
    "- если отступ малый (по модулю), то объект находится близко к разделюящей гиперплоскости, а  знак отступа определяет, правильно ли объект классифицирован.  На рисунке – $x_2$ и $x_4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = '../../img/margin.png' width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь распишем правдоподобие выборки, а именно, вероятность наблюдать данный вектор $\\textbf{y}$ у выборки $\\textbf X$. Делаем сильное предположение: объекты приходят независимо, из одного распределения (*i.i.d.*). Тогда\n",
    "\n",
    "$$\\Large \\text{P}\\left(\\textbf{y} \\mid \\textbf X, \\textbf{w}\\right) = \\prod_{i=1}^{\\ell} \\text{P}\\left(y = y_i \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right),$$\n",
    "\n",
    "где $\\ell$ – длина выборки $\\textbf X$ (число строк).\n",
    "\n",
    "Как водится, возьмем логарифм данного выражения (сумму оптимизировать намного проще, чем произведение):\n",
    "\n",
    "$$\\Large  \\log \\text{P}\\left(\\textbf{y} \\mid \\textbf X, \\textbf{w}\\right) = \\log \\sum_{i=1}^{\\ell} \\text{P}\\left(y = y_i \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\log \\prod_{i=1}^{\\ell} \\sigma(y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i})   = $$\n",
    "\n",
    "$$\\Large  = \\sum_{i=1}^{\\ell} \\log \\sigma(y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}) = \\sum_{i=1}^{\\ell} \\log \\frac{1}{1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}}} = - \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть в данном случае принцип максимизации правдоподобия приводит к минимизации выражения \n",
    "\n",
    "$$\\Large \\mathcal{L_{log}} (\\textbf X, \\textbf{y}, \\textbf{w}) = \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}}).$$\n",
    "\n",
    "Это *логистическая* функция потерь, просуммированная по всем объектам обучающей выборки.\n",
    "\n",
    "Посмотрим на новую фунцию как на функцию от отступа: $L(M) = \\log (1 + \\exp^{-M})$. Нарисуем ее график, а также график 1/0 функциий потерь (*zero-one loss*), которая просто штрафует модель на 1 за ошибку на каждом объекте (отступ отрицательный): $L_{1/0}(M) = [M < 0]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = '../../img/logloss_margin.png' width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Картинка отражает общую идею, что в задаче классификации, не умея напрямую минимизировать число ошибок (по крайней мере, градиентными методами это не сделать – производная 1/0 функциий потерь в нуле обращается в бесконечность), мы минимизируем некоторую ее верхнюю оценку. В данном случае это логистическая функция потерь (где логарифм двоичный, но это не принципиально), и справедливо \n",
    "\n",
    "$$\\Large \\mathcal{L_{\\text{1/0}}} (\\textbf X, \\textbf{y}, \\textbf{w}) = \\sum_{i=1}^{\\ell} [M(\\textbf{x}_\\text{i}) < 0] \\leq \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}}) = \\mathcal{L_{\\log}} (\\textbf X, \\textbf{y}, \\textbf{w}), $$\n",
    "\n",
    "где $\\mathcal{L_{\\text{1/0}}} (\\textbf X, \\textbf{y}, \\textbf{w})$ – попросту число ошибок логистической регрессии с весами $\\textbf{w}$ на выборке $(\\textbf X, \\textbf{y})$.\n",
    "\n",
    "То есть уменьшая верхнюю оценку $\\mathcal{L_{\\log}}$ на число ошибок классификации, мы таким образом надеемся уменьшить и само число ошибок."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2-регуляризация логистической функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L2-регуляризация$ логистической регрессии устроена почти так же, как и в случае с гребневой (Ridge регрессией). Вместо функционала $\\mathcal{L_{\\log}} (X, \\textbf{y}, \\textbf{w})$ минимизируется следующий:\n",
    "\n",
    "$$\\Large J(\\textbf X, \\textbf{y}, \\textbf{w}) = \\mathcal{L_{\\log}} (\\textbf X, \\textbf{y}, \\textbf{w}) + \\lambda |\\textbf{w}|^2$$\n",
    "\n",
    "В случае логистической регрессии принято введение обратного коэффициента регуляризации $C = \\frac{1}{\\lambda}$. И тогда решением задачи будет\n",
    "\n",
    "$$\\Large \\widehat{\\textbf{w}}  = \\arg \\min_{\\textbf{w}} J(\\textbf X, \\textbf{y}, \\textbf{w}) =  \\arg \\min_{\\textbf{w}}\\ (C\\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}})+ |\\textbf{w}|^2)$$ \n",
    "\n",
    "Далее рассмотрим пример, позволяющий интуитивно понять один из смыслов регуляризации. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
