{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\">\n",
    "## Открытый курс по машинному обучению\n",
    "</center>\n",
    "Автор материала: программист-исследователь Mail.ru Group, старший преподаватель <br>Факультета Компьютерных Наук ВШЭ Юрий Кашницкий. Материал распространяется на условиях лицензии [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Можно использовать в любых целях (редактировать, поправлять и брать за основу), кроме коммерческих, но с обязательным упоминанием автора материала."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Тема 4. Линейные модели классификации и регрессии\n",
    "## <center>Часть 2. Логистическая регрессия и метод максимального правдоподобия "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Линейный классификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основная идея линейного классификатора заключается в том, что признаковое пространство может быть разделено гиперплоскостью на две полуплоскости, в каждой из которых прогнозируется одно из двух значений целевого класса. \n",
    "Если это можно сделать без ошибок, то обучающая выборка называется *линейно разделимой*.\n",
    "\n",
    "<img src=\"../../img/logit.png\">\n",
    "\n",
    "Мы уже знакомы с линейной регрессией и методом наименьших квадратов. Рассмотрим задачу бинарной классификации, причем метки целевого класса обозначим \"+1\" (положительные примеры) и \"-1\" (отрицательные примеры).\n",
    "Один из самых простых линейных классификаторов получается на основе регрессии вот таким образом:\n",
    "\n",
    "$$a(\\vec{x}) = sign(\\vec{w}^Tx),$$\n",
    "\n",
    "где\n",
    " - $\\vec{x}$ – вектор признаков примера (вместе с единицей);\n",
    " - $\\vec{w}$ – веса в линейной модели (вместе со смещением $w_0$);\n",
    " - $sign(\\bullet)$ – функция \"сигнум\", возвращающая знак своего аргумента;\n",
    " - $a(\\vec{x})$ – ответ классификатора на примере $\\vec{x}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Логистическая регрессия как линейный классификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия является частным случаем линейного классификатора, но она обладает хорошим \"умением\" – прогнозировать вероятность $p_+$ отнесения примера $\\vec{x_i}$ к классу \"+\":\n",
    "$$p_+ = P\\left(y_i = 1 \\mid \\vec{x_i}, \\vec{w}\\right) $$\n",
    "\n",
    "Прогнозирование не просто ответа (\"+1\" или \"-1\"), а именно *вероятности* отнесения к классу \"+1\" во многих задачах является очень важным бизнес-требованием. Например, в задаче кредитного скоринга, где традиционно применяется логистическая регрессия, часто прогнозируют вероятность невозврата кредита ($p_+$). Клиентов, обратившихся за кредитом, сортируют по этой предсказанной вероятности (по убыванию), и получается скоркарта — по сути, рейтинг клиентов от плохих к хорошим. Ниже приведен игрушечный пример такой скоркарты. \n",
    "    <img src='../../img/toy_scorecard.png' width=60%>\n",
    "\n",
    "Банк выбирает для себя порог $p_*$ предсказанной вероятности невозврата кредита (на картинке – $0.15$) и начиная с этого значения уже не выдает кредит. Более того, можно умножить предсказнную вероятность на выданную сумму и получить матожидание потерь с клиента, что тоже будет хорошей бизнес-метрикой (*Далее в комментариях специалисты по скорингу могут поправить, но главная суть примерно такая*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, мы хотим прогнозировать вероятность $p_+ \\in [0,1]$, а пока умеем строить линейный прогноз с помощью МНК: $b(\\vec{x}) = \\vec{w}^T \\vec{x} \\in \\mathbb{R}$. Каким образом преобразовать полученное значение в вероятность, пределы которой – [0, 1]? Очевидно, для этого нужна некоторая функция $f: \\mathbb{R} \\rightarrow [0,1].$ В модели логистической регрессии для этого берется конкретная функция: $\\sigma(z) = \\frac{1}{1 + \\exp^{-z}}$. И сейчас разберемся, каковы для этого предпосылки. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "# отключим всякие предупреждения Anaconda\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 5,
>>>>>>> 5ab016c1dde955f61ab39bc1daf935f455189dc0
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigma(z):\n",
    "    return 1. / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 9,
>>>>>>> 5ab016c1dde955f61ab39bc1daf935f455189dc0
   "metadata": {},
   "outputs": [
    {
     "data": {
<<<<<<< HEAD
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4780a83518>"
=======
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAETCAYAAAA/NdFSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113cc4518>"
>>>>>>> 5ab016c1dde955f61ab39bc1daf935f455189dc0
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xx = np.linspace(-10, 10, 1000)\n",
    "plt.plot(xx, [sigma(x) for x in xx]);\n",
    "plt.xlabel('z');\n",
    "plt.ylabel('sigmoid(z)')\n",
    "plt.title('Sigmoid function');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обозначим $P(X)$ вероятностью происходящего события $X$. Тогда отношение вероятностей $OR(X)$ определяется из $\\frac{P(X)}{1-P(X)}$, а это — отношение вероятностей того, произойдет ли событие или не произойдет. Очевидно, что вероятность и отношение шансов содержат одинаковую информацию. Но в то время как $P(X)$ находится в пределах от 0 до 1, $OR(X)$ находится в пределах от 0 до $\\infty$.\n",
    "\n",
    "Если вычислить логарифм $OR(X)$ (то есть называется логарифм шансов, или логарифм отношения вероятностей), то легко заметить, что $\\log{OR(X)} \\in \\mathbb{R}$. Его то мы и будем прогнозировать с помощью МНК.\n",
    "\n",
    "Посмотрим, как логистическая регрессия будет делать прогноз $p_+ = P\\left(y_i = 1 \\mid \\vec{x_i}, \\vec{w}\\right)$ (пока считаем, что веса $\\vec{w}$ мы как-то получили (т.е. обучили модель), далее разберемся, как именно). \n",
    "\n",
    "**Шаг 1.** Вычислить значение $w_{0}+w_{1}x_1 + w_{2}x_2 + ... = \\vec{w}^T\\vec{x}$. (уравнение $\\vec{w}^T\\vec{x} = 0$ задает гиперплоскость, разделяющую примеры на 2 класса);\n",
    "\n",
    "\n",
    "**Шаг 2.** Вычислить логарифм отношения шансов: $ \\log(OR_{+}) =  \\vec{w}^T\\vec{x}$.\n",
    "\n",
    "**Шаг 3.** Имея прогноз шансов на отнесение к классу \"+\" – $OR_{+}$, вычислить $p_{+}$ с помощью простой зависимости:\n",
    "\n",
    "$\\Large p_{+} = \\frac{OR_{+}}{1 + OR_{+}} = \\frac{\\exp^{\\vec{w}^T\\vec{x}}}{1 + \\exp^{\\vec{w}^T\\vec{x}}} =  \\frac{1}{1 + \\exp^{-\\vec{w}^T\\vec{x}}} = \\sigma(\\vec{w}^T\\vec{x})$\n",
    "\n",
    "\n",
    "В правой части мы получили как раз сигмоид-функцию.\n",
    "\n",
    "Итак, логистическая регрессия прогнозирует вероятность отнесения примера к классу \"+\" (при условии, что мы знаем его признаки и веса модели) как сигмоид-преобразование линейной комбинации вектора весов модели и вектора признаков примера:\n",
    "\n",
    "$$p_+(x_i) = P\\left(y_i = 1 \\mid \\vec{x_i}, \\vec{w}\\right) = \\sigma(\\vec{w}^T\\vec{x_i}). $$\n",
    "\n",
    "Следующий вопрос: как модель обучается. Тут мы опять обращаемся к приницпу максимального правдоподобия."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Принцип максимального правдоподобия и логистическая регрессия\n",
    "Теперь посмотрим, как из принципа максимального правдоподобия получается оптимизационная задача, которую решает логистическая регрессия, а именно, – минимизация *логистической* функции потерь. \n",
    "Только что мы увидели, что логистическая регрессия моделирует вероятность отнесения примера к классу \"+\" как \n",
    "\n",
    "$$p_+(\\vec{x_i}) = P\\left(y_i = 1 \\mid \\vec{x_i}, \\vec{w}\\right) = \\sigma(\\vec{w}^T\\vec{x_i})$$\n",
    "\n",
    "Тогда для класса \"-\" аналогичная вероятность:\n",
    "$$p_-(\\vec{x_i})  = P\\left(y_i = -1 \\mid \\vec{x_i}, \\vec{w}\\right)  = 1 - \\sigma(\\vec{w}^T\\vec{x_i}) = \\sigma(-\\vec{w}^T\\vec{x_i}) $$\n",
    "\n",
    "Оба этих выражения можно ловко объединить в одно (следите за моими руками – не обманывают ли вас):\n",
    "\n",
    "$$P\\left(y = y_i \\mid \\vec{x_i}, \\vec{w}\\right) = \\sigma(y_i\\vec{w}^T\\vec{x_i})$$\n",
    "\n",
    "Выражение $M(\\vec{x_i}) = y_i\\vec{w}^T\\vec{x_i}$ называется *отступом* (*margin*) классификации на объекте $\\vec{x_i}$ (не путать с зазором (тоже margin), про который чаще всего говорят в контексте SVM). Если он неотрицателен, модель не ошибается на объекте $\\vec{x_i}$, если же отрицателен – значит, класс для $\\vec{x_i}$  спрогнозирован неправильно. \n",
    "Заметим, что отступ определен для объектов именно обучающей выборки, для которых известны реальные метки целевого класса $y_i$.\n",
    "\n",
    "Чтобы понять, почему это мы сделали такие выводы, обратимся к геометрической интерпретации линейного классификатора. Подробно про это можно почитать в материалах Евгения Соколова – [тут](https://github.com/esokolov/ml-course-msu/blob/master/ML16/lecture-notes/Sem09_linear.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Рекомендую решить почти классическую задачу из начального курса линейной алгебры: найти расстояние от точки с радиус-вектором $\\vec{x_A}$ до плоскости, которая задается уравнением $\\vec{w}^T\\vec{x} = 0.$\n",
    "\n",
    "\n",
    "<spoiler title='Ответ'>\n",
    "$\\rho(\\vec{x_A}, \\vec{w}^T\\vec{x} = 0) = \\frac{\\vec{w}^T\\vec{x_A}}{||\\vec{w}||}$\n",
    "</spoiler>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = '../../img/simple_linal_task.png' width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда получим (или посмотрим) ответ, то поймем, что чем больше по модулю выражение $\\vec{w}^T\\vec{x_i}$, тем дальше точка $\\vec{x_i}$ находится от плоскости $\\vec{w}^T\\vec{x} = 0.$\n",
    "\n",
    "Значит, выражение $M(\\vec{x_i}) = y_i\\vec{w}^T\\vec{x_i}$ – это своего рода \"уверенность\" модели в классификации объекта $\\vec{x_i}$: \n",
    "\n",
    "- если отступ большой (по модулю) и положительный, это значит, что метка класса поставлена правильно, а объект находится далеко от разделяюящей гиперплоскости (такой объект классифицируется уверенно). На рисунке – $x_3$.\n",
    "- если отступ большой (по модулю) и отрицательный, значит метка класса поставлена неправильно, а объект находится далеко от разделюящей гиперплоскости (скорее всего такой объект – аномалия, например, его метка в обучающей выборке поставлена неправильно). На рисунке – $x_1$.\n",
    "- если отступ малый (по модулю), то объект находится близко к разделюящей гиперплоскости, а  знак отступа определяет, правильно ли объект классифицирован.  На рисунке – $x_2$ и $x_4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = '../../img/margin.png' width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь распишем правдоподобие выборки, а именно, вероятность наблюдать данный вектор $\\vec{y}$ у выборки $X$. Делаем сильное предположение: объекты приходят независимо, из одного распределения (*i.i.d.*). Тогда\n",
    "\n",
    "$$P\\left(\\vec{y} \\mid X, \\vec{w}\\right) = \\prod_{i=1}^{\\ell} P\\left(y = y_i \\mid \\vec{x_i}, \\vec{w}\\right),$$\n",
    "\n",
    "где $\\ell$ – длина выборки $X$ (число строк).\n",
    "\n",
    "Как водится, возьмем логарифм данного выражения (сумму оптимизировать намного проще, чем произведение):\n",
    "\n",
    "$$\\log P\\left(\\vec{y} \\mid X, \\vec{w}\\right) = \\log \\prod_{i=1}^{\\ell} P\\left(y = y_i \\mid \\vec{x_i}, \\vec{w}\\right) = \\log \\prod_{i=1}^{\\ell} \\sigma(y_i\\vec{w}^T\\vec{x_i})   = $$\n",
    "\n",
    "$$ = \\sum_{i=1}^{\\ell} \\log \\sigma(y_i\\vec{w}^T\\vec{x_i}) = \\sum_{i=1}^{\\ell} \\log \\frac{1}{1 + \\exp^{-y_i\\vec{w}^T\\vec{x_i}}} = - \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\vec{w}^T\\vec{x_i}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть в даном случае принцип максимизации правдоподобия приводит к минимизации выражения \n",
    "\n",
    "$$\\mathcal{L_{log}} (X, \\vec{y}, \\vec{w}) = \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\vec{w}^T\\vec{x_i}}).$$\n",
    "\n",
    "Это *логистическая* функция потерь, просуммированная по всем объектам обучающей выборки.\n",
    "\n",
    "Посмотрим на новую фунцию как на функцию от отступа: $L(M) = \\log (1 + \\exp^{-M})$. Нарисуем ее график, а также график 1/0 функциий потерь (*zero-one loss*), которая просто штрафует модель на 1 за ошибку на каждом объекте (отступ отрицательный): $L_{1/0}(M) = [M < 0]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = '../../img/logloss_margin.png' width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Картинка отражает общую идею, что в задаче классификации, не умея напрямую минимизировать число ошибок (по крайней мере, градиентными методами это не сделать – производная 1/0 функциий потерь в нуле обращается в бесконечность), мы минимизируем некоторую ее верхнюю оценку. В данном случае это логистическая функция потерь (где логарифм двоичный, но это не принципиально), и справедливо \n",
    "\n",
    "$$\\mathcal{L_{1/0}} (X, \\vec{y}, \\vec{w}) = \\sum_{i=1}^{\\ell} [M(\\vec{x_i}) < 0] \\leq \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\vec{w}^T\\vec{x_i}}) = \\mathcal{L_{log}} (X, \\vec{y}, \\vec{w}), $$\n",
    "\n",
    "где $\\mathcal{L_{1/0}} (X, \\vec{y}, \\vec{w})$ – попросту число ошибок логистической регрессии с весами $\\vec{w}$ на выборке $(X, \\vec{y})$.\n",
    "\n",
    "То есть уменьшая верхнюю оценку $\\mathcal{L_{log}}$ на число ошибок классификации, мы таким образом надеемся уменьшить и само число ошибок."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2-регуляризация логистической функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L2-регуляризация$ логистической регрессии устроена почти так же, как и в случае с гребневой (Ridge регрессией). Вместо функционала $\\mathcal{L_{log}} (X, \\vec{y}, \\vec{w})$ минимизируется следующий:\n",
    "\n",
    "$$J(X, \\vec{y}, \\vec{w}) = \\mathcal{L_{log}} (X, \\vec{y}, \\vec{w}) + \\lambda |\\vec{w}|^2$$\n",
    "\n",
    "В случае логистической регрессии принято введение обратного коэффициента регуляризации $C = \\frac{1}{\\lambda}$. И тогда решением задачи будет\n",
    "\n",
    "$$\\hat{w}  = \\arg \\min_{\\vec{w}} J(X, \\vec{y}, \\vec{w}) =  \\arg \\min_{\\vec{w}}\\ (C\\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\vec{w}^T\\vec{x_i}})+ |\\vec{w}|^2)$$ \n",
    "\n",
    "Далее рассмотрим пример, позволяющий интуитивно понять один из смыслов регуляризации. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.6.3"
=======
   "version": "3.6.1"
>>>>>>> 5ab016c1dde955f61ab39bc1daf935f455189dc0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
