{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\">\n",
    "## Открытый курс по машинному обучению\n",
    "</center>\n",
    "Автор материала: программист-исследователь Mail.ru Group, старший преподаватель <br>Факультета Компьютерных Наук ВШЭ Юрий Кашницкий. Материал распространяется на условиях лицензии [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Можно использовать в любых целях (редактировать, поправлять и брать за основу), кроме коммерческих, но с обязательным упоминанием автора материала."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Тема 4. Линейные модели классификации и регрессии\n",
    "## <center>Часть 2. Логистическая регрессия и метод максимального правдоподобия "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Линейный классификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основная идея линейного классификатора заключается в том, что признаковое пространство может быть разделено гиперплоскостью на две полуплоскости, в каждой из которых прогнозируется одно из двух значений целевого класса. \n",
    "Если это можно сделать без ошибок, то обучающая выборка называется *линейно разделимой*.\n",
    "\n",
    "<img src=\"../../img/logit.png\">\n",
    "\n",
    "Мы уже знакомы с линейной регрессией и методом наименьших квадратов. Рассмотрим задачу бинарной классификации, причем метки целевого класса обозначим \"+1\" (положительные примеры) и \"-1\" (отрицательные примеры).\n",
    "Один из самых простых линейных классификаторов получается на основе регрессии вот таким образом:\n",
    "\n",
    "$$a(\\vec{x}) = sign(\\vec{w}^Tx),$$\n",
    "\n",
    "где\n",
    " - $\\vec{x}$ – вектор признаков примера (вместе с единицей);\n",
    " - $\\vec{w}$ – веса в линейной модели (вместе со смещением $w_0$);\n",
    " - $sign(\\bullet)$ – функция \"сигнум\", возвращающая знак своего аргумента;\n",
    " - $a(\\vec{x})$ – ответ классификатора на примере $\\vec{x}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Логистическая регрессия как линейный классификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия является частным случаем линейного классификатора, но она обладает хорошим \"умением\" – прогнозировать вероятность $p_+$ отнесения примера $\\vec{x_i}$ к классу \"+\":\n",
    "$$p_+ = P\\left(y_i = 1 \\mid \\vec{x_i}, \\vec{w}\\right) $$\n",
    "\n",
    "Прогнозирование не просто ответа (\"+1\" или \"-1\"), а именно *вероятности* отнесения к классу \"+1\" во многих задачах является очень важным бизнес-требованием. Например, в задаче кредитного скоринга, где традиционно применяется логистическая регрессия, часто прогнозируют вероятность невозврата кредита ($p_+$). Клиентов, обратившихся за кредитом, сортируют по этой предсказанной вероятности (по убыванию), и получается скоркарта — по сути, рейтинг клиентов от плохих к хорошим. Ниже приведен игрушечный пример такой скоркарты. \n",
    "    <img src='../../img/toy_scorecard.png' width=60%>\n",
    "\n",
    "Банк выбирает для себя порог $p_*$ предсказанной вероятности невозврата кредита (на картинке – $0.15$) и начиная с этого значения уже не выдает кредит. Более того, можно умножить предсказнную вероятность на выданную сумму и получить матожидание потерь с клиента, что тоже будет хорошей бизнес-метрикой (*Далее в комментариях специалисты по скорингу могут поправить, но главная суть примерно такая*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, мы хотим прогнозировать вероятность $p_+ \\in [0,1]$, а пока умеем строить линейный прогноз с помощью МНК: $b(\\vec{x}) = \\vec{w}^T \\vec{x} \\in \\mathbb{R}$. Каким образом преобразовать полученное значение в вероятность, пределы которой – [0, 1]? Очевидно, для этого нужна некоторая функция $f: \\mathbb{R} \\rightarrow [0,1].$ В модели логистической регрессии для этого берется конкретная функция: $\\sigma(z) = \\frac{1}{1 + \\exp^{-z}}$. И сейчас разберемся, каковы для этого предпосылки. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "# отключим всякие предупреждения Anaconda\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigma(z):\n",
    "    return 1. / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAETCAYAAAA/NdFSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113cc4518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xx = np.linspace(-10, 10, 1000)\n",
    "plt.plot(xx, [sigma(x) for x in xx]);\n",
    "plt.xlabel('z');\n",
    "plt.ylabel('sigmoid(z)')\n",
    "plt.title('Sigmoid function');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обозначим $P(X)$ вероятностью происходящего события $X$. Тогда отношение вероятностей $OR(X)$ определяется из $\\frac{P(X)}{1-P(X)}$, а это — отношение вероятностей того, произойдет ли событие или не произойдет. Очевидно, что вероятность и отношение шансов содержат одинаковую информацию. Но в то время как $P(X)$ находится в пределах от 0 до 1, $OR(X)$ находится в пределах от 0 до $\\infty$.\n",
    "\n",
    "Если вычислить логарифм $OR(X)$ (то есть называется логарифм шансов, или логарифм отношения вероятностей), то легко заметить, что $\\log{OR(X)} \\in \\mathbb{R}$. Его то мы и будем прогнозировать с помощью МНК.\n",
    "\n",
    "Посмотрим, как логистическая регрессия будет делать прогноз $p_+ = P\\left(y_i = 1 \\mid \\vec{x_i}, \\vec{w}\\right)$ (пока считаем, что веса $\\vec{w}$ мы как-то получили (т.е. обучили модель), далее разберемся, как именно). \n",
    "\n",
    "**Шаг 1.** Вычислить значение $w_{0}+w_{1}x_1 + w_{2}x_2 + ... = \\vec{w}^T\\vec{x}$. (уравнение $\\vec{w}^T\\vec{x} = 0$ задает гиперплоскость, разделяющую примеры на 2 класса);\n",
    "\n",
    "\n",
    "**Шаг 2.** Вычислить логарифм отношения шансов: $ \\log(OR_{+}) =  \\vec{w}^T\\vec{x}$.\n",
    "\n",
    "**Шаг 3.** Имея прогноз шансов на отнесение к классу \"+\" – $OR_{+}$, вычислить $p_{+}$ с помощью простой зависимости:\n",
    "\n",
    "$\\Large p_{+} = \\frac{OR_{+}}{1 + OR_{+}} = \\frac{\\exp^{\\vec{w}^T\\vec{x}}}{1 + \\exp^{\\vec{w}^T\\vec{x}}} =  \\frac{1}{1 + \\exp^{-\\vec{w}^T\\vec{x}}} = \\sigma(\\vec{w}^T\\vec{x})$\n",
    "\n",
    "\n",
    "В правой части мы получили как раз сигмоид-функцию.\n",
    "\n",
    "Итак, логистическая регрессия прогнозирует вероятность отнесения примера к классу \"+\" (при условии, что мы знаем его признаки и веса модели) как сигмоид-преобразование линейной комбинации вектора весов модели и вектора признаков примера:\n",
    "\n",
    "$$p_+(x_i) = P\\left(y_i = 1 \\mid \\vec{x_i}, \\vec{w}\\right) = \\sigma(\\vec{w}^T\\vec{x_i}). $$\n",
    "\n",
    "Следующий вопрос: как модель обучается. Тут мы опять обращаемся к приницпу максимального правдоподобия."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Принцип максимального правдоподобия и логистическая регрессия\n",
    "Теперь посмотрим, как из принципа максимального правдоподобия получается оптимизационная задача, которую решает логистическая регрессия, а именно, – минимизация *логистической* функции потерь. \n",
    "Только что мы увидели, что логистическая регрессия моделирует вероятность отнесения примера к классу \"+\" как \n",
    "\n",
    "$$p_+(\\vec{x_i}) = P\\left(y_i = 1 \\mid \\vec{x_i}, \\vec{w}\\right) = \\sigma(\\vec{w}^T\\vec{x_i})$$\n",
    "\n",
    "Тогда для класса \"-\" аналогичная вероятность:\n",
    "$$p_-(\\vec{x_i})  = P\\left(y_i = -1 \\mid \\vec{x_i}, \\vec{w}\\right)  = 1 - \\sigma(\\vec{w}^T\\vec{x_i}) = \\sigma(-\\vec{w}^T\\vec{x_i}) $$\n",
    "\n",
    "Оба этих выражения можно ловко объединить в одно (следите за моими руками – не обманывают ли вас):\n",
    "\n",
    "$$P\\left(y = y_i \\mid \\vec{x_i}, \\vec{w}\\right) = \\sigma(y_i\\vec{w}^T\\vec{x_i})$$\n",
    "\n",
    "Выражение $M(\\vec{x_i}) = y_i\\vec{w}^T\\vec{x_i}$ называется *отступом* (*margin*) классификации на объекте $\\vec{x_i}$ (не путать с зазором (тоже margin), про который чаще всего говорят в контексте SVM). Если он неотрицателен, модель не ошибается на объекте $\\vec{x_i}$, если же отрицателен – значит, класс для $\\vec{x_i}$  спрогнозирован неправильно. \n",
    "Заметим, что отступ определен для объектов именно обучающей выборки, для которых известны реальные метки целевого класса $y_i$.\n",
    "\n",
    "Чтобы понять, почему это мы сделали такие выводы, обратимся к геометрической интерпретации линейного классификатора. Подробно про это можно почитать в материалах Евгения Соколова – [тут](https://github.com/esokolov/ml-course-msu/blob/master/ML16/lecture-notes/Sem09_linear.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Рекомендую решить почти классическую задачу из начального курса линейной алгебры: найти расстояние от точки с радиус-вектором $\\vec{x_A}$ до плоскости, которая задается уравнением $\\vec{w}^T\\vec{x} = 0.$\n",
    "\n",
    "\n",
    "<spoiler title='Ответ'>\n",
    "$\\rho(\\vec{x_A}, \\vec{w}^T\\vec{x} = 0) = \\frac{\\vec{w}^T\\vec{x_A}}{||\\vec{w}||}$\n",
    "</spoiler>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = '../../img/simple_linal_task.png' width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда получим (или посмотрим) ответ, то поймем, что чем больше по модулю выражение $\\vec{w}^T\\vec{x_i}$, тем дальше точка $\\vec{x_i}$ находится от плоскости $\\vec{w}^T\\vec{x} = 0.$\n",
    "\n",
    "Значит, выражение $M(\\vec{x_i}) = y_i\\vec{w}^T\\vec{x_i}$ – это своего рода \"уверенность\" модели в классификации объекта $\\vec{x_i}$: \n",
    "\n",
    "- если отступ большой (по модулю) и положительный, это значит, что метка класса поставлена правильно, а объект находится далеко от разделяюящей гиперплоскости (такой объект классифицируется уверенно). На рисунке – $x_3$.\n",
    "- если отступ большой (по модулю) и отрицательный, значит метка класса поставлена неправильно, а объект находится далеко от разделюящей гиперплоскости (скорее всего такой объект – аномалия, например, его метка в обучающей выборке поставлена неправильно). На рисунке – $x_1$.\n",
    "- если отступ малый (по модулю), то объект находится близко к разделюящей гиперплоскости, а  знак отступа определяет, правильно ли объект классифицирован.  На рисунке – $x_2$ и $x_4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = '../../img/margin.png' width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь распишем правдоподобие выборки, а именно, вероятность наблюдать данный вектор $\\vec{y}$ у выборки $X$. Делаем сильное предположение: объекты приходят независимо, из одного распределения (*i.i.d.*). Тогда\n",
    "\n",
    "$$P\\left(\\vec{y} \\mid X, \\vec{w}\\right) = \\prod_{i=1}^{\\ell} P\\left(y = y_i \\mid \\vec{x_i}, \\vec{w}\\right),$$\n",
    "\n",
    "где $\\ell$ – длина выборки $X$ (число строк).\n",
    "\n",
    "Как водится, возьмем логарифм данного выражения (сумму оптимизировать намного проще, чем произведение):\n",
    "\n",
    "$$\\log P\\left(\\vec{y} \\mid X, \\vec{w}\\right) = \\log \\prod_{i=1}^{\\ell} P\\left(y = y_i \\mid \\vec{x_i}, \\vec{w}\\right) = \\log \\prod_{i=1}^{\\ell} \\sigma(y_i\\vec{w}^T\\vec{x_i})   = $$\n",
    "\n",
    "$$ = \\sum_{i=1}^{\\ell} \\log \\sigma(y_i\\vec{w}^T\\vec{x_i}) = \\sum_{i=1}^{\\ell} \\log \\frac{1}{1 + \\exp^{-y_i\\vec{w}^T\\vec{x_i}}} = - \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\vec{w}^T\\vec{x_i}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть в даном случае принцип максимизации правдоподобия приводит к минимизации выражения \n",
    "\n",
    "$$\\mathcal{L_{log}} (X, \\vec{y}, \\vec{w}) = \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\vec{w}^T\\vec{x_i}}).$$\n",
    "\n",
    "Это *логистическая* функция потерь, просуммированная по всем объектам обучающей выборки.\n",
    "\n",
    "Посмотрим на новую фунцию как на функцию от отступа: $L(M) = \\log (1 + \\exp^{-M})$. Нарисуем ее график, а также график 1/0 функциий потерь (*zero-one loss*), которая просто штрафует модель на 1 за ошибку на каждом объекте (отступ отрицательный): $L_{1/0}(M) = [M < 0]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = '../../img/logloss_margin.png' width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Картинка отражает общую идею, что в задаче классификации, не умея напрямую минимизировать число ошибок (по крайней мере, градиентными методами это не сделать – производная 1/0 функциий потерь в нуле обращается в бесконечность), мы минимизируем некоторую ее верхнюю оценку. В данном случае это логистическая функция потерь (где логарифм двоичный, но это не принципиально), и справедливо \n",
    "\n",
    "$$\\mathcal{L_{1/0}} (X, \\vec{y}, \\vec{w}) = \\sum_{i=1}^{\\ell} [M(\\vec{x_i}) < 0] \\leq \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\vec{w}^T\\vec{x_i}}) = \\mathcal{L_{log}} (X, \\vec{y}, \\vec{w}), $$\n",
    "\n",
    "где $\\mathcal{L_{1/0}} (X, \\vec{y}, \\vec{w})$ – попросту число ошибок логистической регрессии с весами $\\vec{w}$ на выборке $(X, \\vec{y})$.\n",
    "\n",
    "То есть уменьшая верхнюю оценку $\\mathcal{L_{log}}$ на число ошибок классификации, мы таким образом надеемся уменьшить и само число ошибок."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2-регуляризация логистической функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L2-регуляризация$ логистической регрессии устроена почти так же, как и в случае с гребневой (Ridge регрессией). Вместо функционала $\\mathcal{L_{log}} (X, \\vec{y}, \\vec{w})$ минимизируется следующий:\n",
    "\n",
    "$$J(X, \\vec{y}, \\vec{w}) = \\mathcal{L_{log}} (X, \\vec{y}, \\vec{w}) + \\lambda |\\vec{w}|^2$$\n",
    "\n",
    "В случае логистической регрессии принято введение обратного коэффициента регуляризации $C = \\frac{1}{\\lambda}$. И тогда решением задачи будет\n",
    "\n",
    "$$\\hat{w}  = \\arg \\min_{\\vec{w}} J(X, \\vec{y}, \\vec{w}) =  \\arg \\min_{\\vec{w}}\\ (C\\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\vec{w}^T\\vec{x_i}})+ |\\vec{w}|^2)$$ \n",
    "\n",
    "Далее рассмотрим пример, позволяющий интуитивно понять один из смыслов регуляризации. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
