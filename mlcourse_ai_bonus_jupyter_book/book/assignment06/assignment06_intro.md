(assignment06_intro)=

# Topic 6. Feature Engineering and Feature Selection

<div align="center">
<img src='../../_static/img/topic6-teaser.png'>  
</div><br>

Feature engineering is one of the most interesting processes in the whole of ML. It's an art or at least craft and is therefore not yet well-automated. The article describes the ways of working with heterogeneous features in various ML tasks with texts, images, geodata, etc. Practice with the ["Alice" competition](https://www.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2) is going to convince you how powerful feature engineering can be. And that it's a lot of fun as well!

1\. Read the [article](https://mlcourse.ai/articles/topic6-features/) (same in a form of a [Kaggle Notebook](https://www.kaggle.com/kashnitsky/topic-6-feature-engineering-and-feature-selection))

2\. Complete [demo assignment 6](https://www.kaggle.com/kashnitsky/assignment-6-linear-models-and-rf-for-regression) (topics 4 and 6 ) on OLS, Lasso and Random Forest in a regression task, and (opt.) check out the [solution](https://www.kaggle.com/kashnitsky/a6-demo-regression-solution) 

## Bonus Assignment 6. Beating benchmarks in "How good is your Medium article?"

Here we walk you through beating a baseline in a competition where the task is to predict the popularity of an article published on Medium. The basic solution uses text only. But on the go, you'll create a lot of additional features to improve the model. Also, in this assignment, you'll learn some dirty Kaggle hacks. 