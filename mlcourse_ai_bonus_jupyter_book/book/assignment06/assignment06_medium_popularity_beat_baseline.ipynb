{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://habrastorage.org/webt/ia/m9/zk/iam9zkyzqebnf_okxipihkgjwnw.jpeg\" />\n",
    "    \n",
    "**<center>[mlcourse.ai](https://mlcourse.ai) – Open Machine Learning Course** </center><br>\n",
    "Author: [Yury Kashnitskiy](https://yorko.github.io) (@yorko). [mlcourse.ai](https://mlcourse.ai) is powered by [OpenDataScience (ods.ai)](https://ods.ai/) © 2017—2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Assignment #6. Task</center><a class=\"tocSkip\">\n",
    "## <center> Beating benchmarks in \"How good is your Medium article?\"</center><a class=\"tocSkip\">\n",
    "    \n",
    "[Competition](https://www.kaggle.com/c/how-good-is-your-medium-article). The task is to beat \"Assignment 6 baseline\" (~1.45 Public LB score). You can refer to [this simple Ridge baseline](https://www.kaggle.com/kashnitsky/ridge-countvectorizer-baseline?rvi=1).\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will help to throw away all HTML tags from an article content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs = True\n",
    "        self.fed = []\n",
    "\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "\n",
    "    def get_data(self):\n",
    "        return \"\".join(self.fed)\n",
    "\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supplementary function to read a JSON line without crashing on escape characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_line(line=None):\n",
    "    result = None\n",
    "    try:\n",
    "        result = json.loads(line)\n",
    "    except Exception as e:\n",
    "        # Find the offending character index:\n",
    "        idx_to_replace = int(str(e).split(\" \")[-1].replace(\")\", \"\"))\n",
    "        # Remove the offending character:\n",
    "        new_line = list(line)\n",
    "        new_line[idx_to_replace] = \" \"\n",
    "        new_line = \"\".join(new_line)\n",
    "        return read_json_line(line=new_line)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features `content`, `published`, `title` and `author`, write them to separate files for train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_and_write(path_to_data, inp_filename, is_train=True):\n",
    "\n",
    "    features = [\"content\", \"published\", \"title\", \"author\"]\n",
    "    prefix = \"train\" if is_train else \"test\"\n",
    "    feature_files = [\n",
    "        open(\n",
    "            os.path.join(path_to_data, \"{}_{}.txt\".format(prefix, feat)),\n",
    "            \"w\",\n",
    "            encoding=\"utf-8\",\n",
    "        )\n",
    "        for feat in features\n",
    "    ]\n",
    "\n",
    "    with open(\n",
    "        os.path.join(path_to_data, inp_filename), encoding=\"utf-8\"\n",
    "    ) as inp_json_file:\n",
    "\n",
    "        for line in tqdm(inp_json_file):\n",
    "            json_data = read_json_line(line)\n",
    "\n",
    "\n",
    "# You code here (read-only in a JupyterBook, pls run jupyter-notebook to edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the [competition data](https://www.kaggle.com/c/how-good-is-your-medium-article/data) and place it where it's convenient for you. You can modify the path to data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"../../_static/data/assignment6/\"  # modify this if you need to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You code here (read-only in a JupyterBook, pls run jupyter-notebook to edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add the following groups of features:**\n",
    "    - Tf-Idf with article content (ngram_range=(1, 2), max_features=100000 but you can try adding more)\n",
    "    - Tf-Idf with article titles (ngram_range=(1, 2), max_features=100000 but you can try adding more)\n",
    "    - Time features: publication hour, whether it's morning, day, night, whether it's a weekend\n",
    "    - Bag of authors (i.e. One-Hot-Encoded author names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_content_sparse = csr_matrix(np.empty([10, 100000]))  # change this\n",
    "X_train_title_sparse = csr_matrix(np.empty([10, 100000]))  # change this\n",
    "X_train_author_sparse = csr_matrix(np.empty([10, 100000]))  # change this\n",
    "X_train_time_features_sparse = csr_matrix(np.empty([10, 5]))  # change this\n",
    "\n",
    "X_test_content_sparse = csr_matrix(np.empty([5, 100000]))  # change this\n",
    "X_test_title_sparse = csr_matrix(np.empty([5, 100000]))  # change this\n",
    "X_test_author_sparse = csr_matrix(np.empty([5, 100000]))  # change this\n",
    "X_test_time_features_sparse = csr_matrix(np.empty([5, 5]))  # change this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You code here (read-only in a JupyterBook, pls run jupyter-notebook to edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Join all sparse matrices.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sparse = hstack(\n",
    "    [\n",
    "        X_train_content_sparse,\n",
    "        X_train_title_sparse,\n",
    "        X_train_author_sparse,\n",
    "        X_train_time_features_sparse,\n",
    "    ]\n",
    ").tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_sparse = hstack(\n",
    "    [\n",
    "        X_test_content_sparse,\n",
    "        X_test_title_sparse,\n",
    "        X_test_author_sparse,\n",
    "        X_test_time_features_sparse,\n",
    "    ]\n",
    ").tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read train target and split data for validation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = pd.read_csv(\n",
    "    os.path.join(PATH_TO_DATA, \"train_log1p_recommends.csv\"), index_col=\"id\"\n",
    ")\n",
    "y_train = train_target[\"log_recommends\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_part_size = int(0.7 * train_target.shape[0])\n",
    "X_train_part_sparse = X_train_sparse[:train_part_size, :]\n",
    "y_train_part = y_train[:train_part_size]\n",
    "X_valid_sparse = X_train_sparse[train_part_size:, :]\n",
    "y_valid = y_train[train_part_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train a simple Ridge model and check MAE on the validation set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You code here (read-only in a JupyterBook, pls run jupyter-notebook to edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the same Ridge with all available data, make predictions for the test set and form a submission file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You code here (read-only in a JupyterBook, pls run jupyter-notebook to edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_test_pred = np.empty([34645, 1])  # change this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_submission_file(\n",
    "    prediction,\n",
    "    filename,\n",
    "    path_to_sample=os.path.join(PATH_TO_DATA, \"sample_submission.csv\"),\n",
    "):\n",
    "    submission = pd.read_csv(path_to_sample, index_col=\"id\")\n",
    "\n",
    "    submission[\"log_recommends\"] = prediction\n",
    "    submission.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission_file(\n",
    "    ridge_test_pred, os.path.join(PATH_TO_DATA, \"assignment6_medium_submission.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now's the time for dirty Kaggle hacks. Form a submission file with all zeros. Make a submission. What do you get if you think about it? How is it going to help you with modifying your predictions?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission_file(\n",
    "    np.zeros_like(ridge_test_pred),\n",
    "    os.path.join(PATH_TO_DATA, \"medium_all_zeros_submission.csv\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modify predictions in an appropriate way (based on your all-zero submission) and make a new submission.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_test_pred_modif = ridge_test_pred\n",
    "# You code here (read-only in a JupyterBook, pls run jupyter-notebook to edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission_file(\n",
    "    ridge_test_pred_modif,\n",
    "    os.path.join(PATH_TO_DATA, \"assignment6_medium_submission_with_hack.csv\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for the assignment. In case you'd like to try some more ideas for improvement:\n",
    "\n",
    "- Engineer good features, this is the key to success. Some simple features will be based on publication time, authors, content length and so on\n",
    "- You may not ignore HTML and extract some features from there\n",
    "- You'd better experiment with your validation scheme. You should see a correlation between your local improvements and LB score\n",
    "- Try TF-IDF, ngrams, Word2Vec and GloVe embeddings\n",
    "- Try various NLP techniques like stemming and lemmatization\n",
    "- Tune hyperparameters. In our example, we've left only 50k features and used C=1 as a regularization parameter, this can be changed\n",
    "- SGD and Vowpal Wabbit will train much faster\n",
    "- Play around with blending and/or stacking. An intro is given in [this Kernel](https://www.kaggle.com/kashnitsky/ridge-and-lightgbm-simple-blending) by @yorko \n",
    "- And neural nets of course. We don't cover them in this course byt still transformer-based architectures will likely perform well in such types of tasks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
