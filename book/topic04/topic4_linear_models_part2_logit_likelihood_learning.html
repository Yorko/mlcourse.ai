
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Topic 4. Linear Classification and Regression &#8212; mlcourse.ai</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://c6.patreon.com/becomePatronButton.bundle.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Topic 4. Linear Classification and Regression" href="topic4_linear_models_part3_regul_example.html" />
    <link rel="prev" title="Topic 4. Linear Classification and Regression" href="topic4_linear_models_part1_mse_likelihood_bias_variance.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-125504619-1', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/mlcourse_ai_logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">mlcourse.ai</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Intro
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Prerequisites
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../prereqs/python.html">
   Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../prereqs/math.html">
   Math
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../prereqs/software_devops.html">
   Software &amp; DevOps
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../prereqs/docker.html">
   Docker
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Topic 1
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../topic01/topic01_intro.html">
   Topic 1 Intro
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic01/topic01_pandas_data_analysis.html">
   Exploratory data analysis with Pandas
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic01/videolecture01.html">
   Videolecture 1
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic01/assignment01_pandas_uci_adult.html">
   Demo Assignment 1 Task
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic01/assignment01_pandas_uci_adult_solution.html">
   Demo Assignment 1 Solution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic01/bonus_assignment01_pandas_olympics.html">
   Bonus Assignment 1
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Topic 2
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../topic02/topic02_intro.html">
   Topic 2 Intro
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic02/topic02_visual_data_analysis.html">
   Visual Data Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic02/topic02_additional_seaborn_matplotlib_plotly.html">
   Seaborn, Matplotlib, Plotly
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic02/videolecture02.html">
   Videolecture 2
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic02/assignment02_analyzing_cardiovascular_desease_data.html">
   Demo Assignment 2 Task
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic02/assignment02_analyzing_cardiovascular_desease_data_solution.html">
   Demo Assignment 2 Solution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic02/bonus_assignment02_visual_analysis.html">
   Bonus Assignment 2
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Topic 3
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../topic03/topic03_intro.html">
   Topic 3 Intro
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic03/topic03_decision_trees_kNN.html">
   Classification, Decision Trees &amp; k Nearest Neighbors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic03/videolecture03.html">
   Videolecture 3
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic03/assignment03_decision_trees.html">
   Demo Assignment 3 Task
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic03/assignment03_decision_trees_solution.html">
   Demo Assignment 3 Solution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic03/bonus_assignment03_decision_trees.html">
   Bonus Assignment 3
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Topic 4
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="topic04_intro.html">
   Topic 4 Intro
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="topic4_linear_models_part1_mse_likelihood_bias_variance.html">
   Ordinary Least Squares
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Linear classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="topic4_linear_models_part3_regul_example.html">
   An illustrative example of logistic regression regularization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="topic4_linear_models_part4_good_bad_logit_movie_reviews_XOR.html">
   When logistic regression is good and when it is not
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="topic4_linear_models_part5_valid_learning_curves.html">
   Validation and learning curves
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="videolecture04.html">
   Videolecture 4
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="assignment04_regression_wine.html">
   Demo Assignment 4 Task
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="assignment04_regression_wine_solution.html">
   Demo Assignment 4 Solution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bonus_assignment04_alice_baselines.html">
   Bonus Assignment 4
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Topic 5
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../topic05/topic05_intro.html">
   Topic 5 Intro
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic05/topic5_part1_bagging.html">
   Bagging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic05/topic5_part2_random_forest.html">
   Random Forest
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic05/topic5_part3_feature_importance.html">
   Feature importance
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic05/videolecture05.html">
   Videolecture 5
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic05/assignment05_logit_rf_credit_scoring.html">
   Demo Assignment 5
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic05/assignment05_logit_rf_credit_scoring_solution.html">
   Demo Assignment 5 Solution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic05/bonus_assignment05_logreg_rf.html">
   Bonus Assignment 5
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Topic 6
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../topic06/topic06_intro.html">
   Topic 6 Intro
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic06/topic6_feature_engineering_feature_selection.html">
   Feature engineering &amp; feature selection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic06/demo_assignment06.html">
   Demo Assignment 6 Task
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic06/bonus_assignment06.html">
   Bonus Assignment 6
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Topic 7
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../topic07/topic07_intro.html">
   Topic 7 Intro
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic07/topic7_pca_clustering.html">
   Unsupervised learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic07/videolecture07.html">
   Videolecture 7
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic07/assignment07_unsupervised_learning.html">
   Demo Assignment 7
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic07/assignment07_unsupervised_learning_solution.html">
   Demo Assignment 7 Solution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic07/bonus_assignment07.html">
   Bonus Assignment 7
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Topic 8
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../topic08/topic08_intro.html">
   Topic 8 Intro
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic08/topic08_sgd_hashing_vowpal_wabbit.html">
   Vowpal Wabbit: Learning with Gigabytes of Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic08/videolecture08.html">
   Videolecture 8
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic08/assignment08_implement_sgd_regressor.html">
   Demo Assignment 8
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic08/assignment08_implement_sgd_regressor_solution.html">
   Demo Assignment 8 Solution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic08/bonus_assignment08.html">
   Bonus Assignment 8
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Topic 9
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../topic09/topic09_intro.html">
   Topic 9 Intro
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic09/topic9_part1_time_series_python.html">
   Time series analysis in Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic09/topic9_part2_facebook_prophet.html">
   Predicting the future with Facebook Prophet
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic09/videolecture09.html">
   Videolecture 9
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic09/assignment09_time_series.html">
   Demo Assignment 9
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic09/assignment09_time_series_solution.html">
   Demo Assignment 9 Solution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic09/bonus_assignment09.html">
   Bonus Assignment 9
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Topic 10
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../topic10/topic10_intro.html">
   Topic 10 Intro
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic10/topic10_gradient_boosting.html">
   Gradient boosting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic10/videolecture10.html">
   Videolecture 10
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic10/assignment10_flight_delays_kaggle.html">
   Demo Assignment 10
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic10/bonus_assignment10.html">
   Bonus Assignment 10
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  About the course
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../extra/tutorials.html">
   Tutorials
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../extra/rating.html">
   Rating
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../extra/resources.html">
   Resources
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../extra/contributors.html">
   Contributors
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/Yorko/mlcourse.ai/main?urlpath=tree/mlcourse_ai_jupyter_book/book/topic04/topic4_linear_models_part2_logit_likelihood_learning.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/Yorko/mlcourse.ai/"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/Yorko/mlcourse.ai//issues/new?title=Issue%20on%20page%20%2Fbook/topic04/topic4_linear_models_part2_logit_likelihood_learning.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/Yorko/mlcourse.ai/edit/main/mlcourse_ai_jupyter_book/book/topic04/topic4_linear_models_part2_logit_likelihood_learning.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/book/topic04/topic4_linear_models_part2_logit_likelihood_learning.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download notebook file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        <a href="../../_sources/book/topic04/topic4_linear_models_part2_logit_likelihood_learning.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#part-2-linear-classification">
   Part 2. Linear Classification
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#article-outline">
   Article outline
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-classifier">
   1. Linear Classifier
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#logistic-regression-as-a-linear-classifier">
   2. Logistic Regression as a Linear Classifier
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#maximum-likelihood-estimation-and-logistic-regression">
   3. Maximum Likelihood Estimation and Logistic Regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#l-2-regularization-of-logistic-loss">
   4.
   <span class="math notranslate nohighlight">
    \(L_2\)
   </span>
   -Regularization of Logistic Loss
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#useful-resources">
   5. Useful resources
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Topic 4. Linear Classification and Regression</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#part-2-linear-classification">
   Part 2. Linear Classification
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#article-outline">
   Article outline
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-classifier">
   1. Linear Classifier
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#logistic-regression-as-a-linear-classifier">
   2. Logistic Regression as a Linear Classifier
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#maximum-likelihood-estimation-and-logistic-regression">
   3. Maximum Likelihood Estimation and Logistic Regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#l-2-regularization-of-logistic-loss">
   4.
   <span class="math notranslate nohighlight">
    \(L_2\)
   </span>
   -Regularization of Logistic Loss
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#useful-resources">
   5. Useful resources
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="topic-4-linear-classification-and-regression">
<span id="topic04-part2"></span><h1>Topic 4. Linear Classification and Regression<a class="headerlink" href="#topic-4-linear-classification-and-regression" title="Permalink to this headline">#</a></h1>
<div class="section" id="part-2-linear-classification">
<h2>Part 2. Linear Classification<a class="headerlink" href="#part-2-linear-classification" title="Permalink to this headline">#</a></h2>
<div class="figure align-default">
<img alt="../../_images/ods_stickers.jpg" src="../../_images/ods_stickers.jpg" />
</div>
<p><strong><center><a class="reference external" href="https://mlcourse.ai">mlcourse.ai</a> – Open Machine Learning Course</strong> </center><br></p>
<p>Author: <a class="reference external" href="https://yorko.github.io">Yury Kashnitsky</a>. Translated and edited by <a class="reference external" href="https://www.linkedin.com/in/christinabutsko/">Christina Butsko</a>, <a class="reference external" href="https://www.linkedin.com/in/nersesbagiyan/">Nerses Bagiyan</a>, <a class="reference external" href="https://www.linkedin.com/in/yuliya-klimushina-7168a9139">Yulia Klimushina</a>, and <a class="reference external" href="https://www.linkedin.com/in/yuanyuanpao/">Yuanyuan Pao</a>. This material is subject to the terms and conditions of the <a class="reference external" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons CC BY-NC-SA 4.0</a> license. Free use is permitted for any non-commercial purpose.</p>
</div>
<div class="section" id="article-outline">
<h2>Article outline<a class="headerlink" href="#article-outline" title="Permalink to this headline">#</a></h2>
<ol class="simple">
<li><p><a class="reference external" href="#linear-classifier">Linear Classifier</a></p></li>
<li><p><a class="reference external" href="#logistic-regression-as-a-linear-classifier">Logistic Regression as a Linear Classifier</a></p></li>
<li><p><a class="reference external" href="#maximum-likelihood-estimation-and-logistic-regression">Maximum Likelihood Estimation and Logistic Regression</a></p></li>
<li><p><a class="reference external" href="#l-2-regularization-of-logistic-loss"><span class="math notranslate nohighlight">\(L_2\)</span>-Regularization of Logistic Loss</a></p></li>
<li><p><a class="reference external" href="#useful-resources">Useful resources</a></p></li>
</ol>
</div>
<div class="section" id="linear-classifier">
<h2>1. Linear Classifier<a class="headerlink" href="#linear-classifier" title="Permalink to this headline">#</a></h2>
<p>The basic idea behind a linear classifier two target classes can be separated by a hyperplane in the feature space. If this can be done without error, the training set is called <em>linearly separable</em>.</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="../../_images/topic4_linear_classifier.png"><img alt="../../_images/topic4_linear_classifier.png" src="../../_images/topic4_linear_classifier.png" style="width: 480px;" /></a>
</div>
<p>We have already seen linear regression and Ordinary Least Squares (OLS). Let’s consider a binary classification problem, and denote target classes to be “+1” (positive examples) and “-1” (negative examples). One of the simplest linear classifiers can be defined using regression as follows:</p>
<div class="math notranslate nohighlight">
\[\Large a(\textbf{x}) = \text{sign}(\textbf{w}^\text{T}\textbf x),\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\textbf{x}\)</span> –  is a feature vector (along with identity);</p></li>
<li><p><span class="math notranslate nohighlight">\(\textbf{w}\)</span> – is a vector of weights in the linear model (with bias <span class="math notranslate nohighlight">\(w_0\)</span>);</p></li>
<li><p><span class="math notranslate nohighlight">\(\text{sign}(\bullet)\)</span> – is the signum function that returns the sign of its argument;</p></li>
<li><p><span class="math notranslate nohighlight">\(a(\textbf{x})\)</span> – is a classifier response for <span class="math notranslate nohighlight">\(\textbf{x}\)</span>.</p></li>
</ul>
</div>
<div class="section" id="logistic-regression-as-a-linear-classifier">
<h2>2. Logistic Regression as a Linear Classifier<a class="headerlink" href="#logistic-regression-as-a-linear-classifier" title="Permalink to this headline">#</a></h2>
<p>Logistic regression is a special case of the linear classifier, but it has an added benefit of predicting a probability <span class="math notranslate nohighlight">\(p_+\)</span> of referring example <span class="math notranslate nohighlight">\(\textbf{x}_\text{i}\)</span> to the class “+”:</p>
<div class="math notranslate nohighlight">
\[
\Large p_+ = P\left(y_i = 1 \mid \textbf{x}_\text{i}, \textbf{w}\right)
\]</div>
<p>Being able to predict not just a response ( “+1” or “-1”) but the <em>probability</em> of assignment to class “+1” is a very important requirement in many business problems e.g. credit scoring where logistic regression is traditionally used. Customers who have applied for a loan are ranked based on this predicted probability (in descending order) to obtain a scoreboard that rates customers from bad to good. Below is an example of such a toy scoreboard.</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="../../_images/topic4_toy_scorecard_eng.png"><img alt="../../_images/topic4_toy_scorecard_eng.png" src="../../_images/topic4_toy_scorecard_eng.png" style="width: 480px;" /></a>
</div>
<p>The bank chooses a threshold <span class="math notranslate nohighlight">\(p_*\)</span> to predict the probability of loan default (in the picture it’s <span class="math notranslate nohighlight">\(0.15\)</span>) and stops approving loans starting from that value. Moreover, it is possible to multiply this predicted probability by the loan amount to get the expectation of losses from the client, which can also constitute good business metrics (scoring experts may have more to add, but the main gist is this).</p>
<p>To predict the probability <span class="math notranslate nohighlight">\(p_+ \in [0,1]\)</span>, we can start by constructing a linear prediction using OLS: <span class="math notranslate nohighlight">\(b(\textbf{x}) = \textbf{w}^\text{T} \textbf{x} \in \mathbb{R}\)</span>. But converting the resulting value to the probability within in the [0, 1] range requires some function <span class="math notranslate nohighlight">\(f: \mathbb{R} \rightarrow [0,1]\)</span>. Logistic regression uses a specific function for this: <span class="math notranslate nohighlight">\(\sigma(z) = \frac{1}{1 + \exp^{-z}}\)</span>. Now let’s understand what the prerequisites are.</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="../../_images/topic4_sigmoid.png"><img alt="../../_images/topic4_sigmoid.png" src="../../_images/topic4_sigmoid.png" style="width: 480px;" /></a>
</div>
<p>Let’s denote the probability of an event <span class="math notranslate nohighlight">\(X\)</span> as <span class="math notranslate nohighlight">\(P(X)\)</span>. Then the odds  <span class="math notranslate nohighlight">\(OR(X)\)</span> are determined by <span class="math notranslate nohighlight">\(\frac{P(X)}{1-P(X)}\)</span>, which is the ratio of the probabilities of whether or not an event will happen. It is obvious that the probability and odds contain the same information, but, while <span class="math notranslate nohighlight">\(P(X)\)</span> ranges from 0 to 1, <span class="math notranslate nohighlight">\(OR(X)\)</span> is in the range of 0 to <span class="math notranslate nohighlight">\(\infty\)</span>.</p>
<p>If we calculate the logarithm of <span class="math notranslate nohighlight">\(OR(X)\)</span> (a logarithm of odds or log probability ratio), it is easy to notice that <span class="math notranslate nohighlight">\(\log{OR(X)} \in \mathbb{R}\)</span>. This is what we will use with OLS.</p>
<p>Let’s see how logistic regression will make a prediction <span class="math notranslate nohighlight">\(p_+ = P\left(y_i = 1 \mid \textbf{x}_\text{i}, \textbf{w}\right)\)</span>. (For now, let’s assume that we have somehow obtained weights <span class="math notranslate nohighlight">\(\textbf{w}\)</span> i.e. trained the model. Later, we’ll look at how it is done.)</p>
<p><strong>Step 1.</strong> Calculate <span class="math notranslate nohighlight">\(w_{0}+w_{1}x_1 + w_{2}x_2 + ... = \textbf{w}^\text{T}\textbf{x}\)</span>. (Equation <span class="math notranslate nohighlight">\(\textbf{w}^\text{T}\textbf{x} = 0\)</span> defines a hyperplane separating the examples into two classes);</p>
<p><strong>Step 2.</strong> Compute the log odds: <span class="math notranslate nohighlight">\( \log(OR_{+}) = \textbf{w}^\text{T}\textbf{x}\)</span>.</p>
<p><strong>Step 3.</strong> Now that we have the chance of assigning an example to the class of “+” - <span class="math notranslate nohighlight">\(OR_{+}\)</span>, calculate <span class="math notranslate nohighlight">\(p_{+}\)</span> using the simple relationship:</p>
<div class="math notranslate nohighlight">
\[\large p_{+} = \frac{OR_{+}}{1 + OR_{+}} = \frac{\exp^{\textbf{w}^\text{T}\textbf{x}}}{1 + \exp^{\textbf{w}^\text{T}\textbf{x}}} = \frac{1}{1 + \exp^{-\textbf{w}^\text{T}\textbf{x}}} = \sigma(\textbf{w}^\text{T}\textbf{x})\]</div>
<p>On the right side, you can see that we have the sigmoid function.</p>
<p>So, logistic regression predicts the probability of assigning an example to the “+” class (assuming that we know the features and weights of the model) as a sigmoid transformation of a linear combination of the weight vector and the feature vector:</p>
<div class="math notranslate nohighlight">
\[\large p_+(\textbf{x}_\text{i}) = P\left(y_i = 1 \mid \textbf{x}_\text{i}, \textbf{w}\right) = \sigma(\textbf{w}^\text{T}\textbf{x}_\text{i}). \]</div>
<p>Next, we will see how the model is trained. We will again rely on maximum likelihood estimation.</p>
</div>
<div class="section" id="maximum-likelihood-estimation-and-logistic-regression">
<h2>3. Maximum Likelihood Estimation and Logistic Regression<a class="headerlink" href="#maximum-likelihood-estimation-and-logistic-regression" title="Permalink to this headline">#</a></h2>
<p>Now let’s see how an optimization problem for logistic regression is obtained from the MLE, namely, minimization of the <em>logistic</em> loss function. We have just seen that logistic regression models the probability of assigning an example to the class “+” as:</p>
<div class="math notranslate nohighlight">
\[\Large p_+(\textbf{x}_\text{i}) = P\left(y_i = 1 \mid \textbf{x}_\text{i}, \textbf{w}\right) = \sigma(\textbf{w}^T\textbf{x}_\text{i})\]</div>
<p>Then, for the class “-”, the corresponding expression is as follows:</p>
<div class="math notranslate nohighlight">
\[
\Large p_-(\textbf{x}_\text{i})  = P\left(y_i = -1 \mid \textbf{x}_\text{i}, \textbf{w}\right)  = 1 - \sigma(\textbf{w}^T\textbf{x}_\text{i}) = \sigma(-\textbf{w}^T\textbf{x}_\text{i})
\]</div>
<p>Both of these expressions can be cleverly combined into one (watch carefully, maybe you are being tricked):</p>
<div class="math notranslate nohighlight">
\[\Large P\left(y = y_i \mid \textbf{x}_\text{i}, \textbf{w}\right) = \sigma(y_i\textbf{w}^T\textbf{x}_\text{i})\]</div>
<p>The expression <span class="math notranslate nohighlight">\(M(\textbf{x}_\text{i}) = y_i\textbf{w}^T\textbf{x}_\text{i}\)</span> is known as the margin of classification on the object <span class="math notranslate nohighlight">\(\textbf{x}_\text{i}\)</span> (not to be confused with a gap, which is also called margin, in the SVM context). If it is non-negative, the model is correct in choosing the class of the object <span class="math notranslate nohighlight">\(\textbf{x}_\text{i}\)</span>; if it is negative, then the object <span class="math notranslate nohighlight">\(\textbf{x}_\text{i}\)</span> is misclassified. Note that the margin is defined for objects in the training set only where real target class labels <span class="math notranslate nohighlight">\(y_i\)</span> are known.</p>
<p>To understand exactly why we have come to such a conclusion, let us turn to the geometrical interpretation of the linear classifier.</p>
<p>First, I would recommend looking at a classic, introductory problem in linear algebra: find the distance from the point with a radius-vector <span class="math notranslate nohighlight">\(\textbf{x}_A\)</span> to a plane defined by the equation <span class="math notranslate nohighlight">\(\textbf{w}^\text{T}\textbf{x} = 0.\)</span></p>
<details>
  <summary> Answer </summary>
<div class="math notranslate nohighlight">
\[
\rho(\textbf{x}_A, \textbf{w}^\text{T}\textbf{x} = 0) = \frac{\textbf{w}^\text{T}\textbf{x}_A}{||\textbf{w}||}
\]</div>
</details>
<div class="figure align-default">
<a class="reference internal image-reference" href="../../_images/topic4_simple_linal_task.png"><img alt="../../_images/topic4_simple_linal_task.png" src="../../_images/topic4_simple_linal_task.png" style="width: 480px;" /></a>
</div>
<p>When we get to the answer, we will understand that the greater the absolute value of the expression <span class="math notranslate nohighlight">\(\textbf{w}^\text{T}\textbf{x}_\text{i}\)</span>, the farther the point <span class="math notranslate nohighlight">\(\textbf{x}_\text{i}\)</span> is from the plane <span class="math notranslate nohighlight">\(\textbf{w}^\text{T}\textbf{x} = 0.\)</span></p>
<p>Hence, our expression <span class="math notranslate nohighlight">\(M(\textbf{x}_\text{i}) = y_i\textbf{w}^\text{T}\textbf{x}_\text{i}\)</span> is a kind of “confidence” in our model’s classification of the object <span class="math notranslate nohighlight">\(\textbf{x}_\text{i}\)</span>:</p>
<ul class="simple">
<li><p>if the margin is large (in absolute value) and positive, the class label is set correctly, and the object is far away from the separating hyperplane i.e. classified confidently. See Point <span class="math notranslate nohighlight">\(x_3\)</span> on the picture;</p></li>
<li><p>if the margin is large (in absolute value) and negative, then class label is set incorrectly, and the object is far from the separating hyperplane (the object is most likely an anomaly; for example, it could be improperly labeled in the training set). See Point <span class="math notranslate nohighlight">\(x_1\)</span> on the picture;</p></li>
<li><p>if the margin is small (in absolute value), then the object is close to the separating hyperplane, and the margin sign determines whether the object is correctly classified. See Points <span class="math notranslate nohighlight">\(x_2\)</span> and <span class="math notranslate nohighlight">\(x_4\)</span> on the plot;</p></li>
</ul>
<div class="figure align-default">
<a class="reference internal image-reference" href="../../_images/topic4_margin.png"><img alt="../../_images/topic4_margin.png" src="../../_images/topic4_margin.png" style="width: 480px;" /></a>
</div>
<p>Let’s now compute the likelihood of the data set i.e. the probability of observing the given vector <span class="math notranslate nohighlight">\(\textbf{y}\)</span> from data set <span class="math notranslate nohighlight">\(X\)</span>. We’ll make a strong assumption: objects come independently from one distribution (<em>i.i.d.</em>). Then, we can write</p>
<div class="math notranslate nohighlight">
\[\Large P\left(\textbf{y} \mid \textbf{X}, \textbf{w}\right) = \prod_{i=1}^{\ell} P\left(y = y_i \mid \textbf{x}_\text{i}, \textbf{w}\right),\]</div>
<p>where <span class="math notranslate nohighlight">\(\ell\)</span> is the length of data set <span class="math notranslate nohighlight">\(\textbf{X}\)</span> (number of rows).</p>
<p>As usual, let’s take the logarithm of this expression because a sum is much easier to optimize than the product:</p>
<div class="math notranslate nohighlight">
\[\Large \log P\left(\textbf{y} \mid \textbf{X}, \textbf{w}\right) = \log \prod_{i=1}^{\ell} P\left(y = y_i \mid \textbf{x}_\text{i}, \textbf{w}\right) = \log \prod_{i=1}^{\ell} \sigma(y_i\textbf{w}^\text{T}\textbf{x}_\text{i})   = \]</div>
<div class="math notranslate nohighlight">
\[\Large  = \sum_{i=1}^{\ell} \log \sigma(y_i\textbf{w}^\text{T}\textbf{x}_\text{i}) = \sum_{i=1}^{\ell} \log \frac{1}{1 + \exp^{-y_i\textbf{w}^\text{T}\textbf{x}_\text{i}}} = - \sum_{i=1}^{\ell} \log (1 + \exp^{-y_i\textbf{w}^\text{T}\textbf{x}_\text{i}})\]</div>
<p>Maximizing the likelihood is equivalent to minimizing the expression:</p>
<div class="math notranslate nohighlight">
\[\Large \mathcal{L_{\log}} (\textbf X, \textbf{y}, \textbf{w}) = \sum_{i=1}^{\ell} \log (1 + \exp^{-y_i\textbf{w}^\text{T}\textbf{x}_\text{i}}).\]</div>
<p>This is <em>logistic</em> loss function that is summed over all objects in the training set.</p>
<p>Let’s look at the new function as a function of margin <span class="math notranslate nohighlight">\(L(M) = \log (1 + \exp^{-M})\)</span> and plot it along with <em>zero-one loss</em> graph, which simply penalizes the model for error on each object by 1 (negative margin): <span class="math notranslate nohighlight">\(L_{1/0}(M) = [M &lt; 0]\)</span>.</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="../../_images/topic4_logloss_margin_eng.png"><img alt="../../_images/topic4_logloss_margin_eng.png" src="../../_images/topic4_logloss_margin_eng.png" style="width: 480px;" /></a>
</div>
<p>The picture reflects the idea that, if we are not able to directly minimize the number of errors in the classification problem (at least not by gradient methods - derivative of the zero-one loss function at zero turns to infinity), we can minimize its upper bounds. For the logistic loss function (where the logarithm is binary, but this does not matter), the following is valid:</p>
<div class="math notranslate nohighlight">
\[\Large \mathcal{L_{1/0}} (\textbf X, \textbf{y}, \textbf{w}) = \sum_{i=1}^{\ell} [M(\textbf{x}_\text{i}) &lt; 0] \leq \sum_{i=1}^{\ell} \log (1 + \exp^{-y_i\textbf{w}^\text{T}\textbf{x}_\text{i}}) = \mathcal{L_{\log}} (\textbf X, \textbf{y}, \textbf{w}), \]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{L_{1/0}} (\textbf X, \textbf{y})\)</span> is simply the number of errors of logistic regression with weights <span class="math notranslate nohighlight">\(\textbf{w}\)</span> on a data set <span class="math notranslate nohighlight">\((\textbf X, \textbf{y})\)</span>.</p>
<p>Thus, by reducing the upper bound of <span class="math notranslate nohighlight">\(\mathcal{L_{log}}\)</span> by the number of classification errors, we hope to reduce the number of errors itself.</p>
</div>
<div class="section" id="l-2-regularization-of-logistic-loss">
<h2>4. <span class="math notranslate nohighlight">\(L_2\)</span>-Regularization of Logistic Loss<a class="headerlink" href="#l-2-regularization-of-logistic-loss" title="Permalink to this headline">#</a></h2>
<p><span class="math notranslate nohighlight">\(L_2\)</span>-regularization of logistic regression is almost the same as in the case of ridge regression. Instead of minimizing the function <span class="math notranslate nohighlight">\(\mathcal{L_{\log}} (\textbf X, \textbf{y}, \textbf{w})\)</span> we minimize the following:</p>
<div class="math notranslate nohighlight">
\[\Large \mathcal{J}(\textbf X, \textbf{y}, \textbf{w}) = \mathcal{L_{\log}} (\textbf X, \textbf{y}, \textbf{w}) + \lambda |\textbf{w}|^2\]</div>
<p>In the case of logistic regression, a reverse regularization coefficient <span class="math notranslate nohighlight">\(C = \frac{1}{\lambda}\)</span> is typically introduced. Then the solution to the problem would be:</p>
<div class="math notranslate nohighlight">
\[\Large \widehat{\textbf w}  = \arg \min_{\textbf{w}} \mathcal{J}(\textbf X, \textbf{y}, \textbf{w}) =  \arg \min_{\textbf{w}}\ (C\sum_{i=1}^{\ell} \log (1 + \exp^{-y_i\textbf{w}^\text{T}\textbf{x}_\text{i}})+ |\textbf{w}|^2)\]</div>
<p>Next, we’ll look at an example that allows us to intuitively understand one of the interpretations of regularization.</p>
</div>
<div class="section" id="useful-resources">
<h2>5. Useful resources<a class="headerlink" href="#useful-resources" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Medium <a class="reference external" href="https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-4-linear-classification-and-regression-44a41b9b5220">“story”</a> based on this notebook</p></li>
<li><p>Main course <a class="reference external" href="https://mlcourse.ai">site</a>, <a class="reference external" href="https://github.com/Yorko/mlcourse.ai">course repo</a>, and YouTube <a class="reference external" href="https://www.youtube.com/watch?v=QKTuw4PNOsU&amp;list=PLVlY_7IJCMJeRfZ68eVfEcu-UcN9BbwiX">channel</a></p></li>
<li><p>Course materials as a <a class="reference external" href="https://www.kaggle.com/kashnitsky/mlcourse">Kaggle Dataset</a></p></li>
<li><p>If you read Russian: an <a class="reference external" href="https://habrahabr.ru/company/ods/blog/323890/">article</a> on <a class="reference external" href="http://Habr.com">Habr.com</a> with ~ the same material. And a <a class="reference external" href="https://youtu.be/oTXGQ-_oqvI">lecture</a> on YouTube</p></li>
<li><p>A nice and concise overview of linear models is given in the book <a class="reference external" href="http://www.deeplearningbook.org">“Deep Learning”</a> (I. Goodfellow, Y. Bengio, and A. Courville).</p></li>
<li><p>Linear models are covered practically in every ML book. We recommend “Pattern Recognition and Machine Learning” (C. Bishop) and “Machine Learning: A Probabilistic Perspective” (K. Murphy).</p></li>
<li><p>If you prefer a thorough overview of linear model from a statistician’s viewpoint, then look at “The elements of statistical learning” (T. Hastie, R. Tibshirani, and J. Friedman).</p></li>
<li><p>The book “Machine Learning in Action” (P. Harrington) will walk you through implementations of classic ML algorithms in pure Python.</p></li>
<li><p><a class="reference external" href="http://scikit-learn.org/stable/documentation.html">Scikit-learn</a> library. These guys work hard on writing really clear documentation.</p></li>
<li><p>Scipy 2017 <a class="reference external" href="https://github.com/amueller/scipy-2017-sklearn">scikit-learn tutorial</a> by Alex Gramfort and Andreas Mueller.</p></li>
<li><p>One more <a class="reference external" href="https://github.com/diefimov/MTH594_MachineLearning">ML course</a> with very good materials.</p></li>
<li><p><a class="reference external" href="https://github.com/rushter/MLAlgorithms">Implementations</a> of many ML algorithms. Search for linear regression and logistic regression.</p></li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./book/topic04"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="topic4_linear_models_part1_mse_likelihood_bias_variance.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Topic 4. Linear Classification and Regression</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="topic4_linear_models_part3_regul_example.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Topic 4. Linear Classification and Regression</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Yury Kashnitsky (yorko)<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>