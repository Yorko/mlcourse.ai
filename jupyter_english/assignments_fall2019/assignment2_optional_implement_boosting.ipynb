{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\" />\n",
    "    \n",
    "## [mlcourse.ai](https://mlcourse.ai) – Open Machine Learning Course \n",
    "Author: [Yury Kashnitsky](https://yorko.github.io) (@yorko). This material is subject to the terms and conditions of the [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) license. Free use is permitted for any non-commercial purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <center>Assignment #2. Optional part\n",
    "## <center> Implementation of the gradient boosting algorithm\n",
    "    \n",
    "#  <center>  <font color = 'red'> Warning! </font>This is a very useful but ungraded assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will implement a the general gradient boosting algorithm -- the same class will implement a binary classifier that minimizes the logistic loss function and two regressors that minimize the mean squared error (MSE) and the root mean squared logarithmic error ([RMSLE](https://www.kaggle.com/wiki/RootMeanSquaredLogarithmicError)). This way, we will see that we can optimize arbitrary differentiable functions using gradient boosting and how this technique adapts in different contexts.\n",
    "\n",
    "We will use the algorithm version from the [article](https://habrahabr.ru/company/ods/blog/327250/#klassicheskiy-gbm-algoritm-friedman-a) but with two simplifications:\n",
    "1. We initialize the algorithm with the mean value of the vector $\\large y$ i.e. $\\large \\hat{f_0} = \\frac{1}{n}\\sum_{i=1}^{n}y_i$.\n",
    "2. We will make the learning rate constant: $\\large \\rho_t = const$.\n",
    "\n",
    "There is a mapping between the pseudo code and the class parameters in `GradientBoosting`:\n",
    "\n",
    "| What | Pseudo code &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;| `GradientBoosting` |\n",
    "|:-----|:--------------------------------------|:-------------------|\n",
    "| Training set  | $\\large \\{x_i, y_i\\}_{i = 1,\\ldots n}$ | `X`, `y` |\n",
    "| Loss function | $\\large L(y,f)$ | `objective` |\n",
    "| Loss function gradient | $\\large \\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)}$ | `objective_grad` |\n",
    "| Number of iterations | $\\large М$ | `n_estimators` |\n",
    "| Base algorithm (decision tree regressor) | $\\large h(x,\\theta)$ | `DecisionionTreeRegressor` |\n",
    "| Decision tree hyperparameters | $\\large \\theta$ | We will use only `max_depth` and `random_state`. |\n",
    "| Learning rate<br>(coefficient for $\\large h_t(x,\\theta)$ in the composition) | $\\large \\rho_t, \\quad t=1,\\ldots,M$ | `learning_rate` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deriving gradients for log_loss, MSE and RMSLE\n",
    "\n",
    "Let's start with the traditional way of deriving formulas with pen and paper:\n",
    "$\\DeclareMathOperator{\\logloss}{log\\_loss}$\n",
    "$\\DeclareMathOperator{\\MSE}{MSE}$\n",
    "$\\DeclareMathOperator{\\RMSLE}{RMSLE}$\n",
    "$\\newcommand{\\y}{\\mathbf{y}}$\n",
    "$\\newcommand{\\p}{\\mathbf{p}}$\n",
    "\n",
    "$$\\begin{array}{rcl}\n",
    "\\logloss(\\y, \\p) &=& -\\y\\log \\p + (1 - \\y)\\log (1 - \\p) \\\\\n",
    "&=& -\\sum_{i=1}^{n}\\left[y_i\\log p_i + (1 - y_i)\\log (1 - p_i)\\right] \\\\\n",
    "\\\\\n",
    "\\MSE(\\y, \\p) &=& \\frac{1}{n}(\\y - \\p)^T(\\y - \\p) = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - p_i)^2 \\\\\n",
    "\\\\\n",
    "\\RMSLE(\\y, \\p) &=& \\sqrt{\\frac{1}{n} (\\log (\\p + 1) - \\log (\\y + 1))^T(\\log (\\p + 1) - \\log (\\y + 1))} \\\\\n",
    "&=& \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}(\\log (p_i + 1) - \\log (y_i + 1))^2}\n",
    "\\end{array}$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $\\y$ and $\\p$ are **vectors** of values and predictions respectively.\n",
    "* $\\logloss$ takes the same values as in `scikit-learn` with $0$ and $1$ instead of $-1$ and $1$, as described in the main article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.** What is the expression for the `MSE` gradient?\n",
    "\n",
    "$\\begin{array}{rcl}\n",
    "&& \\text{1. } (\\p - \\y) && \\text{3. } 2(\\p - \\y) \\\\\n",
    "&& \\text{2. } \\frac{2}{n}(\\y - \\p) && \\text{4. } \\frac{2}{n}(\\p - \\y)\n",
    "\\end{array}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.** What is the expression for the `log_loss` gradient?\n",
    "\n",
    "$\\begin{array}{rcl}\n",
    "&& \\text{1. } \\large \\frac{\\y - \\p}{\\y(1 - \\y)} && \\text{3. } \\large \\frac{\\p - \\y}{\\p(1 - \\p)} \\\\\n",
    "&& \\text{2. } \\large \\frac{\\y - \\p}{\\p(1 - \\p)} && \\text{4. } \\large \\frac{\\p - \\y}{\\y(1 - \\y)}\n",
    "\\end{array}$\n",
    "\n",
    "*Note:* division by a vector is element-wise, i.e. $\\frac{1}{\\p} = (\\frac{1}{p_1}, \\ldots \\frac{1}{p_n})^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.** What is the expression for the `RMSLE` gradient?\n",
    "\n",
    "$\\begin{array}{rcl}\n",
    "&& \\text{1. } \\frac{1}{n}(\\p + 1)\\RMSLE^{-1}(\\y, \\p) \\log \\frac{\\p+1}{\\y+1} && \\text{3. } [n(\\y + 1)\\RMSLE(\\y, \\p)]^{-1} \\log \\frac{\\p+1}{\\y+1} \\\\\n",
    "&& \\text{2. } [n(\\p + 1)\\RMSLE(\\y, \\p)]^{-1} \\log \\frac{\\p+1}{\\y+1} && \\text{4. } \\frac{1}{n}\\frac{\\y+1}{(\\p + 1)}\\RMSLE^{-1}(\\y, \\p) \\log \\frac{\\p+1}{\\y+1}\n",
    "\\end{array}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm implementation\n",
    "\n",
    "### Task:\n",
    "\n",
    "Implement the `GradientBoosting` class using the following specification:\n",
    "- The class inherits from `sklearn.base.BaseEstimator`.\n",
    "- The constructor has the following parameters:\n",
    "\n",
    "     - `loss` – a loss function to be optimized: `log_loss`, `mse` (by default) or `rmsle`;\n",
    "     - `n_estimators` – the number of trees, that is, the number of boosting iterations (10 by default);\n",
    "     - `learning_rate` – the learning rate ($10^{-2}$ by default);\n",
    "     - `max_depth` – the maximum depth of a tree (3 by default);\n",
    "     - `random_state` – the seed for the random number generator, only used for trees (17 by default).\n",
    "     \n",
    "- Depending on the value of `loss`, `objective` and `objective_grad` are initialized differently:\n",
    "\n",
    "     - For `mse`, use `sklearn.metrics.mean_squared_error`;\n",
    "     - For `log_loss`, use `sklearn.metrics.log_loss`;\n",
    "     - For `rmsle`, you will need to implement the loss function by yourself as well as the gradients of all three loss functions. Also, don't leave out constants like $2$ or $n$ when computing the gradients.\n",
    "     \n",
    "- You will be using element-wise vector division in the implementations of the gradients for `log_loss` and `rmsle`. In order to avoid division by zero, replace all values less than $10^{-5}$ with $10^{-5}$, but do it only where it is absolutely neccesary. For example, when computing $\\frac{y}{p}$, only replace the values in vector $p$.\n",
    "- The constructor must create lists `loss_by_iter_` and `residuals_by_iter_` for debugging purposes and `trees_` for storing trained trees.\n",
    "- The class must have methods `fit`, `predict`, and `predict_proba`:\n",
    "    - The method `fit` takes a matrix `X` and a vector `y` (both are instances of `numpy.array`) as arguments, and returns the current instance of `GradientBoosting` i.e `self`. This method will implement the main logic of the algorithm. At each iteration, the current value of the loss function is stored in `loss_by_iter`, the value of the anti gradient (what we called *pseudo residuals* in the article) in `residuals_by_iter_`. Optionally, you could add a flag `debug=False` to the constructor arguments and store the anti gradient values when it is set to `True`. You must store each new trained tree in the `trees_` list.\n",
    "    - The method `predict_proba` returns a linear combination of predictions over the trees. Don't forget the initial approximation. In the case of regression, the name of the method is somewhat misleading, but let's keep it so that we do not have to implement it twice for both the regressor and classifier. In the case of classification, apply $\\sigma$-transformation to the returned value. In the impementation of the $\\sigma$-function, replace arguments with absolute values larger than $100$ with $100$ or $-100$, depending on the sign of the argument, to prevent underflow or overflow.\n",
    "    - In the case of regression, method `predict` returns a linear combination of predictions over all the trees (plus the initial aproximation) i.e. the same as the method `predict_proba`. In the case of classification, `predict` uses `predict_proba` and returns a vector composed of $0$s and $1$s obtained by comparing the predicted probabilities with a threshold that maximizes a share of correct answers on the training set. Here, it would be better to solve a one-dimensional optimization problem, but, for the sake of reproducibility, you should choose the threshold from `np.linspace(0.01, 1.01, 100)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import log_loss, mean_squared_error, roc_auc_score, accuracy_score\n",
    "from sklearn.datasets import load_breast_cancer, load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GradientBoosting(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def sigma(self, z):\n",
    "        pass\n",
    "    \n",
    "    def log_loss(self, y, p):\n",
    "        pass\n",
    "    \n",
    "    def log_loss_grad(self, y, p):\n",
    "        pass\n",
    "    \n",
    "    def mse_grad(self, y, p):\n",
    "        pass\n",
    "    \n",
    "    def rmsle(self, y, p):\n",
    "        pass\n",
    "    \n",
    "    def rmsle_grad(self, y, p):\n",
    "        pass\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "                 \n",
    "    def predict_proba(self, X):\n",
    "        pass\n",
    "        \n",
    "    def predict(self, X):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Regression with a toy example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10afd2ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_regr_toy = np.arange(7).reshape(-1, 1)\n",
    "\n",
    "y_regr_toy = ((X_regr_toy - 3) ** 2).astype('float64')\n",
    "\n",
    "plt.scatter(X_regr_toy, y_regr_toy);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task:\n",
    "\n",
    "Train an instance of the `GradientBoosting` regressor with the loss function `MSE` and the following input parameters: `learning_rate=0.1`, `max_depth=3`, `n_estimators=200`. Then, plot the change of the loss function over boosting iterations. You could also visualize the initial approximation and pseudo residuals on the first iterations as done in the article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task:\n",
    "\n",
    "Train another `GradientBoosting` regressor with the same input parameters, but change the loss function to `RMSLE`. Plot the same values as in the previous task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with a toy example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_clf_toy = np.c_[np.arange(7), (np.arange(7) - 3) ** 2]\n",
    "y_clf_toy = np.array([0, 1, 0, 1, 0, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1071bf7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_clf_toy[:, 0], X_clf_toy[:, 1], c=y_clf_toy,\n",
    "            s=100, edgecolors='black', linewidth=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task:\n",
    "\n",
    "Train a classifier of type `GradientBoosting` with the loss function `log_loss` and the following parameters: `learning_rate=0.05`, `max_depth=3`, `n_estimators=10`. Then, plot the change of the loss function over boosting iterations. You could also visualize the initial approximation and pseudo residuals on the first iterations as done in the article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.** For all $7$ objects in the toy dataset, calculate the predicted probabilities of being in the class $+1$? What are the two unique values in the computed vector?\n",
    "\n",
    "1. $0.42$ and $0.77$\n",
    "2. $0.36$ and $0.82$\n",
    "3. $0.48$ and $0.53$\n",
    "4. $0.46$ and $0.75$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression with the Boston House-Prices Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task:\n",
    "\n",
    "- Train a `GradientBoosting` regressor with the loss function `MSE` and the following parameters: `learning_rate=3`,  `max_depth=10`, `n_estimators=300`.\n",
    "- Plot the change of the loss function over boosting iterations.\n",
    "- Make predictions on the test set.\n",
    "- Plot the distribution of `y_test`, which are the output variable values from the training set, along with the distribution of `test_pred`, which contains the values predicted by gradient boosting. Use the method `hist` from `matplotlib.pyplot` with the parameter `bins=15`.\n",
    "\n",
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.** Choose the correct statement regarding the resulting histograms:\n",
    "1. On average, boosting predictions are overestimated by 10.\n",
    "2. In the bin that contains the median of the answers on the test set (i.e. `numpy.median(y_test)`), there are more values from the vector of predictions `test_pred` than from the vector of answers `y_test`.\n",
    "3. Sometimes our boosting algorithm predicts values that are far beyond the range of the `y_test` vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with the Breast Cancer Wisconsin Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()\n",
    "X, y = cancer.data, cancer.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task:\n",
    "\n",
    "- Train a `GradientBoosting` classifier with the loss function `log_loss` and the parameters `learning_rate=0.01`, `max_depth=3`, `n_estimators=200`.\n",
    "- Plot the change of the loss function over boosting iterations.\n",
    "- Make predictions on the test set: both the probabilities of being in the class $+1$ and binary predictions. \n",
    "- Calculate ROC AUC for the case of probabilites and the share of correct answers for binary predictions.\n",
    "\n",
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6.** What are the ROC AUC value and the share of correct predictions on the test set `(X_test, y_test)`?\n",
    "1. $0.99$ and $0.97$\n",
    "2. $1$ and $0.97$\n",
    "3. $0.98$ and $0.96$\n",
    "4. $0.97$ and $0.95$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
