{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Logistic Regression and maximum likelihood estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea behind the linear classifier is that the two values of the target class can be separated by a hyperplane in the feature space. If this can be done without error, the training set is called *linearly separable*.\n",
    "\n",
    "<img src=\"../../img/logit.png\">\n",
    "\n",
    "We have already seen linear regression and Ordinary Least Squares (OLS). Let's consider a binary classification problem, and denote target classes to be \"+1\" (positive examples) and \"-1\" (negative examples). One of the simplest linear classifiers can be defined using regression as follows:\n",
    "\n",
    "$$a(\\vec{x}) = sign(\\vec{w}^Tx),$$\n",
    "\n",
    "where\n",
    " - $\\vec{x}$ –  feature vector (along with identity);\n",
    " - $\\vec{w}$ – weights in the linear model (with bias $w_0$);\n",
    " - $sign(\\bullet)$ – the signum function that returns the sign of its argument;\n",
    " - $a(\\vec{x})$ – classifier response for $\\vec{x}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression as a Linear Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a special case of the linear classifier, but it has an added benefit of predicting a probability $p_+$ of referring example $\\vec{x_i}$ to the class \"+\":\n",
    "$$p_+ = P\\left(y_i = 1 \\mid \\vec{x_i}, \\vec{w}\\right) $$\n",
    "\n",
    "Being able to predict not just a response ( \"+1\" or \"-1\") but the *probability* of assignment to class \"+1\" is a very important requirement in many business problems e.g. credit scoring where logistic regression is traditionally used. Customers who have applied for a loan are ranked based on this predicted probability (in descending order) to obtain a scoreboard that rates customers from bad to good. Below is an example of such a toy scoreboard.\n",
    "    <img src='../../img/toy_scorecard.png' width=60%>\n",
    "\n",
    "The bank chooses a threshold $p_*$ to predict the probability of loan default (in the picture it's $0.15$) and stops approving loans starting from that value. Moreover, it is possible to multiply this predicted probability by the outstanding amount to get the expectation of losses from the client, which can also constitute good business metrics (scoring experts may have more to add, but the main gist is this).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the probability $p_+ \\in [0,1]$, we can start by constructing a linear prediction using OLS: $b(\\vec{x}) = \\vec{w}^T \\vec{x} \\in \\mathbb{R}$. But converting the resulting value to the probability within in the [0, 1] range requires some function $f: \\mathbb{R} \\rightarrow [0,1].$. Logistic regression uses a specific function for this: $\\sigma(z) = \\frac{1}{1 + \\exp^{-z}}$. Now let's understand what the prerequisites are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "# turning off Anaconda's warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigma(z):\n",
    "    return 1. / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAETCAYAAAA/NdFSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113cc4518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xx = np.linspace(-10, 10, 1000)\n",
    "plt.plot(xx, [sigma(x) for x in xx]);\n",
    "plt.xlabel('z');\n",
    "plt.ylabel('sigmoid(z)')\n",
    "plt.title('Sigmoid function');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's denote the probability of an event $X$ as $P(X)$. Then the odds ratio $OR(X)$ is determined by $\\frac{P(X)}{1-P(X)}$, which is the ratio of the probabilities of whether or not an event will happen. It is obvious that the probability and odds ratio contain the same information, but, while $P(X)$ ranges from 0 to 1, $OR(X)$ is in the range of 0 to $\\infty$.\n",
    "\n",
    "If we calculate the logarithm of $OR(X)$ (a logarithm of odds or log probability ratio), it is easy to notice that $\\log{OR(X)} \\in \\mathbb{R}$. This is what we will use with OLS.\n",
    "\n",
    "Let's see how logistic regression will make a prediction $p_+ = P\\left(y_i = 1 \\mid \\vec{x_i}, \\vec{w}\\right)$. (For now, let's assume that we have somehow obtained weights $\\vec{w}$ i.e. trained the model. Later, we'll look at how it is done.)\n",
    "\n",
    "**Step 1.** Calculate $w_{0}+w_{1}x_1 + w_{2}x_2 + ... = \\vec{w}^T\\vec{x}$. (Equation $\\vec{w}^T\\vec{x} = 0$ defines a hyperplane separating the examples into two classes);\n",
    "\n",
    "**Step 2.** Compute the log odds ratio: $ \\log(OR_{+}) = \\vec{w}^T\\vec{x}$.\n",
    "\n",
    "**Step 3.** Now that we have the chance of assigning an example to the class of \"+\" - $OR_{+}$, calculate $p_{+}$ using the simple relationship:\n",
    "\n",
    "$$\\large p_{+} = \\frac{OR_{+}}{1 + OR_{+}} = \\frac{\\exp^{\\vec{w}^T\\vec{x}}}{1 + \\exp^{\\vec{w}^T\\vec{x}}} = \\frac{1}{1 + \\exp^{-\\vec{w}^T\\vec{x}}} = \\sigma(\\vec{w}^T\\vec{x})$$\n",
    "\n",
    "On the right side, you can see that we have the sigmoid function.\n",
    "\n",
    "So, logistic regression predicts the probability of assigning an example to the \"+\" class (assuming that we know the features and weights of the model) as a sigmoid transformation of a linear combination of the weight vector and the feature vector:\n",
    "\n",
    "$$\\large p_+(x_i) = P\\left(y_i = 1 \\mid \\vec{x_i}, \\vec{w}\\right) = \\sigma(\\vec{w}^T\\vec{x_i}). $$\n",
    "\n",
    "Next, we will see how the model is trained. We will again rely on maximum likelihood estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Estimation and Logistic Regression\n",
    "\n",
    "Now let's see how an optimization problem for logistic regression is obtained from the MLE, namely, minimization of the *logistic* loss function. We have just seen that logistic regression models the probability of assigning an example to the class \"+\" as:\n",
    "\n",
    "\n",
    "$$p_+(\\vec{x_i}) = P\\left(y_i = 1 \\mid \\vec{x_i}, \\vec{w}\\right) = \\sigma(\\vec{w}^T\\vec{x_i})$$\n",
    "\n",
    "Тhen, for the class \"-\", the corresponding expression is as follows:\n",
    "$$p_-(\\vec{x_i})  = P\\left(y_i = -1 \\mid \\vec{x_i}, \\vec{w}\\right)  = 1 - \\sigma(\\vec{w}^T\\vec{x_i}) = \\sigma(-\\vec{w}^T\\vec{x_i}) $$\n",
    "\n",
    "Both of these expressions can be cleverly combined into one (watch carefully, maybe you are being tricked):\n",
    "\n",
    "$$P\\left(y = y_i \\mid \\vec{x_i}, \\vec{w}\\right) = \\sigma(y_i\\vec{w}^T\\vec{x_i})$$\n",
    "\n",
    "The expression $M(\\vec{x_i}) = y_i\\vec{w}^T\\vec{x_i}$ is known as the margin of classification on the object $\\vec{x_i}$ (not to be confused with a gap, which is also called margin, in the SVM context). If it is non-negative, the model is correct in choosing the class of the object $\\vec{x_i}$; if it is negative, then the object $\\vec{x_i}$ is misclassified. Note that the margin is defined for objects in the training set only where real target class labels $y_i$ are known.\n",
    "\n",
    "To understand exactly why we have come to such a conclusion, let us turn to the geometrical interpretation of the linear classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First, I would recommend looking at a classic, introductory problem in linear algebra: find the distance from the point with a radius-vector $\\vec{x_A}$ to a plane defined by the equation $\\vec{w}^T\\vec{x} = 0.$\n",
    "\n",
    "\n",
    "<spoiler title='Answer'>\n",
    "$\\rho(\\vec{x_A}, \\vec{w}^T\\vec{x} = 0) = \\frac{\\vec{w}^T\\vec{x_A}}{||\\vec{w}||}$\n",
    "</spoiler>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = '../../img/simple_linal_task.png' width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we get to the answer, we will understand that the greater the absolute value of the expression $\\vec{w}^T\\vec{x_i}$, the farther the point $\\vec{x_i}$ is from the plane $\\vec{w}^T\\vec{x} = 0.$\n",
    "\n",
    "Hence, our expression $M(\\vec{x_i}) = y_i\\vec{w}^T\\vec{x_i}$ is a kind of \"confidence\" in our model's classification of the object $\\vec{x_i}$:\n",
    "\n",
    "- if the margin is large (in absolute value) and positive, the class label is set correctly, and the object is far away from the separating hyperplane i.e. classified confidently. See Point $x_3$ on the picture;\n",
    "- if the margin is large (in absolute value) and negative, then class label is set incorrectly, and the object is far from the separating hyperplane (the object is most likely an anomaly; for example, it could be improperly labeled in the training set). See Point $x_1$ on the picture;\n",
    "- if the margin is small (in absolute value), then the object is close to the separating hyperplane, and the margin sign determines whether the object is correctly classified. See Points $x_2$ and $x_4$ on the plot;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = '../../img/margin.png' width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now compute the likelihood of the data set i.e. the probability of observing the given vector $\\vec{y}$ from data set $X$. We'll make a strong assumption: objects come independently from one distribution (*i.i.d.*). Then, we can write\n",
    "\n",
    "$$P\\left(\\vec{y} \\mid X, \\vec{w}\\right) = \\prod_{i=1}^{\\ell} P\\left(y = y_i \\mid \\vec{x_i}, \\vec{w}\\right),$$\n",
    "\n",
    "where $\\ell$ is the length of data set $X$ (number of rows).\n",
    "\n",
    "As usual, let's take the logarithm of this expression because a sum is much easier to optimize than the product:\n",
    "\n",
    "$$\\log P\\left(\\vec{y} \\mid X, \\vec{w}\\right) = \\log \\sum_{i=1}^{\\ell} P\\left(y = y_i \\mid \\vec{x_i}, \\vec{w}\\right) = \\log \\prod_{i=1}^{\\ell} \\sigma(y_i\\vec{w}^T\\vec{x_i})   = $$\n",
    "\n",
    "$$ = \\sum_{i=1}^{\\ell} \\log \\sigma(y_i\\vec{w}^T\\vec{x_i}) = \\sum_{i=1}^{\\ell} \\log \\frac{1}{1 + \\exp^{-y_i\\vec{w}^T\\vec{x_i}}} = - \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\vec{w}^T\\vec{x_i}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximizing the likelihood is equivalent to minimizing the expression:\n",
    "\n",
    "$$\\mathcal{L_{log}} (X, \\vec{y}, \\vec{w}) = \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\vec{w}^T\\vec{x_i}}).$$\n",
    "\n",
    "This is *logistic* loss function that is summed over all objects in the training set.\n",
    "\n",
    "Let's look at the new function as a function of margin $L(M) = \\log (1 + \\exp^{-M})$ and plot it along with *zero-one loss* graph, which simply penalizes the model for error on each object by 1 (negative margin): $L_{1/0}(M) = [M < 0]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = '../../img/logloss_margin.png' width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The picture reflects the idea that, if we are not able to directly minimize the number of errors in the classification problem (at least not by gradient methods - derivative of the zero-one loss function at zero turns to infinity), we can minimize its upper bounds. For the logistic loss function (where the logarithm is binary, but this does not matter), the following is valid:\n",
    "\n",
    "$$\\mathcal{L_{1/0}} (X, \\vec{y}, \\vec{w}) = \\sum_{i=1}^{\\ell} [M(\\vec{x_i}) < 0] \\leq \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\vec{w}^T\\vec{x_i}}) = \\mathcal{L_{log}} (X, \\vec{y}, \\vec{w}), $$\n",
    "\n",
    "where $\\mathcal{L_{1/0}} (X, \\vec{y})$ is simply the number of errors of logistic regression with weights $\\vec{w}$ on a data set $(X, \\vec{y})$.\n",
    "\n",
    "Thus, by reducing the upper bound of $\\mathcal{L_{log}}$ by the number of classification errors, we hope to reduce the number of errors itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $L_2$-Regularization of Logistic Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2-regularization of logistic regression is almost the same as ridge regression. Instead of the function $\\mathcal{L_{log}} (X, \\vec{y}, \\vec{w})$ we minimize the following:\n",
    "\n",
    "$$\\large J(X, \\vec{y}, \\vec{w}) = \\mathcal{L_{log}} (X, \\vec{y}, \\vec{w}) + \\lambda |\\vec{w}|^2$$\n",
    "\n",
    "In the case of logistic regression, a reverse regularization coefficient $C = \\frac{1}{\\lambda}$ is typically introduced. Then the solution to the problem would be:\n",
    "\n",
    "$$\\hat{w}  = \\arg \\min_{\\vec{w}} J(X, \\vec{y}, \\vec{w}) =  \\arg \\min_{\\vec{w}}\\ (C\\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\vec{w}^T\\vec{x_i}})+ |\\vec{w}|^2)$$ \n",
    "\n",
    "Next, we'll look at an example that allows us to intuitively understand one of the interpretations of regularization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
