{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\" />\n",
    "    \n",
    "## [mlcourse.ai](https://mlcourse.ai) – Open Machine Learning Course \n",
    "\n",
    "Author: [Yury Kashnitskiy](https://yorko.github.io). This material is subject to the terms and conditions of the [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) license. Free use is permitted for any non-commercial purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Topic 7. Unsupervised learning\n",
    "## <center>Part 1. Principal Component Analysis\n",
    "    \n",
    "**This is mostly to demonstrate some applications of PCA, for theory, study [topic 7](https://mlcourse.ai/notebooks/blob/master/jupyter_english/topic07_unsupervised/topic7_pca_clustering.ipynb?flush_cache=true) in our course. <br> The notebook is used in lecture 7 recording, [part 1](https://youtu.be/-AswHf7h0I4).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Singular value decomposition (SVD) of matrix $X$:\n",
    "\n",
    "$$\\Large X = UDV^T,$$\n",
    "\n",
    "where $U \\in R^{m \\times m}$,  $V \\in R^{n \\times n}$ - are orthogonal matrices, and $D \\in R^{m \\times n}$ - is a diagonal matrix\n",
    "\n",
    "<img src='../../img/svd_diag_matrix.png' width=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Components Analysis\n",
    "1. Define $k<d$ – a new dimension\n",
    "2. Scale $X$: \n",
    " - change $\\Large x^{(i)}$ to $\\Large  x^{(i)} - \\frac{1}{m} \\sum_{i=1}^{m}{x^{(i)}}$\n",
    " - $\\Large  \\sigma_j^2 = \\frac{1}{m} \\sum_{i=1}^{m}{(x^{(i)})^2}$\n",
    " - change $\\Large x_j^{(i)}$ to $\\Large \\frac{x_j^{(i)}}{\\sigma_j}$ \n",
    "4. Find singular value decomposition of $X$:\n",
    "$$\\Large X = UDV^T$$\n",
    "5. Define $V =$ [$k$ left-most columns of $V$]\n",
    "6. Return $$\\Large Z = XV \\in \\mathbb{R}^{m \\times k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "mean = np.array([0.0, 0.0])\n",
    "cov = np.array([[1.0, -1.0], \n",
    "                [-2.0, 3.0]])\n",
    "X = np.random.multivariate_normal(mean, cov, 300)\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "print('Proportion of variance explained by each component:\\n' +\\\n",
    "      '1st component - %.2f,\\n2nd component - %.2f\\n' %\n",
    "      tuple(pca.explained_variance_ratio_))\n",
    "print('Directions of principal components:\\n' +\\\n",
    "      '1st component:', pca.components_[0],\n",
    "      '\\n2nd component:', pca.components_[1])\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50, c='r')\n",
    "for l, v in zip(pca.explained_variance_ratio_, pca.components_):\n",
    "    d = 5 * np.sqrt(l) * v\n",
    "    plt.plot([0, d[0]], [0, d[1]], '-k', lw=3)\n",
    "plt.axis('equal')\n",
    "plt.title('2D normal distribution sample and its principal components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1st principal component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep enough components to explain 90% of variance\n",
    "pca = PCA(0.90)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "# Map the reduced data into the initial feature space\n",
    "X_new = pca.inverse_transform(X_reduced)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(X[:, 0], X[:, 1], 'or', alpha=0.3)\n",
    "plt.plot(X_new[:, 0], X_new[:, 1], 'or', alpha=0.8)\n",
    "plt.axis('equal')\n",
    "plt.title('Projection of sample onto the 1st principal component')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have reduced the feature space dimensionality by half, while retaining the most significant features. This is the basic idea of dimensionality reduction - to approximate a multidimensional data set using data of a smaller dimension, while preserving as much information about the dataset as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing high-dimensional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Iris example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "print(\"Meaning of the 2 components:\")\n",
    "for component in pca.components_:\n",
    "    print(\" + \".join(\"%.3f x %s\" % (value, name)\n",
    "                     for value, name in zip(component,\n",
    "                                            iris.feature_names)))\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, s=70, cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handwritten digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "print('Projecting %d-dimensional data to 2D' % X.shape[1])\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, \n",
    "            edgecolor='none', alpha=0.7, s=40,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at first 2 principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "\n",
    "im = pca.components_[0]\n",
    "ax1.imshow(im.reshape((8, 8)), cmap='binary')\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "ax1.set_title('First principal component')\n",
    "\n",
    "im = pca.components_[1]\n",
    "ax2.imshow(im.reshape((8, 8)), cmap='binary')\n",
    "ax2.set_xticks([])\n",
    "ax2.set_yticks([])\n",
    "ax2.set_title('Second principal component')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Data compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,2))\n",
    "plt.imshow(X[15].reshape((8, 8)), cmap='binary')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title('Source image')\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(8, 8, figsize=(10, 10))\n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    pca = PCA(i + 1).fit(X)\n",
    "    im = pca.inverse_transform(pca.transform(X[15].reshape(1, -1)))\n",
    "\n",
    "    ax.imshow(im.reshape((8, 8)), cmap='binary')\n",
    "    ax.text(0.95, 0.05, 'n = {0}'.format(i + 1), ha='right',\n",
    "            transform=ax.transAxes, color='red')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many components to choose? Typically, they leave at least 90% of data variance to be preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(X)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_), color='k', lw=2)\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Total explained variance')\n",
    "plt.xlim(0, 63)\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.axvline(21, c='b')\n",
    "plt.axhline(0.9, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(0.9).fit(X)\n",
    "print('We need %d components to explain 90%% of variance' \n",
    "      % pca.n_components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA as preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA often serves as a preprocessing technique. Let's take a look at a face recognition task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "lfw_people = datasets.fetch_lfw_people(min_faces_per_person=50, \n",
    "                resize=0.4, data_home='../../data/faces')\n",
    "\n",
    "print('%d objects, %d features, %d classes' % (lfw_people.data.shape[0],\n",
    "      lfw_people.data.shape[1], len(lfw_people.target_names)))\n",
    "print('\\nPersons:')\n",
    "for name in lfw_people.target_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target class distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, name in enumerate(lfw_people.target_names):\n",
    "    print(\"{}: {} photos.\".format(name, (lfw_people.target == i).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "for i in range(15):\n",
    "    ax = fig.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(lfw_people.images[i], cmap='bone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(lfw_people.data, lfw_people.target, random_state=0)\n",
    "\n",
    "print('Train size:', X_train.shape[0], 'Test size:', X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we resort to `RandomizedPCA` (a bit more efficient algorithm) to find first 100 principal components which retain >90% variance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=100, svd_solver='randomized')\n",
    "pca.fit(X_train)\n",
    "\n",
    "print('100 principal components explain %.2f%% of variance' %\n",
    "      (100 * np.cumsum(pca.explained_variance_ratio_)[-1]))\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_), lw=2, color='k')\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Total explained variance')\n",
    "plt.xlim(0, 100)\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First principal components. Nice features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 6))\n",
    "for i in range(30):\n",
    "    ax = fig.add_subplot(3, 10, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(pca.components_[i].reshape((50, 37)), cmap='bone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Mean face\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pca.mean_.reshape((50, 37)), cmap='bone')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's perform classification after we reduced dimensionality from 1850 features to only 100.\n",
    "We'll fit a softmax classifier, a.k.a multinomial logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(multi_class='multinomial', \n",
    "                         random_state=17, solver='lbfgs', \n",
    "                         max_iter=10000)\n",
    "clf.fit(X_train_pca, y_train)\n",
    "y_pred = clf.predict(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix)\n",
    "\n",
    "print(\"Accuracy: %f\" % accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, target_names=lfw_people.target_names))\n",
    "\n",
    "M = confusion_matrix(y_test, y_pred)\n",
    "M_normalized = M.astype('float') / M.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "im = plt.imshow(M_normalized, interpolation='nearest', cmap='Greens')\n",
    "plt.colorbar(im, shrink=0.71)\n",
    "tick_marks = np.arange(len(lfw_people.target_names))\n",
    "plt.xticks(tick_marks - 0.5, lfw_people.target_names, rotation=45)\n",
    "plt.yticks(tick_marks, lfw_people.target_names)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True person')\n",
    "plt.xlabel('Predicted person')\n",
    "plt.title('Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this result with the case when we train the model with all initial 1850 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf = LogisticRegression(multi_class='multinomial', \n",
    "                         random_state=17, solver='lbfgs', \n",
    "                         max_iter=10000)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_full = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: %f\" % accuracy_score(y_test, y_pred_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well.. didn't work out as a cool example for illustration, logistic regression is still trained pretty fast with 1850 features, but accuracy is 79%, quite remarkably higher than 72%. But for images of higher resolution, and with heavier models, the difference in training time will be much more remarkable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some references\n",
    "- [PCA wiki page](https://en.wikipedia.org/wiki/Principal_component_analysis)\n",
    "- [PCA in 3 steps](http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html)\n",
    "- [SVD wiki page](https://en.wikipedia.org/wiki/Singular_value_decomposition)\n",
    "- [Eigenface](https://en.wikipedia.org/wiki/Eigenface)\n",
    "- [sklearn.decomposition](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "name": "lesson8_part2_PCA.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
