{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\" />\n",
    "    \n",
    "## [mlcourse.ai](mlcourse.ai) – Open Machine Learning Course \n",
    "Author: [Yury Kashnitskiy](https://yorko.github.io) (@yorko). Translated by [Eugene Mashkin](https://www.linkedin.com/in/eugene-mashkin-88490883/) (@emashkin). This material is subject to the terms and conditions of the [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) license. Free use is permitted for any non-commercial purpose. This material is a translated version of the Capstone project (by the same author) from specialization \"Machine learning and data analysis\" by Yandex and MIPT. No solutions shared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Project \"Alice\". User Identification Based on Visited Websites\n",
    "\n",
    "\n",
    "In this project, we are going to tackle a user identification problem using their behavioral patterns on the Internet. It is a complicated and interesting problem that combines data analysis and behavioral psychology.\n",
    "As an example, Yandex solves a mailbox intruder detection problem. In a nutshell, intruder's patterns differ from that of the mailbox owner:\n",
    "\n",
    "- intruder might not delete emails right after they are read as the mailbox owner would\n",
    "- intruder might mark emails or even move the cursor differently\n",
    "- etc.\n",
    " \n",
    "Therefore, the intruder could be detected and thrown out from the mailbox, forcing the user to authenticate via SMS-code. This pilot project is described in the Habrahabr [article](https://habrahabr.ru/company/yandex/blog/230583/) (in Russian). \n",
    " \n",
    "Similar things are being developed in Google Analytics and, generally, in academia. You can find more by searching for \"Traversal Pattern Mining\" or  \"Sequential Pattern Mining\".\n",
    "\n",
    "<img src='../../img/stock-illustration-21546327-identification-de-l-utilisateur.jpg'>\n",
    "\n",
    "In this project, we are going to solve a similar problem: identify a user given a series of consequently visited. The main idea is the following: Internet users have different patterns in visiting websites: some person might check mailbox first, then read latest football news, and only after that get down to business, another person might get to work right away. \n",
    "\n",
    "Our algorithm needs to analyze the sequence of websites consequently visited by a particular person and predict whether this person is Alice or someone else. We will measure ROC AUC. Stay tuned until the end of the course to find out who Alice is.\n",
    "\n",
    "We will use the data from the paper [\"A Tool for Classification of Sequential Data\"](http://ceur-ws.org/Vol-1703/paper12.pdf). Even though we can't recommend this paper (the described methods are far from state-of-the-art, it's better to refer to the [\"Frequent Pattern Mining\"](http://www.charuaggarwal.net/freqbook.pdf) book and the latest ICDM papers), but the dataset is carefully collected and is therefore of interest.\n",
    "\n",
    "This is data from Blaise Pascal University's proxy-servers. The structure is crystal clear. There is a file `user[USER_ID].csv` for each user (where [USER_ID] - is an id of a particular user). All website visits have the following format: <br><br>\n",
    "\n",
    "<center>timestamp, visited website</center>\n",
    "\n",
    "You can download the data using the link provided in the article. Data description could be found there as well. Using the data of all 3000 users is not necessary for this project. Data of 10 and 150 users will be enough. Capstone_user_identification archive [link](https://yadi.sk/d/3gscKIdN3BCASG) (~7 Mb, unziped data ~60 Mb).\n",
    "\n",
    "In the final project, you'll face an issue that not all operations can be executed within a reasonable time (e.g. it's unlikely you will be able to perform cross-validation grid search over 100 combinations of random forest parameters on this data). Therefore we are going to use two sets of data:\n",
    "- data of 10 users - we will use it to test and debug our code\n",
    "- data of 150 users - it will be our main working data\n",
    "\n",
    "The data has the following\n",
    "- there are 10 `user[USER_ID].csv` files in `10users` directory\n",
    "- the same for the `150users` directory - but there 150 files in total\n",
    "- there is a toy example of 3 users data in the `3users` directory - use it to debug your preprocessing code, which you are supposed to develop further\n",
    "\n",
    "\n",
    "Your work on this project can be nicely coupled with participating in [this](https://www.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2) Kaggle Inclass competition. Check out [mlcourse.ai roadmap](https://mlcourse.ai/roadmap) for more info about grading. \n",
    "\n",
    "\n",
    "\n",
    "## <center> Week 1. Preparing the training set\n",
    "\n",
    "The fisrt part of the project is devoted to data preparation for further descriptive analysis and predictive model development. You need to write a code for data preprocessing and producing a single training set (initially the data is stored in separate files). Also we will learn sparse data format (`Scipy.sparse` matricies), which is well-suited for this purpose.\n",
    "\n",
    "\n",
    "**Week 1 roadmap**\n",
    " - Part 1. Training set preparation\n",
    " - Part 2. Working with sparse data format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your task**\n",
    "1. Fill in th missing code in the provided notebook\n",
    "2. Choose the answers in the [form](https://docs.google.com/forms/d/10yakW9zN85pTVnTo9uStzdAUlqj8nARwIhZxlT4stZk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You might find the following materials useful:**\n",
    "- [Loops](https://www.datacamp.com/community/tutorials/loops-python-tutorial), [functions](https://www.datacamp.com/community/tutorials/functions-python-tutorial), [generators](https://www.learnpython.org/en/Generators), [list_comprehension](https://www.datacamp.com/community/tutorials/python-list-comprehension)\n",
    "- [Reading and writing data from/to files in python](https://www.datacamp.com/community/tutorials/reading-writing-files-python)\n",
    "- [Pandas Tutorial: DataFrames in Python](https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python)\n",
    "   \n",
    "**Besides, we are going to use [`pickle`](https://docs.python.org/2/library/pickle.html), [`glob`](https://docs.python.org/3/library/glob.html) and `Scipy.sparse` ([`csr_matrix`](https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.sparse.csr_matrix.html) class) python libraries.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's list versions of all main libraries used in this project for better results reproducibility: NumPy, SciPy, Pandas, Matplotlib, Statsmodels and Scikit-learn. To do this use [watermark](https://github.com/rasbt/watermark) extension. It is recommended to use Open Data Science docker container with all  libraries and dependecies preinstalled (details are [here](https://github.com/Yorko/mlcourse.ai/wiki/Software-requirements-and-Docker-container)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pip install watermark\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%watermark -v -m -p numpy,scipy,pandas,matplotlib,statsmodels,sklearn -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Disable Anaconda warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from glob import glob\n",
    "import os\n",
    "import pickle\n",
    "# pip install tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's look at one of the files containing the data about websites visited by a particular user (user 31).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change the path to data\n",
    "PATH_TO_DATA = '../../data/capstone_user_identification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user31_data = pd.read_csv(os.path.join(PATH_TO_DATA, \n",
    "                                       '10users/user0031.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-11-15 08:12:07</td>\n",
       "      <td>fpdownload2.macromedia.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-11-15 08:12:17</td>\n",
       "      <td>laposte.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-11-15 08:12:17</td>\n",
       "      <td>www.laposte.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-11-15 08:12:17</td>\n",
       "      <td>www.google.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-11-15 08:12:18</td>\n",
       "      <td>www.laposte.net</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp                        site\n",
       "0  2013-11-15 08:12:07  fpdownload2.macromedia.com\n",
       "1  2013-11-15 08:12:17                 laposte.net\n",
       "2  2013-11-15 08:12:17             www.laposte.net\n",
       "3  2013-11-15 08:12:17              www.google.com\n",
       "4  2013-11-15 08:12:18             www.laposte.net"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user31_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's define the problem: identify a user given a session containing 10 consequentially visited websites. Each sample is a session of 10 websites consequentially visited by a particular user, features are indexes of these 10 websites (later, we will get a bag of websites applying bag-of-words approach). Target class is the user id.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Toy example</center>\n",
    "**Suppose there are only 2 users, and session length is 2 (websites).**\n",
    "\n",
    "<center>user0001.csv</center>\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg .tg-yw4l{vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-031e\">timestamp</th>\n",
    "    <th class=\"tg-031e\">site</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-031e\">00:00:01</td>\n",
    "    <td class=\"tg-031e\">vk.com</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">00:00:11</td>\n",
    "    <td class=\"tg-yw4l\">google.com</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-031e\">00:00:16</td>\n",
    "    <td class=\"tg-031e\">vk.com</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-031e\">00:00:20</td>\n",
    "    <td class=\"tg-031e\">yandex.ru</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<center>user0002.csv</center>\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg .tg-yw4l{vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-031e\">timestamp</th>\n",
    "    <th class=\"tg-031e\">site</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-031e\">00:00:02</td>\n",
    "    <td class=\"tg-031e\">yandex.ru</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">00:00:14</td>\n",
    "    <td class=\"tg-yw4l\">google.com</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-031e\">00:00:17</td>\n",
    "    <td class=\"tg-031e\">facebook.com</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-031e\">00:00:25</td>\n",
    "    <td class=\"tg-031e\">yandex.ru</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Iterate through the first file and assign consequtive site_id-s (numbers) to each new incoming websites: vk.com – gets site_id=1, google.com – gets site_id=2, etc. Then, iterate through the second file. So we collect a bag of websites. \n",
    "\n",
    "You are supposed to get the following mapping:\n",
    "\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg .tg-yw4l{vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-031e\">site</th>\n",
    "    <th class=\"tg-yw4l\">site_id</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">vk.com</td>\n",
    "    <td class=\"tg-yw4l\">1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">google.com</td>\n",
    "    <td class=\"tg-yw4l\">2</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">yandex.ru</td>\n",
    "    <td class=\"tg-yw4l\">3</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">facebook.com</td>\n",
    "    <td class=\"tg-yw4l\">4</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Then your training set should be (target class - user_id):\n",
    "\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg .tg-s6z2{text-align:center}\n",
    ".tg .tg-baqh{text-align:center;vertical-align:top}\n",
    ".tg .tg-hgcj{font-weight:bold;text-align:center}\n",
    ".tg .tg-amwm{font-weight:bold;text-align:center;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-hgcj\">session_id</th>\n",
    "    <th class=\"tg-hgcj\">site1</th>\n",
    "    <th class=\"tg-hgcj\">site2</th>\n",
    "    <th class=\"tg-amwm\">user_id</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-s6z2\">1</td>\n",
    "    <td class=\"tg-s6z2\">1</td>\n",
    "    <td class=\"tg-s6z2\">2</td>\n",
    "    <td class=\"tg-baqh\">1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-s6z2\">2</td>\n",
    "    <td class=\"tg-s6z2\">1</td>\n",
    "    <td class=\"tg-s6z2\">3</td>\n",
    "    <td class=\"tg-baqh\">1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-s6z2\">3</td>\n",
    "    <td class=\"tg-s6z2\">3</td>\n",
    "    <td class=\"tg-s6z2\">2</td>\n",
    "    <td class=\"tg-baqh\">2</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-s6z2\">4</td>\n",
    "    <td class=\"tg-s6z2\">4</td>\n",
    "    <td class=\"tg-s6z2\">3</td>\n",
    "    <td class=\"tg-baqh\">2</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "Here, the first sample is a session containing two websites visited by the first user (target=1). There are `vk.com` and `google.com` in it (site1=1 and site2=2). Four sessions in total. In this example sessions are not overlapping each other, i.e. each website belongs to only one session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Training set preparation\n",
    "\n",
    "Implement the function `prepare_train_set`, which takes as an input:\n",
    "- *path_to_csv_files* - path to the directory containing csv-files\n",
    "- *session_length* - number of websites in a session\n",
    "\n",
    "and returns two objects:\n",
    "- DataFrame with rows corresponding to a unique user session (each of *session_length* websites) and *session_length* columns corresponding to the websites in the session. And there should be one more column - *user_id* which is target class of a sample. Hence the DataFrame should have *session_length + 1* columns.\n",
    "- Dictionary of websites' frequncies which has {'site_string': [site_id, site_freq]} structure, where `site_string` - original website name, `site_id` - a number the website got be encoded, `site_freq` - total number of website occurrences in all files among all users. For our recent toy example it is:\n",
    "{'vk.com': (1, 2),<br> 'google.com': (2, 2),<br> 'yandex.ru': (3, 3),<br> 'facebook.com': (4, 1)}\n",
    "\n",
    "Details:\n",
    "- You can find an example of function's output below\n",
    "- Use `glob` (or somthing similar) to iterate through files in directory. For certainty, sort the list of files lexicographically. It is handy to use `tqdm_notebook` (or just `tqdm` in case of a python script) to track the number of loop iterations done\n",
    "- Create the dictionary of websites' frequencies (with the following structure: {'site_string': (site_id, site_freq)}) and fill it iterating through the file. Start with 1\n",
    "- It is recommended to give lesser indeces to more frequent websites \n",
    "- Don't do entity recognition, consider *google.com*, *http://www.google.com* and *www.google.com* as different websites (you can try entity recognition in scope of your individual work on the project)\n",
    "- It's likely that the number of records in a file is not divisible by *session_length*. In this case, the last session shall be shorter. Fill empty values with zeros. I.e. if there 24 recods in a file and session_length is 10, then third session will contain 4 websites and its corresponding vector will be:<br> \n",
    "[*site1_id*, *site2_id*, *site3_id*, *site4_id*, 0, 0, 0, 0, 0, 0, *user_id*] \n",
    "- Some sessions might be repeatitions of previous ones - leave it as is, don't drop duplicates. If there are identical web_sites in two sessions, but these sessions belong to different users, leave it as is as well. It's natural data uncertainty\n",
    "- Don't keep the website with `site_id`=0 in the dictionary (after your function returns that dictionary).\n",
    "- It took me 1.7 sec to process 150 files from *capstone_websites_data/150users/*, but, surely, it depends on your implementation of the function and hardware you use. Frankly, it's more likely that your first implementation won't be the most efficient one. As a next step, you can profile your code (especially if you plan to run your code on 3000 users data). Also, an efficient implementation of that function will help you next week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_train_set(path_to_csv_files, session_length=10):\n",
    "    ''' YOUR CODE IS HERE '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply your function to the toy example and make sure that everything works fine:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp,site\r\n",
      "2013-11-15 09:28:17,vk.com\r\n",
      "2013-11-15 09:33:04,oracle.com\r\n",
      "2013-11-15 09:52:48,oracle.com\r\n",
      "2013-11-15 11:37:26,geo.mozilla.org\r\n",
      "2013-11-15 11:40:32,oracle.com\r\n",
      "2013-11-15 11:40:34,google.com\r\n",
      "2013-11-15 11:40:35,accounts.google.com\r\n",
      "2013-11-15 11:40:37,mail.google.com\r\n",
      "2013-11-15 11:40:40,apis.google.com\r\n",
      "2013-11-15 11:41:35,plus.google.com\r\n",
      "2013-11-15 12:40:35,vk.com\r\n",
      "2013-11-15 12:40:37,google.com\r\n",
      "2013-11-15 12:40:40,google.com\r\n",
      "2013-11-15 12:41:35,google.com\r\n"
     ]
    }
   ],
   "source": [
    "!cat $PATH_TO_DATA/3users/user0001.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp,site\r\n",
      "2013-11-15 09:28:17,vk.com\r\n",
      "2013-11-15 09:33:04,oracle.com\r\n",
      "2013-11-15 09:52:48,football.kulichki.ru\r\n",
      "2013-11-15 11:37:26,football.kulichki.ru\r\n",
      "2013-11-15 11:40:32,oracle.com\r\n"
     ]
    }
   ],
   "source": [
    "!cat $PATH_TO_DATA/3users/user0002.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp,site\r\n",
      "2013-11-15 09:28:17,meduza.io\r\n",
      "2013-11-15 09:33:04,google.com\r\n",
      "2013-11-15 09:52:48,oracle.com\r\n",
      "2013-11-15 11:37:26,google.com\r\n",
      "2013-11-15 11:40:32,oracle.com\r\n",
      "2013-11-15 11:40:34,google.com\r\n",
      "2013-11-15 11:40:35,google.com\r\n",
      "2013-11-15 11:40:37,mail.google.com\r\n",
      "2013-11-15 11:40:40,yandex.ru\r\n",
      "2013-11-15 11:41:35,meduza.io\r\n",
      "2013-11-15 12:28:17,meduza.io\r\n",
      "2013-11-15 12:33:04,google.com\r\n",
      "2013-11-15 12:52:48,oracle.com\r\n"
     ]
    }
   ],
   "source": [
    "!cat $PATH_TO_DATA/3users/user0003.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "512943efe802447b912ba235e44202e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_data_toy, site_freq_3users = prepare_train_set(os.path.join(PATH_TO_DATA, '3users'), \n",
    "                                                     session_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>site2</th>\n",
       "      <th>site3</th>\n",
       "      <th>site4</th>\n",
       "      <th>site5</th>\n",
       "      <th>site6</th>\n",
       "      <th>site7</th>\n",
       "      <th>site8</th>\n",
       "      <th>site9</th>\n",
       "      <th>site10</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   site1  site2  site3  site4  site5  site6  site7  site8  site9  site10  \\\n",
       "0      3      2      2      8      2      1     10      5      7       9   \n",
       "1      3      1      1      1      0      0      0      0      0       0   \n",
       "2      3      2      6      6      2      0      0      0      0       0   \n",
       "3      4      1      2      1      2      1      1      5     11       4   \n",
       "4      4      1      2      0      0      0      0      0      0       0   \n",
       "\n",
       "   user_id  \n",
       "0        1  \n",
       "1        1  \n",
       "2        2  \n",
       "3        3  \n",
       "4        3  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_toy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Websites' frequencies (second element of a tuple) should be exactly the same. Numeration (websites order) (first elements of tuples) may vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'google.com': (1, 9),\n",
       " 'oracle.com': (2, 8),\n",
       " 'vk.com': (3, 3),\n",
       " 'meduza.io': (4, 3),\n",
       " 'mail.google.com': (5, 2),\n",
       " 'football.kulichki.ru': (6, 2),\n",
       " 'apis.google.com': (7, 1),\n",
       " 'geo.mozilla.org': (8, 1),\n",
       " 'plus.google.com': (9, 1),\n",
       " 'accounts.google.com': (10, 1),\n",
       " 'yandex.ru': (11, 1)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site_freq_3users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply your function to the data with 10 users.\n",
    "\n",
    "**<font color='red'> Question 1. </font>How many unique sessions with length of 10 websites are there in 10users data?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_10users, site_freq_10users = ''' YOUR CODE IS HERE '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='red'> Questioin 2. </font>How many unique websites are there in 10users data?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' YOUR CODE IS HERE '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply your function to the data with 150 users.\n",
    "\n",
    "**<font color='red'> Question 3. </font> How many unique sessions with length of 10 websites are there in 150users data?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train_data_150users, site_freq_150users = ''' YOUR CODE IS HERE '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='red'> Question 4. </font> How many unique websites are there in 150users data?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' YOUR CODE IS HERE '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='red'> Question 5. </font> \n",
    "Which of these websites is NOT in top-10 most visited websites among 150 users?**\n",
    "- www.google.fr\n",
    "- www.youtube.com\n",
    "- safebrowsing-cache.google.com\n",
    "- www.linkedin.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' YOUR CODE IS HERE '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write dataframes to csv files for further analysis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_10users.to_csv(os.path.join(PATH_TO_DATA, \n",
    "                                       'train_data_10users.csv'), \n",
    "                        index_label='session_id', float_format='%d')\n",
    "train_data_150users.to_csv(os.path.join(PATH_TO_DATA, \n",
    "                                        'train_data_150users.csv'), \n",
    "                         index_label='session_id', float_format='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Working with sparse data format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you think carefully, the features we've got - *site1*, ..., *site10* - won't work at all in a classification problem. But if we use the Bag of Words idea from text analysis - it's another story. Create new matrices with sessions as rows, and site_id as columns. The intersection of row $i$ and column $j$ is $n_{ij}$ - the number of website $j$ occurrences in session $i$. We are going to do this using sparse Scipy matrices - [csr_matrix](https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.sparse.csr_matrix.html). Read the docs, figure out how to use sparse matrices and create such matrices for our data. First, debug your code on toy example and then apply it to 10users and 150users data.\n",
    "\n",
    "Note, that in short sessions (less than 10 websites) there are trailing zeros, hence the meaning of the first feature (number of 0 occurrences in a session) differs from others (number of $i$-th website occurrences in a session). Therefore you need to drop the first column in a Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_toy, y_toy = train_data_toy.iloc[:, :-1].values, train_data_toy.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_toy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_sparse_toy = csr_matrix ''' YOUR CODE IS HERE '''   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of columns in `X_sparse_toy` should be 11, because in the toy example 3 users visited 11 unique websites.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_sparse_toy.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_10users, y_10users = train_data_10users.iloc[:, :-1].values, \\\n",
    "                       train_data_10users.iloc[:, -1].values\n",
    "X_150users, y_150users = train_data_150users.iloc[:, :-1].values, \\\n",
    "                         train_data_150users.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_sparse_10users = ''' YOUR CODE IS HERE '''\n",
    "X_sparse_150users = ''' YOUR CODE IS HERE '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save these sparse matrices with [pickle](https://docs.python.org/2/library/pickle.html) (serialization in Python), and *y_10users, y_150users* - target variables (user_id-s) for our 10users and 150users data. The fact that the names of these matrices start with X and y implies that we are going to check our first classification models on them.\n",
    "Finally, save frequency dictionaries for 3users, 10users and 150users data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH_TO_DATA, 'X_sparse_10users.pkl'), 'wb') as X10_pkl:\n",
    "    pickle.dump(X_sparse_10users, X10_pkl, protocol=2)\n",
    "with open(os.path.join(PATH_TO_DATA, 'y_10users.pkl'), 'wb') as y10_pkl:\n",
    "    pickle.dump(y_10users, y10_pkl, protocol=2)\n",
    "with open(os.path.join(PATH_TO_DATA, 'X_sparse_150users.pkl'), 'wb') as X150_pkl:\n",
    "    pickle.dump(X_sparse_150users, X150_pkl, protocol=2)\n",
    "with open(os.path.join(PATH_TO_DATA, 'y_150users.pkl'), 'wb') as y150_pkl:\n",
    "    pickle.dump(y_150users, y150_pkl, protocol=2)\n",
    "with open(os.path.join(PATH_TO_DATA, 'site_freq_3users.pkl'), 'wb') as site_freq_3users_pkl:\n",
    "    pickle.dump(site_freq_3users, site_freq_3users_pkl, protocol=2)\n",
    "with open(os.path.join(PATH_TO_DATA, 'site_freq_10users.pkl'), 'wb') as site_freq_10users_pkl:\n",
    "    pickle.dump(site_freq_10users, site_freq_10users_pkl, protocol=2)\n",
    "with open(os.path.join(PATH_TO_DATA, 'site_freq_150users.pkl'), 'wb') as site_freq_150users_pkl:\n",
    "    pickle.dump(site_freq_150users, site_freq_150users_pkl, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Just in case doublecheck that number of columns in sparse matrices `X_sparse_10users` and `X_sparse_150users` equals to the number of unique websites in 10users and 150users data evaluated earlier.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert X_sparse_10users.shape[1] == len(site_freq_10users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert X_sparse_150users.shape[1] == len(site_freq_150users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next week we will preprocess the data a bit and check first hypotheses regarding our observations."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
