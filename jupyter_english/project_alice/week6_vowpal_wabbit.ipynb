{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\" />\n",
    "    \n",
    "## [mlcourse.ai](https://mlcourse.ai) – Open Machine Learning Course \n",
    "Author: [Yury Kashnitskiy](https://yorko.github.io) (@yorko). This material is subject to the terms and conditions of the [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) license. Free use is permitted for any non-commercial purpose. This material is a translated version of the Capstone project (by the same author) from specialization \"Machine learning and data analysis\" by Yandex and MIPT. No solutions shared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Week 6. Vowpal Wabbit\n",
    "This week, we explore the popular library called Vowpal Wabbit and apply it to site visits data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Week 6 roadmap:\n",
    "- Part 1. Overview of Vowpal Wabbit\n",
    "- Part 2. Applying Vowpal Wabbit to site visits data\n",
    "    - 2.1 Data preprocssing\n",
    "    - 2.2 Holdout validation\n",
    "    - 2.3 Test set validation (on public leaderboard)\n",
    "    \n",
    "Resources: Vowpal Wabbit's [documentation](https://github.com/JohnLangford/vowpal_wabbit/wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The task \n",
    "1. Fill in code in this notebook\n",
    "2. Choose answers in the [webform](https://docs.google.com/forms/d/1VWfSupfYXvb6gyROR0enXYVMjuxqgRaTScYhEz4f6YQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Overview of Vowpal Wabbit\n",
    "\n",
    "Read the [article](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-8-vowpal-wabbit-fast-learning-with-gigabytes-of-data-60f750086237) on Vowpal Wabbit from the OpenDataScience machine learning course. Download the [notebook](https://mlcourse.ai/notebooks/blob/master/jupyter_english/topic08_sgd_hashing_vowpal_wabbit/topic8_sgd_hashing_vowpal_wabbit.ipynb?flush_cache=true), play with the code a bit - that is the most effective way to get started.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Applying Vowpal Wabbit to site visits data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will see Vowpal Wabbit in action. If we were to use in a binary classification task, we would not have noticed any difference neither in terms of accuracy nor in terms of speed. Instead, we will do 400-class classification. Source data is the same, but now we have 400 users and our goal is to identify each one of them. \n",
    "- Download the data from [here](https://www.kaggle.com/c/identify-me-if-you-can4/data) - files **train_sessions_400users.csv** and **test_sessions_400users.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to your data path\n",
    "PATH_TO_DATA = '../../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read train and test data. You may notice, that sessions in the test subset are spanning the different time period than train sessions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_400 = pd.read_csv(os.path.join(PATH_TO_DATA,'train_sessions_400users.csv'), \n",
    "                           index_col='session_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_400 = pd.read_csv(os.path.join(PATH_TO_DATA,'test_sessions_400users.csv'), \n",
    "                           index_col='session_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>time5</th>\n",
       "      <th>...</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23713</td>\n",
       "      <td>2014-03-24 15:22:40</td>\n",
       "      <td>23720.0</td>\n",
       "      <td>2014-03-24 15:22:48</td>\n",
       "      <td>23713.0</td>\n",
       "      <td>2014-03-24 15:22:48</td>\n",
       "      <td>23713.0</td>\n",
       "      <td>2014-03-24 15:22:54</td>\n",
       "      <td>23720.0</td>\n",
       "      <td>2014-03-24 15:22:54</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-03-24 15:22:55</td>\n",
       "      <td>23713.0</td>\n",
       "      <td>2014-03-24 15:23:01</td>\n",
       "      <td>23713.0</td>\n",
       "      <td>2014-03-24 15:23:03</td>\n",
       "      <td>23713.0</td>\n",
       "      <td>2014-03-24 15:23:04</td>\n",
       "      <td>23713.0</td>\n",
       "      <td>2014-03-24 15:23:05</td>\n",
       "      <td>653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8726</td>\n",
       "      <td>2014-04-17 14:25:58</td>\n",
       "      <td>8725.0</td>\n",
       "      <td>2014-04-17 14:25:59</td>\n",
       "      <td>665.0</td>\n",
       "      <td>2014-04-17 14:25:59</td>\n",
       "      <td>8727.0</td>\n",
       "      <td>2014-04-17 14:25:59</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2014-04-17 14:25:59</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-04-17 14:26:01</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2014-04-17 14:26:01</td>\n",
       "      <td>5320.0</td>\n",
       "      <td>2014-04-17 14:26:18</td>\n",
       "      <td>5320.0</td>\n",
       "      <td>2014-04-17 14:26:47</td>\n",
       "      <td>5320.0</td>\n",
       "      <td>2014-04-17 14:26:48</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>303</td>\n",
       "      <td>2014-03-21 10:12:24</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2014-03-21 10:12:36</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2014-03-21 10:12:54</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2014-03-21 10:13:01</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2014-03-21 10:13:24</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-03-21 10:13:36</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2014-03-21 10:13:54</td>\n",
       "      <td>309.0</td>\n",
       "      <td>2014-03-21 10:14:01</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2014-03-21 10:14:06</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2014-03-21 10:14:24</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1359</td>\n",
       "      <td>2013-12-13 09:52:28</td>\n",
       "      <td>925.0</td>\n",
       "      <td>2013-12-13 09:54:34</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>2013-12-13 09:54:34</td>\n",
       "      <td>1360.0</td>\n",
       "      <td>2013-12-13 09:54:34</td>\n",
       "      <td>1344.0</td>\n",
       "      <td>2013-12-13 09:54:34</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-12-13 09:54:34</td>\n",
       "      <td>1346.0</td>\n",
       "      <td>2013-12-13 09:54:34</td>\n",
       "      <td>1345.0</td>\n",
       "      <td>2013-12-13 09:54:34</td>\n",
       "      <td>1344.0</td>\n",
       "      <td>2013-12-13 09:58:19</td>\n",
       "      <td>1345.0</td>\n",
       "      <td>2013-12-13 09:58:19</td>\n",
       "      <td>601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>2013-11-26 12:35:29</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2013-11-26 12:35:31</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2013-11-26 12:35:31</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2013-11-26 12:35:32</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2013-11-26 12:35:32</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-11-26 12:35:32</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2013-11-26 12:37:03</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2013-11-26 12:37:03</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2013-11-26 12:37:03</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2013-11-26 12:37:04</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1                time1    site2                time2    site3  \\\n",
       "session_id                                                                      \n",
       "1           23713  2014-03-24 15:22:40  23720.0  2014-03-24 15:22:48  23713.0   \n",
       "2            8726  2014-04-17 14:25:58   8725.0  2014-04-17 14:25:59    665.0   \n",
       "3             303  2014-03-21 10:12:24     19.0  2014-03-21 10:12:36    303.0   \n",
       "4            1359  2013-12-13 09:52:28    925.0  2013-12-13 09:54:34   1240.0   \n",
       "5              11  2013-11-26 12:35:29     85.0  2013-11-26 12:35:31     52.0   \n",
       "\n",
       "                          time3    site4                time4    site5  \\\n",
       "session_id                                                               \n",
       "1           2014-03-24 15:22:48  23713.0  2014-03-24 15:22:54  23720.0   \n",
       "2           2014-04-17 14:25:59   8727.0  2014-04-17 14:25:59     45.0   \n",
       "3           2014-03-21 10:12:54    303.0  2014-03-21 10:13:01    303.0   \n",
       "4           2013-12-13 09:54:34   1360.0  2013-12-13 09:54:34   1344.0   \n",
       "5           2013-11-26 12:35:31     85.0  2013-11-26 12:35:32     11.0   \n",
       "\n",
       "                          time5   ...                  time6    site7  \\\n",
       "session_id                        ...                                   \n",
       "1           2014-03-24 15:22:54   ...    2014-03-24 15:22:55  23713.0   \n",
       "2           2014-04-17 14:25:59   ...    2014-04-17 14:26:01     45.0   \n",
       "3           2014-03-21 10:13:24   ...    2014-03-21 10:13:36    303.0   \n",
       "4           2013-12-13 09:54:34   ...    2013-12-13 09:54:34   1346.0   \n",
       "5           2013-11-26 12:35:32   ...    2013-11-26 12:35:32     11.0   \n",
       "\n",
       "                          time7    site8                time8    site9  \\\n",
       "session_id                                                               \n",
       "1           2014-03-24 15:23:01  23713.0  2014-03-24 15:23:03  23713.0   \n",
       "2           2014-04-17 14:26:01   5320.0  2014-04-17 14:26:18   5320.0   \n",
       "3           2014-03-21 10:13:54    309.0  2014-03-21 10:14:01    303.0   \n",
       "4           2013-12-13 09:54:34   1345.0  2013-12-13 09:54:34   1344.0   \n",
       "5           2013-11-26 12:37:03     85.0  2013-11-26 12:37:03     10.0   \n",
       "\n",
       "                          time9   site10               time10 user_id  \n",
       "session_id                                                             \n",
       "1           2014-03-24 15:23:04  23713.0  2014-03-24 15:23:05     653  \n",
       "2           2014-04-17 14:26:47   5320.0  2014-04-17 14:26:48     198  \n",
       "3           2014-03-21 10:14:06    303.0  2014-03-21 10:14:24      34  \n",
       "4           2013-12-13 09:58:19   1345.0  2013-12-13 09:58:19     601  \n",
       "5           2013-11-26 12:37:03     85.0  2013-11-26 12:37:04     273  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_400.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are 182793 sessions in train data, 46473 in test data and 400 unique users.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((182793, 21), (46473, 20), 400)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_400.shape, test_df_400.shape, train_df_400['user_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vowpal Wabbit requires class labels to be encoded from 1 to K, where K is total number of classes in classification task (in our case that is 400). So we should apply *LabelEncoder* and add +1 to its result. (*LabelEncoder* translates all labels in 0 to K-1 range). We will also have to perform inverse transformation later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = '''YOUR CODE HERE'''\n",
    "class_encoder = '''YOUR CODE HERE'''\n",
    "y_for_vw = '''YOUR CODE HERE'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will compare VW wih SGDClassifier and logistic regression. All these models require processed data. Prepare sparse matrices for sklearn models (just like we did in previous part):\n",
    "- concatenate train and test data\n",
    "- choose only websites (features 'site1' through 'site10')\n",
    "- impute missing values with 0 (we started enumerating sites from 0)\n",
    "- transform data to *csr_matrix* \n",
    "- split back to *train* and *test*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = ['site' + str(i) for i in range(1, 11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''YOUR CODE HERE'''\n",
    "X_train_sparse = '''YOUR CODE HERE'''\n",
    "X_test_sparse = '''YOUR CODE HERE'''\n",
    "y = '''YOUR CODE HERE'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Holdout validation\n",
    "\n",
    "Split data into training (70%) and validation (30%) subsets. We do not shuffle data and take into account that sessions are sorted by time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_share = int(.7 * train_df_400.shape[0])\n",
    "train_df_part = train_df_400[sites].iloc[:train_share, :]\n",
    "valid_df = train_df_400[sites].iloc[train_share:, :]\n",
    "X_train_part_sparse = X_train_sparse[:train_share, :]\n",
    "X_valid_sparse = X_train_sparse[train_share:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_part = y[:train_share]\n",
    "y_valid = y[train_share:]\n",
    "y_train_part_for_vw = y_for_vw[:train_share]\n",
    "y_valid_for_vw = y_for_vw[train_share:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement function **arrays_to_vw** which transforms data to Vowpal Wabbit format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input: \n",
    "- X - numpy matrix (training data)\n",
    "- y (optional) - target numpy vector. It is optional since we will apply the same function to test data.\n",
    "- train (flag) - True, if we are passing training data as X, False otherwise\n",
    "- out_file - path to .vw file, in which we'll write results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details:\n",
    "- you should iterate over every row of X and write to file all the data using whitespace separator. Also you should add target value at the start of each row, separating it with | from the features.\n",
    "- when applying function to test data, you can wirte any target value (1 for example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrays_to_vw(X, y=None, train=True, out_file='tmp.vw'):\n",
    "    '''YOUR CODE HERE'''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply function to subset of training data (train_df_part, y_train_part_for_vw), to holdout set (valid_df, y_valid_for_wv), to whole training data and to whole test data. **Notice, that our method takes only numpy arrays as inputs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# should be 4 calls\n",
    "arrays_to_vw '''YOUR CODE HERE'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Won't work on Windows\n",
    "!head -3 $PATH_TO_DATA/train_part.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Won't work on Windows\n",
    "!head -3  $PATH_TO_DATA/valid.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Won't work on Windows\n",
    "!head -3 $PATH_TO_DATA/test.vw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Vowpal Wabbit on **train_part.wv**. Specify classification task with 400 classes **(--oaa)**, make 3 passes over dataset **(--passes)**. You can also specify cache file (**--cache_file** or flag **-c**) so VW would perform all passes following first one faster (you can delete previous cache file with argument **-k**). Also specify parameter **b=26**. That is number of bits to use for hashing, in this case we need more than deafult 18 bits. Finally, specifiy **random_seed=17**. Do not change other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_part_vw = os.path.join(PATH_TO_DATA, 'train_part.vw')\n",
    "valid_vw = os.path.join(PATH_TO_DATA, 'valid.vw')\n",
    "train_vw = os.path.join(PATH_TO_DATA, 'train.vw')\n",
    "test_vw = os.path.join(PATH_TO_DATA, 'test.vw')\n",
    "model = os.path.join(PATH_TO_DATA, 'vw_model.vw')\n",
    "pred = os.path.join(PATH_TO_DATA, 'vw_pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!vw '''YOUR CODE HERE'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write predictions for **valid.vw** to **vw_valid_pred.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!vw '''YOUR CODE HERE'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read predictions *kaggle_data/vw_valid_pred.csv* from file and see fraction of correct answers on holdout set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''YOUR CODE HERE'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train *SGDClassifier* (3 passes, logistic loss function) and *LogisticRegression* on 70% of sparse train dataset (X_train_part_sparse, y_train_part), make prediction for holdout set (X_valid_sparse, y_valid) and calculate accuracy. Logistic regression will take some time to fit (for me it took around 8 minutes) - this is okay, set multinomial multi_class to make it train much faster. Specify *random_state=17*, *n_jobs=-1* everywhere. For *SGDClassifier* also specify *max_iter=3*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = '''YOUR CODE HERE'''\n",
    "sgd_logit = '''YOUR CODE HERE'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "logit.fit '''YOUR CODE HERE'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sgd_logit.fit '''YOUR CODE HERE'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Calculate accuracy on the holdout set for Vowpal Wabbit, round to 3 decimal places**\n",
    "- **Calculate accuracy on the holdout set for SGD, round to 3 decimal places**\n",
    "- **Calculate accuracy on the holdout set for logistic regression, round to 3 decimal places**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vw_valid_acc = '''YOUR CODE HERE'''\n",
    "sgd_valid_acc = '''YOUR CODE HERE'''\n",
    "logit_valid_acc = '''YOUR CODE HERE'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Test set validation (public leaderboard)\n",
    "\n",
    "Train a VW model with same parameters on the whole training data - **train.wv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!vw '''YOUR CODE HERE'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!vw '''YOUR CODE HERE'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write predictions to file, perform reverse label transformation (we got our labels via adding +1 to output of *LabelEncoder* instance) and send submission to Kaggle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='user_id', index_label=\"session_id\"):\n",
    "    # turn predictions into data frame and save as csv file\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vw_pred = '''YOUR CODE HERE'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_submission_file(vw_pred, os.path.join(PATH_TO_DATA, 'vw_400_users.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same for SGD and logistic regression. I know, it is pretty annoying to wait for logistic regression to fit on this data, but let's be patient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''YOUR CODE HERE'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_submission_file(sgd_logit_test_pred, \n",
    "                         os.path.join(PATH_TO_DATA, 'logit_400_users.csv'))\n",
    "write_to_submission_file(sgd_logit_test_pred, \n",
    "                         os.path.join(PATH_TO_DATA, 'sgd_400_users.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at Public Leaderboard scores in [this](https://www.kaggle.com/c/identify-me-if-you-can4) competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What is the Public Leaderboard score for Vowpal Wabbit?**\n",
    "- **What is the Public Leaderboard score for SGD?**\n",
    "- **What is the Public Leaderboard score for logistic regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion:\n",
    "- think how do Vowpal Wabbit, SGD and logistic regression compare in terms of training speed/classification quality\n",
    "- 400 user classification task probably can't be solved good enough if we use \"honest\" time based split for testing. Next we will compete in identification of only one user (Alice) - [here](https://inclass.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2) is the competition you are advised to participate in.\n",
    "\n",
    "Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
