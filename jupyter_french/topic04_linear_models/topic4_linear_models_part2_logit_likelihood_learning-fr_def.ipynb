{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "<center>\n",
    "<img src=\"https://github.com/oussou-dev/mlcourse.ai/blob/jupyter_french/img/ods_stickers.jpg?raw=true\">\n",
    "    \n",
    "<br>    \n",
    "    \n",
    "    \n",
    "<div style=\"font-weight: 700; font-size: 25px;\"> [mlcourse.ai](https://mlcourse.ai) - Open Machine Learning Course  </div>   \n",
    "\n",
    "\n",
    "<br>\n",
    "Auteur: [Yury Kashnitsky](https://yorko.github.io). Traduit et édité par [Christina Butsko](https://www.linkedin.com/in/christinabutsko/), [Nerses Bagiyan](https://www.linkedin.com/in/nersesbagiyan/), [Yulia Klimushina](https://www.linkedin.com/in/yuliya-klimushina-7168a9139), [Yuanyuan Pao](https://www.linkedin.com/in/yuanyuanpao/) et [Ousmane Cissé](https://github.com/oussou-dev). Ce matériel est soumis aux termes et conditions de la licence [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). L'utilisation gratuite est autorisée à des fins non commerciales.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "# <center>Topic 4. Classification et régression linéaires\n",
    "## <center> Partie 2. Classification linéaire</center></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "### Classifieur linéaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "L'idée de base d'un classifieur linéaire est que deux classes cibles peuvent être séparées par un hyperplan dans l'espace des fonctions. Si cela peut être fait sans erreur, le jeu d'apprentissage est appelé *séparable linéairement* (linearly separable).\n",
    "\n",
    "<img src=\"https://github.com/oussou-dev/mlcourse.ai/blob/jupyter_french/img/logit.png?raw=true\">\n",
    "\n",
    "Nous avons déjà vu la régression linéaire et les moindres carrés ordinaires (MCO). Considérons un problème de classification binaire et désignons les classes cibles par \"+1\" (exemples positifs) et \"-1\" (exemples négatifs). L'un des classifieurs linéaires les plus simples peut être défini à l'aide de la régression suivante:\n",
    "\n",
    "$$\\Large a(\\textbf{x}) = \\text{sign}(\\textbf{w}^\\text{T}\\textbf x),$$\n",
    "\n",
    "où\n",
    " - $\\textbf{x}$ - est un vecteur de caractéristiques (avec identité);\n",
    " - $\\textbf{w}$ - est un vecteur de poids dans le modèle linéaire (avec biais $w_0$);\n",
    " - $\\text{sign}(\\bullet)$ - est la fonction signum (ou fonction signe) qui renvoie le signe de son argument;\n",
    " - $a(\\textbf{x})$ - est une réponse du classifieur pour $\\textbf{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "### Régression logistique en tant que classifieur linéaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "La régression logistique est un cas particulier du classifieur linéaire, mais présente l'avantage supplémentaire de prédire une probabilité $p_+$ de renvoyer l'exemple $\\textbf{x}_\\text{i}$ à la classe \"+\":\n",
    "$$\\Large p_+ = P\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) $$\n",
    "\n",
    "Etre capable de prédire non seulement une réponse (\"+1\" ou \"-1\"), mais la *probabilité* d'attribution à la classe \"+1\" est une exigence très importante dans de nombreux problèmes métier, par exemple. scoring de crédit où la régression logistique est traditionnellement utilisée. Les clients qui ont demandé un prêt sont classés en fonction de cette probabilité prédite (par ordre décroissant) pour obtenir un tableau de bord évaluant les clients de mauvais à bons. Vous trouverez ci-dessous un exemple d’un tel tableau de bord.\n",
    "\n",
    "<img src=\"https://github.com/oussou-dev/mlcourse.ai/blob/jupyter_french/img/toy_scorecard_eng.png?raw=true\" width=\"60%\">\n",
    "\n",
    "La banque choisit un seuil $p_*$ pour prédire la probabilité de défaut de prêt (dans l’image, il s'agit de $0.15$) et arrête d’approuver les prêts à partir de cette valeur. En outre, il est possible de multiplier cette probabilité prédite par le montant du prêt pour obtenir la prévision de perte des clients, ce qui peut également constituer une bonne mesure de gestion (les experts en scoring peuvent en avoir plus à ajouter, mais l'essentiel est le suivant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "Pour prédire la probabilité $p_+ \\in [0,1]$, nous pouvons commencer par construire une prédiction linéaire en utilisant OLS (la méthode des moindres carrés linéaires ou régression linéaire) : $b(\\textbf{x}) = \\textbf{w}^\\text{T} \\textbf{x} \\in \\mathbb{R}$. Mais convertir la valeur obtenue en probabilité dans la plage [0, 1] nécessite une fonction $f: \\mathbb{R} \\rightarrow [0,1]$. La régression logistique utilise une fonction spécifique pour cela: $\\sigma(z) = \\frac{1}{1 + \\exp^{-z}}$. Maintenant, comprenons ce que sont les conditions préalables.\n",
    "\n",
    "<img src=\"https://github.com/oussou-dev/mlcourse.ai/blob/jupyter_french/img/sigmoid.png?raw=true\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "Notons la probabilité d'un événement $X$ comme $P(X)$. Ensuite, le rapport de cotes $OR(X)$ est déterminé par $\\frac{P(X)}{1-P(X)}$, qui est le rapport entre les probabilités qu'un événement se produise ou non. Il est évident que la probabilité et le rapport de cotes contiennent les mêmes informations, mais si $P(X)$ est compris entre 0 et 1, $OR(X)$ est compris entre 0 et $\\infty$.\n",
    "\n",
    "Si nous calculons le logarithme de $OR(X)$ (logarithme de probabilité ou log de probabilité), il est facile de remarquer que $\\log{OR(X)} \\in \\mathbb{R}$. C'est ce que nous allons utiliser avec OLS.\n",
    "\n",
    "Voyons comment la régression logistique fera une prédiction $p_+ = P\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right)$. (Pour le moment, supposons que nous ayons obtenu des poids $\\textbf{w}$, c’est-à-dire que nous avons formé le modèle. Nous verrons plus tard comment le faire.)\n",
    "\n",
    "**Étape 1.** Calculez $w_{0}+w_{1}x_1 + w_{2}x_2 + ... = \\textbf{w}^\\text{T}\\textbf{x}$. (L'équation $\\textbf{w}^\\text{T}\\textbf{x} = 0$ définit un hyperplan séparant les exemples en deux classes);\n",
    "\n",
    "**Étape 2.** Calculez le rapport de cotes du journal: $ \\log(OR_{+}) = \\textbf{w}^\\text{T}\\textbf{x}$.\n",
    "\n",
    "**Étape 3.** Maintenant que nous avons la possibilité d'assigner un exemple à la classe de \"+\" - $OR_{+}$, calculez $p_{+}$ en utilisant la relation simple:\n",
    "\n",
    "$$\\large p_{+} = \\frac{OR_{+}}{1 + OR_{+}} = \\frac{\\exp^{\\textbf{w}^\\text{T}\\textbf{x}}}{1 + \\exp^{\\textbf{w}^\\text{T}\\textbf{x}}} = \\frac{1}{1 + \\exp^{-\\textbf{w}^\\text{T}\\textbf{x}}} = \\sigma(\\textbf{w}^\\text{T}\\textbf{x})$$\n",
    "\n",
    "Sur le côté droit, vous pouvez voir que nous avons la fonction sigmoïde.\n",
    "\n",
    "Ainsi, la régression logistique prédit la probabilité d'assigner un exemple à la classe \"+\" (en supposant que nous connaissions les caractéristiques et les poids du modèle) en tant que transformation sigmoïde d'une combinaison linéaire du vecteur de pondération et du vecteur de caractéristiques:\n",
    "\n",
    "$$\\large p_+(\\textbf{x}_\\text{i}) = P\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\sigma(\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}). $$\n",
    "\n",
    "Ensuite, nous verrons comment le modèle est formé. Nous nous appuierons à nouveau sur l'estimation du maximum de vraisemblance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "### Estimation du maximum de vraisemblance (MLE en anglais) et régression logistique\n",
    "\n",
    "Voyons maintenant comment un problème d’optimisation pour la régression logistique est obtenu à partir de la MLE, à savoir la minimisation de la fonction de perte *logistique*. Nous venons de voir que la régression logistique modélise la probabilité d'assigner un exemple à la classe \"+\" comme suit:\n",
    "\n",
    "\n",
    "$$\\Large p_+(\\textbf{x}_\\text{i}) = P\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\sigma(\\textbf{w}^T\\textbf{x}_\\text{i})$$\n",
    "\n",
    "Alors, pour la classe \"-\", l'expression correspondante est la suivante:\n",
    "$$\\Large p_-(\\textbf{x}_\\text{i})  = P\\left(y_i = -1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right)  = 1 - \\sigma(\\textbf{w}^T\\textbf{x}_\\text{i}) = \\sigma(-\\textbf{w}^T\\textbf{x}_\\text{i}) $$\n",
    "\n",
    "Ces deux expressions peuvent être intelligemment combinées en une seule (observez attentivement, vous êtes peut-être trompé):\n",
    "\n",
    "$$\\Large P\\left(y = y_i \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\sigma(y_i\\textbf{w}^T\\textbf{x}_\\text{i})$$\n",
    "\n",
    "L'expression $M(\\textbf{x}_\\text{i}) = y_i\\textbf{w}^T\\textbf{x}_\\text{i}$ est appelée marge de classification sur l'objet $\\textbf{x}_\\text{i}$ (à ne pas confondre avec un espace, également appelé marge, dans le contexte de la SVM). S'il est non négatif, le modèle choisit correctement la classe de l'objet $\\textbf{x}_\\text{i}$; s'il est négatif, l'objet $\\textbf{x}_\\text{i}$ est mal classé. Notez que la marge est définie pour les objets de l'ensemble d'apprentissage uniquement lorsque les étiquettes de classe cible réelles $y_i$ sont connues.\n",
    "\n",
    "Pour comprendre exactement pourquoi nous sommes arrivés à une telle conclusion, passons à l'interprétation géométrique du classifieur linéaire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "Premièrement, je recommanderais de regarder un problème classique et introductif en algèbre linéaire: trouver la distance entre le point avec un rayon vecteur-$\\textbf{x}_A$ et un plan défini par l'équation $\\textbf{w}^\\text{T}\\textbf{x} = 0.$\n",
    "\n",
    "Répondre:\n",
    "$$\\rho(\\textbf{x}_A, \\textbf{w}^\\text{T}\\textbf{x} = 0) = \\frac{\\textbf{w}^\\text{T}\\textbf{x}_A}{||\\textbf{w}||}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/oussou-dev/mlcourse.ai/blob/jupyter_french/img/simple_linal_task.png?raw=true\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "Lorsque nous aurons trouvé la réponse, nous comprendrons que plus la valeur absolue de l'expression $\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}$ est élevée, plus le point $\\textbf{x}_\\text{i}$ est éloigné du plan $\\textbf{w}^\\text{T}\\textbf{x} = 0.$.\n",
    "\n",
    "Par conséquent, notre expression $M(\\textbf{x}_\\text{i}) = y_i\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}$ est une sorte de \"confiance\" dans la classification par notre modèle de l'objet $\\textbf{x}_\\text{i}$:\n",
    "\n",
    "- si la marge est grande (en valeur absolue) et positive, l’étiquette de classe est correctement définie et l’objet est loin de l’hyperplan séparateur, c’est-à-dire qu'il est classé en toute confiance. Voir Point $x_3$ sur la photo;\n",
    "- si la marge est grande (en valeur absolue) et négative, l'étiquette de classe est définie de manière incorrecte et l'objet est loin de l'hyperplan de séparation (l'objet est très probablement une anomalie; par exemple, il pourrait ne pas être étiqueté correctement dans le training set). Voir Point $x_1$ sur la photo;\n",
    "- si la marge est petite (en valeur absolue), l'objet est proche de l'hyperplan de séparation et le signe de la marge détermine si l'objet est correctement classé. Voir les points $x_2$ et $x_4$ sur le graphique;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/oussou-dev/mlcourse.ai/blob/jupyter_french/img/margin.png?raw=true\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "Calculons maintenant la probabilité de l’ensemble de données, c’est-à-dire la probabilité d’observer le vecteur donné $\\textbf{y}$ à partir de l’ensemble de données $X$. Nous allons faire une hypothèse forte: les objets viennent indépendamment d'une distribution (*i.i.d.*). Alors, nous pouvons écrire\n",
    "\n",
    "$$\\Large P\\left(\\textbf{y} \\mid \\textbf{X}, \\textbf{w}\\right) = \\prod_{i=1}^{\\ell} P\\left(y = y_i \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right),$$\n",
    "\n",
    "où $\\ell$ est la longueur de l'ensemble de données $\\textbf{X}$ (nombre de lignes).\n",
    "\n",
    "Comme d'habitude, prenons le logarithme de cette expression car une somme est beaucoup plus facile à optimiser que le produit:\n",
    "\n",
    "$$\\Large \\log P\\left(\\textbf{y} \\mid \\textbf{X}, \\textbf{w}\\right) = \\log \\prod_{i=1}^{\\ell} P\\left(y = y_i \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\log \\prod_{i=1}^{\\ell} \\sigma(y_i\\textbf{w}^\\text{T}\\textbf{x}_\\text{i})   = $$\n",
    "\n",
    "$$\\Large  = \\sum_{i=1}^{\\ell} \\log \\sigma(y_i\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}) = \\sum_{i=1}^{\\ell} \\log \\frac{1}{1 + \\exp^{-y_i\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}}} = - \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "Maximiser la vraisemblance équivaut à minimiser l'expression:\n",
    "\n",
    "$$\\Large \\mathcal{L_{\\log}} (\\textbf X, \\textbf{y}, \\textbf{w}) = \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}}).$$\n",
    "\n",
    "Il s'agit de la fonction de perte *logistique* qui est ajoutée à tous les objets du jeu d'apprentissage.\n",
    "\n",
    "Examinons la nouvelle fonction en tant que fonction de la marge $L(M) = \\log (1 + \\exp^{-M})$ et tracez-la avec le graphique *zero-one loss*, qui pénalise simplement le modèle en cas d'erreur sur chaque objet de 1 (marge négative): $L_{1/0}(M) = [M &lt; 0]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/oussou-dev/mlcourse.ai/blob/jupyter_french/img/logloss_margin_eng.png?raw=true\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "L'image reflète l'idée que, si nous ne sommes pas en mesure de minimiser directement le nombre d'erreurs dans le problème de classification (du moins pas par les méthodes de gradient - dérivée de la fonction de perte zéro-un à zéro virage à l'infini), nous pouvons minimiser sa limite supérieure. Pour la fonction de perte logistique (où le logarithme est binaire, mais cela n'a pas d'importance), ce qui suit est valide:\n",
    "\n",
    "$$\\Large \\mathcal{L_{1/0}} (\\textbf X, \\textbf{y}, \\textbf{w}) = \\sum_{i=1}^{\\ell} [M(\\textbf{x}_\\text{i}) &lt; 0] \\leq \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}}) = \\mathcal{L_{\\log}} (\\textbf X, \\textbf{y}, \\textbf{w}), $$\n",
    "\n",
    "où $\\mathcal{L_{1/0}} (\\textbf X, \\textbf{y})$ est simplement le nombre d'erreurs de régression logistique avec les pondérations $\\textbf{w}$ sur un ensemble de données $(\\textbf X, \\textbf{y})$.\n",
    "\n",
    "Ainsi, en réduisant la limite supérieure de $\\mathcal{L_{log}}$ par le nombre d'erreurs de classification, nous espérons réduire le nombre d'erreurs lui-même.\n",
    "\n",
    "### $L_2$-Régularisation de la perte logistique\n",
    "$L_2$-régularisation de la régression logistique est presque la même que dans le cas de la régression de ridge. Au lieu de minimiser la fonction $\\mathcal{L_{\\log}} (\\textbf X, \\textbf{y}, \\textbf{w})$, nous minimisons les éléments suivants:\n",
    "\n",
    "$$\\Large \\mathcal{J}(\\textbf X, \\textbf{y}, \\textbf{w}) = \\mathcal{L_{\\log}} (\\textbf X, \\textbf{y}, \\textbf{w}) + \\lambda |\\textbf{w}|^2$$\n",
    "\n",
    "Dans le cas de la régression logistique, un coefficient de régularisation inverse $C = \\frac{1}{\\lambda}$ est généralement introduit. Alors la solution au problème serait:\n",
    "\n",
    "$$\\Large \\widehat{\\textbf w}  = \\arg \\min_{\\textbf{w}} \\mathcal{J}(\\textbf X, \\textbf{y}, \\textbf{w}) =  \\arg \\min_{\\textbf{w}}\\ (C\\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}})+ |\\textbf{w}|^2)$$\n",
    "\n",
    "Ensuite, nous examinerons un exemple qui nous permet de comprendre intuitivement l’une des interprétations de la régularisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "### Ressources utiles\n",
    "- Main course [site](https://mlcourse.ai), [course repo](https://github.com/Yorko/mlcourse.ai), and YouTube [channel](https://www.youtube.com/watch?v=QKTuw4PNOsU&amp;list=PLVlY_7IJCMJeRfZ68eVfEcu-UcN9BbwiX)\n",
    "- Medium [\"story\"](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-4-linear-classification-and-regression-44a41b9b5220) based on this notebook\n",
    "- Course materials as a [Kaggle Dataset](https://www.kaggle.com/kashnitsky/mlcourse)\n",
    "- If you read Russian: an [article](https://habrahabr.ru/company/ods/blog/323890/) on Habrahabr with ~ the same material. And a [lecture](https://youtu.be/oTXGQ-_oqvI) on YouTube\n",
    "- A nice and concise overview of linear models is given in the book [“Deep Learning”](http://www.deeplearningbook.org) (I. Goodfellow, Y. Bengio, and A. Courville).\n",
    "- Linear models are covered practically in every ML book. We recommend “Pattern Recognition and Machine Learning” (C. Bishop) and “Machine Learning: A Probabilistic Perspective” (K. Murphy).\n",
    "- If you prefer a thorough overview of linear model from a statistician’s viewpoint, then look at “The elements of statistical learning” (T. Hastie, R. Tibshirani, and J. Friedman).\n",
    "- The book “Machine Learning in Action” (P. Harrington) will walk you through implementations of classic ML algorithms in pure Python.\n",
    "- [Scikit-learn](http://scikit-learn.org/stable/documentation.html) library. These guys work hard on writing really clear documentation.\n",
    "- Scipy 2017 [scikit-learn tutorial](https://github.com/amueller/scipy-2017-sklearn) by Alex Gramfort and Andreas Mueller.\n",
    "- One more [ML course](https://github.com/diefimov/MTH594_MachineLearning) with very good materials.\n",
    "- [Implementations](https://github.com/rushter/MLAlgorithms) of many ML algorithms. Search for linear regression and logistic regression.<i class=\"fa fa-lightbulb-o \"></i>"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
