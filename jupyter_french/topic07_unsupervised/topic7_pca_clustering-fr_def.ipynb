{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/Yorko/mlcourse.ai/master/img/ods_stickers.jpg\">\n",
    "## Open Machine Learning Course\n",
    "<center>\n",
    "Auteur: [Sergey Korolev](https://www.linkedin.com/in/sokorolev/), Ing√©nieur Logiciel chez Snap Inc. <br>\n",
    "Traduit et √©dit√© par [Egor Polusmak](https://www.linkedin.com/in/egor-polusmak/), [Anastasia Manokhina](https://www.linkedin.com/in/anastasiamanokhina/), [Anna Golovchenko](https://www.linkedin.com/in/anna-golovchenko-b0ba5a112/), [Eugene Mashkin](https://www.linkedin.com/in/eugene-mashkin-88490883/), [Yuanyuan Pao](https://www.linkedin.com/in/yuanyuanpao/) et [Ousmane Ciss√©](https://www.linkedin.com/in/ousmane-ciss√©/).\n",
    "\n",
    "Ce mat√©riel est soumis aux termes et conditions de la licence [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). La libre utilisation est autoris√©e √† toute fin non commerciale avec indication obligatoire des noms des auteurs et de la source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "# <center>Topic 7. Apprentissage non supervis√©: PCA et clustering\n",
    "\n",
    "Bienvenue dans la septi√®me partie de notre cours Open Machine Learning!\n",
    "¬†¬†\n",
    "Dans cette le√ßon, nous travaillerons avec des m√©thodes d'apprentissage non supervis√©es telles que l'analyse en composantes principales (ACP) et le clustering. Vous apprendrez pourquoi et comment nous pouvons r√©duire la dimensionnalit√© des donn√©es d'origine et quelles sont les principales approches pour regrouper des points de donn√©es similaires."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "### Plan de l'article\n",
    "1. Introduction\n",
    "2. PCA\n",
    "¬†- Intuition, th√©ories et probl√®mes d'application\n",
    "¬†- Exemples d'application\n",
    "3. Analyse des clusters\n",
    "¬†- K-Means\n",
    "¬†- Propagation d'affinit√©\n",
    "¬†- Regroupement spectral\n",
    "¬†- Cluster agglom√©ratif\n",
    "¬†- Mesures de pr√©cision\n",
    "4. Liens utiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "## 1. Introduction\n",
    "\n",
    "La principale caract√©ristique des algorithmes d'apprentissage non supervis√©s, par rapport aux m√©thodes de classification et de r√©gression, est que les donn√©es d'entr√©e ne sont pas √©tiquet√©es (c'est-√†-dire qu'aucune √©tiquette ou classe n'est donn√©e) et que l'algorithme apprend la structure des donn√©es sans aucune assistance. Cela cr√©e deux diff√©rences principales. Tout d'abord, cela nous permet de traiter de grandes quantit√©s de donn√©es car les donn√©es n'ont pas besoin d'√™tre √©tiquet√©es manuellement. Deuxi√®mement, il est difficile d'√©valuer la qualit√© d'un algorithme non supervis√© en raison de l'absence d'une m√©trique de \"justesse\" explicite telle qu'elle est utilis√©e dans l'apprentissage supervis√©.\n",
    "\n",
    "L'une des t√¢ches les plus courantes dans l'apprentissage non supervis√© est la r√©duction de la dimensionnalit√©. D'une part, la r√©duction de la dimensionnalit√© peut aider √† la visualisation des donn√©es (par exemple la m√©thode **t-SNE**) tandis que, d'autre part, elle peut aider √† g√©rer la multicolin√©arit√© de vos donn√©es et √† pr√©parer les donn√©es pour une m√©thode d'apprentissage supervis√© (par exemple, les **arbres de d√©cision**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "## 2. Analyse en composantes principales (ACP)\n",
    "\n",
    "### Intuition, th√©ories et probl√®mes d'application\n",
    "\n",
    "L'analyse en composantes principales est l'une des m√©thodes les plus simples, les plus intuitives et les plus utilis√©es pour r√©duire la dimensionnalit√©, projetant des donn√©es sur son sous-espace d'entit√©s orthogonales.\n",
    "\n",
    "\n",
    "<img align=\"right\" src=\"https://habrastorage.org/getpro/habr/post_images/bb6/fe7/f06/bb6fe7f06e114bcc9c354a1cb025b966.png\" width=\"400\">\n",
    "\n",
    "\n",
    "Plus g√©n√©ralement, toutes les observations peuvent √™tre consid√©r√©es comme un ellipso√Øde dans un sous-espace d'un espace caract√©ristique initial, et la nouvelle base d√©finie dans ce sous-espace est align√©e avec les axes ellipso√Ødes. Cette hypoth√®se nous permet de supprimer les caract√©ristiques hautement corr√©l√©es puisque les vecteurs d'ensemble de base sont orthogonaux.\n",
    "Dans le cas g√©n√©ral, la dimensionnalit√© ellipso√Øde r√©sultante correspond √† la dimensionnalit√© spatiale initiale, mais l'hypoth√®se que nos donn√©es se trouvent dans un sous-espace avec une dimension plus petite nous permet de couper l'espace \"excessif\" avec la nouvelle projection (sous-espace). Nous accomplissons cela d'une mani√®re ¬´gourmande¬ª, en s√©lectionnant s√©quentiellement chacun des axes ellipso√Ødes en identifiant o√π la dispersion est maximale.\n",
    "¬†\n",
    "\n",
    "> \"Pour g√©rer les hyper-plans dans un espace en 14 dimensions, visualisez un espace 3D et dites\" quatorze \"tr√®s fort. Tout le monde le fait.\" - Geoffrey Hinton\n",
    "\n",
    "\n",
    "Jetons un coup d'≈ìil √† la formulation math√©matique de ce processus:\n",
    "\n",
    "Afin de diminuer la dimensionnalit√© de nos donn√©es de $n$ √† $k$ avec $k \\leq n$, nous trions notre liste d'axes par ordre de dispersion d√©croissante et prenons le top-$k$ d'entre eux.\n",
    "\n",
    "Nous commen√ßons par calculer la dispersion et la covariance des caract√©ristiques initiales. Cela se fait g√©n√©ralement avec la matrice de covariance. Selon la d√©finition de covariance, la covariance de deux entit√©s est calcul√©e comme suit: $$cov(X_i, X_j) = E[(X_i - \\mu_i) (X_j - \\mu_j)] = E[X_i X_j] - \\mu_i \\mu_j,$$ o√π $\\mu_i$ est la valeur attendue de l'entit√© $i$th. Il convient de noter que la covariance est sym√©trique et que la covariance d'un vecteur avec lui-m√™me est √©gale √† sa dispersion.\n",
    "\n",
    "Par cons√©quent, la matrice de covariance est sym√©trique avec la dispersion des caract√©ristiques correspondantes sur la diagonale. Les valeurs non diagonales sont les covariances de la paire d'entit√©s correspondante. En termes de matrices o√π $\\mathbf{X}$ est la matrice des observations, la matrice de covariance est la suivante:\n",
    "\n",
    "$$\\Sigma = E[(\\mathbf{X} - E[\\mathbf{X}]) (\\mathbf{X} - E[\\mathbf{X}])^{T}]$$\n",
    "\n",
    "Br√®ve r√©capitulation : les matrices, en tant qu'op√©rateurs lin√©aires, ont des valeurs propres et des vecteurs propres. Ils sont tr√®s pratiques car ils d√©crivent des parties de notre espace qui ne tournent pas et ne s'√©tirent que lorsque nous leur appliquons des op√©rateurs lin√©aires; les vecteurs propres restent dans la m√™me direction mais sont √©tir√©s par une valeur propre correspondante. Formellement, une matrice $M$ avec vecteur propre $w_i$ et valeur propre $\\lambda_i$ satisfait cette √©quation: $M w_i = \\lambda_i w_i$.\n",
    "\n",
    "La matrice de covariance d'un √©chantillon $\\mathbf{X}$ peut √™tre √©crite comme un produit de $\\mathbf{X}^{T} \\mathbf{X}$. Selon le [quotient de Rayleigh](https://en.wikipedia.org/wiki/Rayleigh_quotient), la variation maximale de notre √©chantillon se situe le long du vecteur propre de cette matrice et est coh√©rente avec la valeur propre maximale. Par cons√©quent, les principaux composants que nous visons √† retenir des donn√©es ne sont que les vecteurs propres correspondant aux valeurs propres les plus √©lev√©es de la matrice-$k$.\n",
    "\n",
    "Les prochaines √©tapes sont plus faciles √† dig√©rer. Nous multiplions la matrice de nos donn√©es $X$ par ces composants pour obtenir la projection de nos donn√©es sur la base orthogonale des composants choisis. Si le nombre de composants √©tait inf√©rieur √† la dimensionnalit√© d'espace initiale, n'oubliez pas que nous perdrons certaines informations lors de l'application de cette transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "## Exemples\n",
    "### Ensemble de donn√©es sur l'iris de Fisher\n",
    "\n",
    "Commen√ßons par t√©l√©charger tous les modules essentiels et essayons l'exemple iris de la documentation `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"white\")\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import datasets, decomposition\n",
    "\n",
    "# Loading the dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Let's create a beautiful 3d-plot\n",
    "fig = plt.figure(1, figsize=(6, 5))\n",
    "plt.clf()\n",
    "ax = Axes3D(fig, rect=[0, 0, 0.95, 1], elev=48, azim=134)\n",
    "\n",
    "plt.cla()\n",
    "\n",
    "for name, label in [(\"Setosa\", 0), (\"Versicolour\", 1), (\"Virginica\", 2)]:\n",
    "    ax.text3D(\n",
    "        X[y == label, 0].mean(),\n",
    "        X[y == label, 1].mean() + 1.5,\n",
    "        X[y == label, 2].mean(),\n",
    "        name,\n",
    "        horizontalalignment=\"center\",\n",
    "        bbox=dict(alpha=0.5, edgecolor=\"w\", facecolor=\"w\"),\n",
    "    )\n",
    "# Change the order of labels, so that they match\n",
    "y_clr = np.choose(y, [1, 2, 0]).astype(np.float)\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y_clr, cmap=plt.cm.nipy_spectral)\n",
    "\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.w_zaxis.set_ticklabels([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "Voyons maintenant comment PCA am√©liorera les r√©sultats d'un mod√®le simple qui n'est pas capable d'ajuster correctement toutes les donn√©es d'entra√Ænement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Train, test splits\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Decision trees with depth = 2\n",
    "clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict_proba(X_test)\n",
    "print(\"Accuracy: {:.5f}\".format(accuracy_score(y_test, preds.argmax(axis=1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "Essayons √† nouveau, mais, cette fois, r√©duisons la dimensionnalit√© √† 2 dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PCA from sklearn PCA\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "X_centered = X - X.mean(axis=0)\n",
    "pca.fit(X_centered)\n",
    "X_pca = pca.transform(X_centered)\n",
    "\n",
    "# Plotting the results of PCA\n",
    "plt.plot(X_pca[y == 0, 0], X_pca[y == 0, 1], \"bo\", label=\"Setosa\")\n",
    "plt.plot(X_pca[y == 1, 0], X_pca[y == 1, 1], \"go\", label=\"Versicolour\")\n",
    "plt.plot(X_pca[y == 2, 0], X_pca[y == 2, 1], \"ro\", label=\"Virginica\")\n",
    "plt.legend(loc=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test-train split and apply PCA\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_pca, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict_proba(X_test)\n",
    "print(\"Accuracy: {:.5f}\".format(accuracy_score(y_test, preds.argmax(axis=1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "La pr√©cision n'a pas augment√© de mani√®re significative dans ce cas, mais, avec d'autres ensembles de donn√©es avec un nombre √©lev√© de dimensions, l'ACP peut consid√©rablement am√©liorer la pr√©cision des arbres de d√©cision et d'autres m√©thodes d'ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "Voyons maintenant le pourcentage de variance qui peut √™tre expliqu√© par chacun des composants s√©lectionn√©s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, component in enumerate(pca.components_):\n",
    "    print(\n",
    "        \"{} component: {}% of initial variance\".format(\n",
    "            i + 1, round(100 * pca.explained_variance_ratio_[i], 2)\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \" + \".join(\n",
    "            \"%.3f x %s\" % (value, name)\n",
    "            for value, name in zip(component, iris.feature_names)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "### Ensemble de donn√©es de nombres manuscrits\n",
    "\n",
    "Regardons l'ensemble de donn√©es de nombres manuscrits que nous avons utilis√© pr√©c√©demment dans la [3e le√ßon](https://habrahabr.ru/company/ods/blog/322534/#derevya-resheniy-i-metod-blizhayshih-sosedey-v-zadache -raspoznavaniya-rukopisnyh-cifr-mnist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "X = digits.data\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "Commen√ßons par visualiser nos donn√©es. R√©cup√©rez les 10 premiers chiffres. Les nombres sont repr√©sent√©s par des matrices 8 x 8 avec l'intensit√© de couleur pour chaque pixel. Chaque matrice est aplatie en un vecteur de 64 nombres, nous obtenons donc la version caract√©ristique des donn√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f, axes = plt.subplots(5, 2, sharey=True, figsize=(16,6))\n",
    "plt.figure(figsize=(16, 6))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(X[i, :].reshape([8, 8]), cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "Nos donn√©es ont 64 dimensions, mais nous allons les r√©duire √† seulement 2 et voir que, m√™me avec seulement 2 dimensions, nous pouvons clairement voir que les chiffres se s√©parent en grappes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=2)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "print(\"Projecting %d-dimensional data to 2D\" % X.shape[1])\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(\n",
    "    X_reduced[:, 0],\n",
    "    X_reduced[:, 1],\n",
    "    c=y,\n",
    "    edgecolor=\"none\",\n",
    "    alpha=0.7,\n",
    "    s=40,\n",
    "    cmap=plt.cm.get_cmap(\"nipy_spectral\", 10),\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.title(\"MNIST. PCA projection\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "En effet, avec t-SNE, l'image semble meilleure car PCA a une contrainte lin√©aire alors que t-SNE n'en a pas. Cependant, m√™me avec un si petit ensemble de donn√©es, l'algorithme t-SNE prend beaucoup plus de temps √† compl√©ter que PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(random_state=17)\n",
    "\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(\n",
    "    X_tsne[:, 0],\n",
    "    X_tsne[:, 1],\n",
    "    c=y,\n",
    "    edgecolor=\"none\",\n",
    "    alpha=0.7,\n",
    "    s=40,\n",
    "    cmap=plt.cm.get_cmap(\"nipy_spectral\", 10),\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.title(\"MNIST. t-SNE projection\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "Dans la pratique, nous choisirions le nombre de composants principaux de fa√ßon √† pouvoir expliquer 90% de la dispersion initiale des donn√©es (via le `explained_variance_ratio`). Ici, cela signifie conserver 21 composants principaux; par cons√©quent, nous r√©duisons la dimensionnalit√© de 64 entit√©s √† 21."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA().fit(X)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_), color=\"k\", lw=2)\n",
    "plt.xlabel(\"Number of components\")\n",
    "plt.ylabel(\"Total explained variance\")\n",
    "plt.xlim(0, 63)\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.axvline(21, c=\"b\")\n",
    "plt.axhline(0.9, c=\"r\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "## 2. Clustering\n",
    "\n",
    "L'id√©e principale derri√®re le clustering est assez simple. Fondamentalement, nous nous disons: ¬´J'ai ces points ici, et je peux voir qu'ils s'organisent en groupes. Ce serait bien de d√©crire ces choses plus concr√®tement et, quand un nouveau point arrive, de les affecter au bon groupe . \" Cette id√©e g√©n√©rale encourage l'exploration et ouvre une vari√©t√© d'algorithmes pour le clustering.\n",
    "\n",
    "<figure><img align=\"center\" src=\"https://habrastorage.org/getpro/habr/post_images/8b9/ae5/586/8b9ae55861f22a2809e8b3a00ef815ad.png\"><figcaption> *Les exemples des r√©sultats de diff√©rents algorithmes de scikit-learn* </figcaption></figure>\n",
    "\n",
    "Les algorithmes r√©pertori√©s ci-dessous ne couvrent pas toutes les m√©thodes de clustering disponibles, mais ce sont les plus couramment utilis√©es.\n",
    "\n",
    "### K-means\n",
    "\n",
    "L'algorithme K-means est le plus populaire et le plus simple de tous les algorithmes de clustering. Voici comment cela fonctionne:\n",
    "1. S√©lectionnez le nombre de grappes $k$ que vous pensez √™tre le nombre optimal.\n",
    "2. Initialisez les points $k$ comme des \"centro√Ødes\" au hasard dans l'espace de nos donn√©es.\n",
    "3. Attribuez chaque observation √† son centro√Øde le plus proche.\n",
    "4. Mettez √† jour les centro√Ødes au centre de l'ensemble des observations attribu√©es.\n",
    "5. R√©p√©tez les √©tapes 3 et 4 un nombre fixe de fois ou jusqu'√† ce que tous les centro√Ødes soient stables (c'est-√†-dire qu'ils ne changent plus √† l'√©tape 4).\n",
    "\n",
    "Cet algorithme est facile √† d√©crire et √† visualiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's begin by allocation 3 cluster's points\n",
    "X = np.zeros((150, 2))\n",
    "\n",
    "np.random.seed(seed=42)\n",
    "X[:50, 0] = np.random.normal(loc=0.0, scale=0.3, size=50)\n",
    "X[:50, 1] = np.random.normal(loc=0.0, scale=0.3, size=50)\n",
    "\n",
    "X[50:100, 0] = np.random.normal(loc=2.0, scale=0.5, size=50)\n",
    "X[50:100, 1] = np.random.normal(loc=-1.0, scale=0.2, size=50)\n",
    "\n",
    "X[100:150, 0] = np.random.normal(loc=-1.0, scale=0.2, size=50)\n",
    "X[100:150, 1] = np.random.normal(loc=2.0, scale=0.5, size=50)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(X[:, 0], X[:, 1], \"bo\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scipy has function that takes 2 tuples and return\n",
    "# calculated distance between them\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Randomly allocate the 3 centroids\n",
    "np.random.seed(seed=42)\n",
    "centroids = np.random.normal(loc=0.0, scale=1.0, size=6)\n",
    "centroids = centroids.reshape((3, 2))\n",
    "\n",
    "cent_history = []\n",
    "cent_history.append(centroids)\n",
    "\n",
    "for i in range(3):\n",
    "    # Calculating the distance from a point to a centroid\n",
    "    distances = cdist(X, centroids)\n",
    "    # Checking what's the closest centroid for the point\n",
    "    labels = distances.argmin(axis=1)\n",
    "\n",
    "    # Labeling the point according the point's distance\n",
    "    centroids = centroids.copy()\n",
    "    centroids[0, :] = np.mean(X[labels == 0, :], axis=0)\n",
    "    centroids[1, :] = np.mean(X[labels == 1, :], axis=0)\n",
    "    centroids[2, :] = np.mean(X[labels == 2, :], axis=0)\n",
    "\n",
    "    cent_history.append(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot K-means\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i in range(4):\n",
    "    distances = cdist(X, cent_history[i])\n",
    "    labels = distances.argmin(axis=1)\n",
    "\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.plot(X[labels == 0, 0], X[labels == 0, 1], \"bo\", label=\"cluster #1\")\n",
    "    plt.plot(X[labels == 1, 0], X[labels == 1, 1], \"co\", label=\"cluster #2\")\n",
    "    plt.plot(X[labels == 2, 0], X[labels == 2, 1], \"mo\", label=\"cluster #3\")\n",
    "    plt.plot(cent_history[i][:, 0], cent_history[i][:, 1], \"rX\")\n",
    "    plt.legend(loc=0)\n",
    "    plt.title(\"Step {:}\".format(i + 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "Nous avons utilis√© la distance euclidienne, mais l'algorithme convergera avec toute autre m√©trique. Vous pouvez non seulement faire varier le nombre d'√©tapes ou les crit√®res de convergence, mais √©galement la mesure de distance entre les points et les centro√Ødes de cluster.\n",
    "\n",
    "Une autre \"caract√©ristique\" de cet algorithme est sa sensibilit√© aux positions initiales des centro√Ødes du cluster. Vous pouvez ex√©cuter l'algorithme plusieurs fois, puis faire la moyenne de tous les r√©sultats du centro√Øde."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "## Choix du nombre de clusters pour K-means\n",
    "\n",
    "Contrairement aux t√¢ches d'apprentissage supervis√© telles que la classification et la r√©gression, le clustering n√©cessite plus d'efforts pour choisir le crit√®re d'optimisation. Habituellement, lorsque nous travaillons avec k-means, nous optimisons la somme des distances au carr√© entre les observations et leurs centro√Ødes.\n",
    "\n",
    "$$ J(C) = \\sum_{k=1}^K\\sum_{i~\\in~C_k} ||x_i - \\mu_k|| \\rightarrow \\min\\limits_C,$$\n",
    "\n",
    "o√π $C$ - est un ensemble de clusters avec la puissance $K$, $\\mu_k$ est un centre de gravit√© d'un cluster $C_k$.\n",
    "\n",
    "Cette d√©finition semble raisonnable - nous voulons que nos observations soient aussi proches que possible de leurs centro√Ødes. Mais, il y a un probl√®me - l'optimum est atteint lorsque le nombre de centro√Ødes est √©gal au nombre d'observations, donc vous vous retrouveriez avec chaque observation comme son propre cluster s√©par√©.\n",
    "\n",
    "Afin d'√©viter ce cas, nous devons choisir un certain nombre de clusters apr√®s quoi une fonction $J(C_k)$ diminue moins rapidement. Plus formellement,\n",
    "$$ D(k) = \\frac{|J(C_k) - J(C_{k+1})|}{|J(C_{k-1}) - J(C_k)|}  \\rightarrow \\min\\limits_k $$\n",
    "\n",
    "Regardons un exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia = []\n",
    "for k in range(1, 8):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=1).fit(X)\n",
    "    inertia.append(np.sqrt(kmeans.inertia_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, 8), inertia, marker=\"s\")\n",
    "plt.xlabel(\"$k$\")\n",
    "plt.ylabel(\"$J(C_k)$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "Nous voyons que $J(C_k)$ diminue de mani√®re significative jusqu'√† ce que le nombre de clusters soit de 3 puis ne change plus autant. Cela signifie que le nombre optimal de clusters est de 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "#### Probl√®mes\n",
    "\n",
    "En soi, K-means est NP-difficile. Pour les dimensions $d$, les clusters $k$ et les observations $n$, nous trouverons une solution en temps $O(n^{d k+1})$. Il y a quelques heuristiques pour g√©rer cela; un exemple est MiniBatch K-means, qui prend des parties (lots) de donn√©es au lieu d'ajuster l'ensemble de donn√©es, puis d√©place les centro√Ødes en prenant la moyenne des √©tapes pr√©c√©dentes. Comparez l'impl√©mentation de K-means et MiniBatch K-means dans la [documentation sckit-learn](http://scikit-learn.org/stable/auto_examples/cluster/plot_mini_batch_kmeans.html).\n",
    "\n",
    "L'[impl√©mentation](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) de l'algorithme utilisant `scikit-learn` a ses avantages tels que la possibilit√© d'indiquer le nombre d'initialisations avec le param√®tre de fonction `n_init`, qui nous permet d'identifier des centro√Ødes plus robustes. De plus, ces analyses peuvent √™tre effectu√©es en parall√®le pour diminuer le temps de calcul."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "## Propagation d'affinit√©\n",
    "\n",
    "La propagation d'affinit√© est un autre exemple d'algorithme de clustering. Contrairement √† K-means, cette approche ne n√©cessite pas de fixer au pr√©alable le nombre de clusters. L'id√©e principale ici est que nous aimerions regrouper nos donn√©es en fonction de la similitude des observations (ou de la fa√ßon dont elles ¬´correspondent¬ª les unes aux autres).\n",
    "\n",
    "D√©finissons une m√©trique de similitude telle que $s(x_i, x_j) > s(x_i, x_k)$ si une observation $x_i$ est plus similaire √† l'observation $x_j$ et moins similaire √† l'observation $x_k$. Un exemple simple d'une telle m√©trique de similitude est un carr√© n√©gatif de la distance $s(x_i, x_j) = - ||x_i - x_j||^{2}$.\n",
    "\n",
    "\n",
    "D√©crivons maintenant la \"correspondance\" en faisant deux matrices nulles. L'un d'eux, $r_{i,k}$, d√©termine dans quelle mesure l'observation $k$th est en tant que ¬´mod√®le de r√¥le¬ª pour l'observation $i$th par rapport √† tous les autres ¬´mod√®les de r√¥le¬ª possibles. Une autre matrice, $a_{i,k}$, d√©termine dans quelle mesure il serait appropri√© pour $i$th observation de prendre l'observation $k$th comme \"mod√®le\".\n",
    "\n",
    "Les matrices sont mises √† jour s√©quentiellement avec les r√®gles suivantes:\n",
    "\n",
    "$$r_{i,k} \\leftarrow s_(x_i, x_k) - \\max_{k' \\neq k} \\left\\{ a_{i,k'} + s(x_i, x_k') \\right\\}$$\n",
    "\n",
    "$$a_{i,k} \\leftarrow \\min \\left( 0, r_{k,k} + \\sum_{i' \\not\\in \\{i,k\\}} \\max(0, r_{i',k}) \\right), \\ \\ \\  i \\neq k$$\n",
    "\n",
    "$$a_{k,k} \\leftarrow \\sum_{i' \\neq k} \\max(0, r_{i',k})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "## Regroupement spectral\n",
    "\n",
    "Le regroupement spectral combine certaines des approches d√©crites ci-dessus pour cr√©er une m√©thode de regroupement plus solide.\n",
    "\n",
    "Tout d'abord, cet algorithme nous oblige √† d√©finir la matrice de similitude pour les observations appel√©e matrice d'adjacence. Cela peut √™tre fait d'une mani√®re similaire √† celle de l'algorithme de propagation d'affinit√©: $A_{i, j} = - ||x_i - x_j||^{2}$. Cette matrice d√©crit un graphique complet avec les observations sous forme de sommets et la valeur de similitude estim√©e entre une paire d'observations sous forme de poids de bord pour cette paire de sommets. Pour la m√©trique d√©finie ci-dessus et les observations bidimensionnelles, cela est assez intuitif - deux observations sont similaires si le bord entre elles est plus court.\n",
    "Nous aimerions diviser le graphique en deux sous-graphiques de telle sorte que chaque observation de chaque sous-graphique soit similaire √† une autre observation de ce sous-graphique. Formellement, il s'agit d'un probl√®me de coupes normalis√©es; pour plus de d√©tails, nous vous recommandons de lire [ce document](http://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "## Cluster agglom√©ratif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "L'algorithme suivant est le plus simple et le plus facile √† comprendre parmi tous les algorithmes de clustering sans nombre fixe de clusters.\n",
    "\n",
    "\n",
    "L'algorithme est assez simple:\n",
    "1. Nous commen√ßons par assigner chaque observation √† son propre cluster\n",
    "2. Ensuite, triez les distances par paires entre les centres des grappes dans l'ordre d√©croissant\n",
    "3. Prenez les deux grappes voisines les plus proches et fusionnez-les, puis recalculez les centres\n",
    "4. R√©p√©tez les √©tapes 2 et 3 jusqu'√† ce que toutes les donn√©es soient fusionn√©es en un seul cluster\n",
    "\n",
    "Le processus de recherche du cluster le plus proche peut √™tre men√© avec diff√©rentes m√©thodes de d√©limitation des observations:\n",
    "1. Liaison unique\n",
    "$d(C_i, C_j) = min_{x_i \\in C_i, x_j \\in C_j} ||x_i - x_j||$\n",
    "2. Lien complet\n",
    "$d(C_i, C_j) = max_{x_i \\in C_i, x_j \\in C_j} ||x_i - x_j||$\n",
    "3. Lien moyen\n",
    "$d(C_i, C_j) = \\frac{1}{n_i n_j} \\sum_{x_i \\in C_i} \\sum_{x_j \\in C_j} ||x_i - x_j||$\n",
    "4. Liaison centro√Øde\n",
    "$d(C_i, C_j) = ||\\mu_i - \\mu_j||$\n",
    "\n",
    "Le 3√®me est le plus efficace en temps de calcul car il ne n√©cessite pas de recalculer les distances √† chaque fusion des clusters.\n",
    "\n",
    "Les r√©sultats peuvent √™tre visualis√©s comme un bel arbre de cluster (dendogramme) pour aider √† reconna√Ætre le moment o√π l'algorithme doit √™tre arr√™t√© pour obtenir des r√©sultats optimaux. Il existe de nombreux outils Python pour construire ces dendogrammes pour le clustering agglom√©ratif.\n",
    "\n",
    "Prenons un exemple avec les clusters que nous avons obtenus de K-means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "X = np.zeros((150, 2))\n",
    "\n",
    "np.random.seed(seed=42)\n",
    "X[:50, 0] = np.random.normal(loc=0.0, scale=0.3, size=50)\n",
    "X[:50, 1] = np.random.normal(loc=0.0, scale=0.3, size=50)\n",
    "\n",
    "X[50:100, 0] = np.random.normal(loc=2.0, scale=0.5, size=50)\n",
    "X[50:100, 1] = np.random.normal(loc=-1.0, scale=0.2, size=50)\n",
    "\n",
    "X[100:150, 0] = np.random.normal(loc=-1.0, scale=0.2, size=50)\n",
    "X[100:150, 1] = np.random.normal(loc=2.0, scale=0.5, size=50)\n",
    "\n",
    "# pdist will calculate the upper triangle of the pairwise distance matrix\n",
    "distance_mat = pdist(X)\n",
    "# linkage ‚Äî is an implementation of agglomerative algorithm\n",
    "Z = hierarchy.linkage(distance_mat, \"single\")\n",
    "plt.figure(figsize=(10, 5))\n",
    "dn = hierarchy.dendrogram(Z, color_threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "## Mesures de pr√©cision\n",
    "\n",
    "Contrairement √† la classification, il est difficile d'√©valuer la qualit√© des r√©sultats du clustering. Ici, une m√©trique ne peut pas d√©pendre des √©tiquettes mais uniquement de la qualit√© de la division. Deuxi√®mement, nous n'avons g√©n√©ralement pas de v√©ritables √©tiquettes des observations lorsque nous utilisons le clustering.\n",
    "\n",
    "Il existe des mesures de qualit√© *internes* et *externes*. Les m√©triques externes utilisent les informations sur la v√©ritable s√©paration connue tandis que les m√©triques internes n'utilisent aucune information externe et √©valuent la qualit√© des clusters en se basant uniquement sur les donn√©es initiales. Le nombre optimal de clusters est g√©n√©ralement d√©fini par rapport √† certaines m√©triques internes.\n",
    "\n",
    "Toutes les m√©triques d√©crites ci-dessous sont impl√©ment√©es dans `sklearn.metrics`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "**Adjusted Rand Index (ARI)**\n",
    "\n",
    "Ici, nous supposons que les v√©ritables √©tiquettes des objets sont connues. Cette m√©trique ne d√©pend pas des valeurs des √©tiquettes mais de la division du cluster de donn√©es. Soit $N$ le nombre d'observations dans un √©chantillon. Soit $a$ le nombre de paires d'observation avec les m√™mes √©tiquettes et situ√©es dans le m√™me cluster, et que $b$ soit le nombre de paires d'observation avec des √©tiquettes diff√©rentes et situ√©es dans diff√©rents clusters. L'indice Rand peut √™tre calcul√© √† l'aide de la formule suivante: $$\\text{RI} = \\frac{2(a + b)}{n(n-1)}.$$\n",
    "En d'autres termes, il √©value une part de paires d'observation pour lesquelles ces divisions (r√©sultat initial et clustering) sont coh√©rentes. L'indice Rand (RI) √©value la similitude des deux divisions du m√™me √©chantillon. Pour que cet indice soit proche de z√©ro pour tout r√©sultat de clustering avec n'importe quel $n$ et nombre de clusters, il est essentiel de le mettre √† l'√©chelle, d'o√π l'indice Adjusted Rand Index: $$\\text{ARI} = \\frac{\\text{RI} - E[\\text{RI}]}{\\max(\\text{RI}) - E[\\text{RI}]}.$$\n",
    "\n",
    "Cette m√©trique est sym√©trique et ne d√©pend pas de la permutation d'√©tiquette. Par cons√©quent, cet indice est une mesure des distances entre diff√©rentes divisions d'√©chantillon. $\\text{ARI}$ prend des valeurs dans la plage $[-1, 1]$. Les valeurs n√©gatives indiquent l'ind√©pendance des divisions et les valeurs positives indiquent que ces divisions sont coh√©rentes (elles correspondent √† $\\text{ARI} = 1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "**Adjusted Mutual Information (AMI)**\n",
    "\n",
    "Cette m√©trique est similaire √† $\\text{ARI}$. Elle est √©galement sym√©trique et ne d√©pend pas des valeurs et de la permutation des √©tiquettes. Il est d√©fini par la fonction [entropie](https://en.wikipedia.org/wiki/Entropy_(information_theory) et interpr√®te un √©chantillon divis√© comme une distribution discr√®te (la probabilit√© d'affecter √† un cluster est √©gal au pourcentage d'objets qu'il contient.) L'index $MI$ est d√©fini comme l'[information mutuelle](https://en.wikipedia.org/wiki/Mutual_information) pour deux distributions, correspondant √† l'√©chantillon divis√© en grappes. Intuitivement, les informations mutuelles mesurent la part des informations communes aux deux clusters, c'est-√†-dire comment les informations sur l'un d'entre eux diminuent l'incertitude de l'autre.\n",
    "\n",
    "De la m√™me mani√®re que $\\text{ARI}$, $\\text{AMI}$ est d√©fini. Cela nous permet de nous d√©barrasser de l'augmentation de l'indice $MI$ avec le nombre de clusters. Le $\\text{AMI}$ se situe dans la gamme $[0, 1]$. Des valeurs proches de z√©ro signifient que les divisions sont ind√©pendantes, et celles proches de 1 signifient qu'elles sont similaires (avec correspondance compl√®te √† $\\text{AMI} = 1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "**Homog√©n√©it√©, exhaustivit√©, V-mesure**\n",
    "\n",
    "Formellement, ces m√©triques sont √©galement d√©finies en fonction de la fonction d'entropie et de la fonction d'entropie conditionnelle, interpr√©tant les r√©partitions d'√©chantillon comme des distributions discr√®tes: $$h = 1 - \\frac{H(C\\mid K)}{H(C)}, c = 1 - \\frac{H(K\\mid C)}{H(K)},$$\n",
    "o√π $K$ est un r√©sultat de clustering et $C$ est la division initiale. Par cons√©quent, $h$ √©value si chaque cluster est compos√© des m√™mes objets de classe, et $c$ mesure dans quelle mesure les m√™mes objets de classe s'adaptent aux clusters. Ces m√©triques ne sont pas sym√©triques. Les deux se situent dans la plage $[0, 1]$, et des valeurs plus proches de 1 indiquent des r√©sultats de clustering plus pr√©cis. Les valeurs de ces m√©triques ne sont pas mises √† l'√©chelle comme le sont les m√©triques $\\text{ARI}$ ou $\\text{AMI}$ et d√©pendent donc du nombre de clusters. Un r√©sultat de clustering al√©atoire n'aura pas de valeurs de mesures plus proches de z√©ro lorsque le nombre de clusters est suffisamment grand et le nombre d'objets est petit. Dans un tel cas, il serait plus raisonnable d'utiliser $\\text{ARI}$. Cependant, avec un grand nombre d'observations (plus de 100) et un nombre de grappes inf√©rieur √† 10, ce probl√®me est moins critique et peut √™tre ignor√©.\n",
    "\n",
    "$V$-mesure (ùëâ-measure) est une combinaison de $h$ et $c$ et est leur moyenne harmonique:\n",
    "$$v = 2\\frac{hc}{h+c}.$$\n",
    "Il est sym√©trique et mesure la coh√©rence de deux r√©sultats de regroupement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "**Silhouette**\n",
    "\n",
    "Contrairement aux m√©triques d√©crites ci-dessus, ce coefficient n'implique pas la connaissance des v√©ritables √©tiquettes des objets. Il nous permet d'estimer la qualit√© du clustering en utilisant uniquement l'√©chantillon initial sans √©tiquette et le r√©sultat du clustering. Pour commencer, pour chaque observation, le coefficient de silhouette est calcul√©. Soit $a$ la moyenne de la distance entre un objet et d'autres objets au sein d'un cluster et $b$ la distance moyenne d'un objet aux objets du cluster le plus proche (diff√©rente de celle √† laquelle appartient l'objet). Ensuite, la mesure de silhouette pour cet objet est $$s = \\frac{b - a}{\\max(a, b)}.$$\n",
    "\n",
    "La silhouette d'un √©chantillon est une valeur moyenne des valeurs de silhouette de cet √©chantillon. Par cons√©quent, la distance de la silhouette montre dans quelle mesure la distance entre les objets d'une m√™me classe diff√®re de la distance moyenne entre les objets de diff√©rents groupes. Ce coefficient prend des valeurs dans la plage $[-1, 1]$. Des valeurs proches de -1 correspondent √† de mauvais r√©sultats de clustering tandis que des valeurs plus proches de 1 correspondent √† des clusters denses et bien d√©finis. Par cons√©quent, plus la valeur de silhouette est √©lev√©e, meilleurs sont les r√©sultats du clustering.\n",
    "\n",
    "√Ä l'aide de silhouette, nous pouvons identifier le nombre optimal de grappes $k$ (si nous ne le savons pas d√©j√† √† partir des donn√©es) en prenant le nombre de grappes qui maximise le coefficient de silhouette."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "Pour conclure, examinons comment ces mesures fonctionnent avec le jeu de donn√©es de nombres manuscrits MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, metrics\n",
    "from sklearn.cluster import (\n",
    "    AffinityPropagation,\n",
    "    AgglomerativeClustering,\n",
    "    KMeans,\n",
    "    SpectralClustering,\n",
    ")\n",
    "\n",
    "data = datasets.load_digits()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "algorithms = []\n",
    "algorithms.append(KMeans(n_clusters=10, random_state=1))\n",
    "algorithms.append(AffinityPropagation())\n",
    "algorithms.append(\n",
    "    SpectralClustering(n_clusters=10, random_state=1, affinity=\"nearest_neighbors\")\n",
    ")\n",
    "algorithms.append(AgglomerativeClustering(n_clusters=10))\n",
    "\n",
    "data = []\n",
    "for algo in algorithms:\n",
    "    algo.fit(X)\n",
    "    data.append(\n",
    "        (\n",
    "            {\n",
    "                \"ARI\": metrics.adjusted_rand_score(y, algo.labels_),\n",
    "                \"AMI\": metrics.adjusted_mutual_info_score(y, algo.labels_),\n",
    "                \"Homogenity\": metrics.homogeneity_score(y, algo.labels_),\n",
    "                \"Completeness\": metrics.completeness_score(y, algo.labels_),\n",
    "                \"V-measure\": metrics.v_measure_score(y, algo.labels_),\n",
    "                \"Silhouette\": metrics.silhouette_score(X, algo.labels_),\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "results = pd.DataFrame(\n",
    "    data=data,\n",
    "    columns=[\"ARI\", \"AMI\", \"Homogenity\", \"Completeness\", \"V-measure\", \"Silhouette\"],\n",
    "    index=[\"K-means\", \"Affinity\", \"Spectral\", \"Agglomerative\"],\n",
    ")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "## 4. Liens utiles\n",
    "- Overview of clustering methods in the [scikit-learn doc](http://scikit-learn.org/stable/modules/clustering.html).\n",
    "- [Q&A](http://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues) for PCA with examples\n",
    "- [Notebook](https://github.com/diefimov/MTH594_MachineLearning/blob/master/ipython/Lecture10.ipynb) on k-means and the EM-algorithm</div><i class=\"fa fa-lightbulb-o \"></i>"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
