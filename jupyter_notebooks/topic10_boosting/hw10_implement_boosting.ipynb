{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\">\n",
    "## Открытый курс по машинному обучению\n",
    "</center>\n",
    "<center>Автор материала: программист-исследователь Mail.ru Group, старший преподаватель <br>Факультета Компьютерных Наук ВШЭ Юрий Кашницкий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Домашнее задание № 10\n",
    "## <center> Реализация градиентного бустинга\n",
    "\n",
    "В этом задании мы реализуем алгоритм градиентного бустинга в довольно общем виде, один и тот же класс будет описывать бинарный классификатор, при обучении которого минимизируется логистическая фунцкция потерь и 2 регрессора, минимизирующих среднеквадратическую ошибку MSE и [RMSLE](https://www.kaggle.com/wiki/RootMeanSquaredLogarithmicError). Это даст представление о том, что с помощью градиентного бустинга можно оптимизировать произвольные дифференцируемые функции потерь, а также что бустинг адаптируется под разные задачи. [Веб-форма](https://goo.gl/forms/mMUhGSDiOHJI9NHN2) для ответов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Версию алгоритма берем из [статьи](https://habrahabr.ru/company/ods/blog/327250/#klassicheskiy-gbm-algoritm-friedman-a) (см. псевдокод), но с двумя упрощениями:\n",
    "- инициализация – средним значением вектора $\\large y$, то есть $\\large \\hat{f_0} = \\frac{1}{n}\\sum_{i=1}^{n}y_i$\n",
    "- шаг градиентного спуска (то же что и вес очередного базового алгоритма в композиции) постоянный: $\\large \\rho_t = const$\n",
    "\n",
    "Соответствие обозначений в псевдокоде и в классе `GradientBoosting`, который мы сейчас напишем:\n",
    "- $\\large \\{x_i, y_i\\}_{i = 1,\\ldots n}$ или `X`, `y` – обучающая выборка\n",
    "- $\\large L(y,f)$ или `objective` – функция потерь\n",
    "- $\\large \\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)}$ или `objective_grad` – градиент функции потерь\n",
    "- $М$ или `n_estimators` – число итераций бустинга\n",
    "- $\\large h(x,\\theta)$ или `DecisionionTreeRegressor` – базовый алгоритм, дерево решений для регрессии\n",
    "- $\\large \\theta$ – гиперпараметры деревьев, мы рассмотрим только `max_depth` и `random_state`\n",
    "- $\\large \\rho_t$ или `learning_rate` – коэффициент, с которым  $\\large h_t(x,\\theta)$ входят в композицию, $t=1,\\ldots,M$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вычисление градиентов log_loss, MSE и RMSLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала классика – возьмем ручку и бумажку и посчитаем градиенты функций потерь:\n",
    "\n",
    "$$log\\_loss(y, p) = y\\log p + (1 - y)\\log (1 - p) = \\sum_{i=1}^{n}y_i\\log p_i + (1 - y_i)\\log (1 - p_i)$$\n",
    "\n",
    "$$MSE(y, p) = \\frac{1}{n}(y - p)^T(y - p) = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - p_i)^2$$\n",
    "\n",
    "$$RMSLE(y, p) = \\sqrt{\\frac{1}{n} (\\log (p + 1) - \\log (y + 1))^T(\\log (p + 1) - \\log (y + 1))} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}(\\log (p_i + 1) - \\log (y_i + 1))^2}$$\n",
    "\n",
    "Здесь $y$ и $p$ – это **векторы** истинных ответов и прогнозов соответственно.\n",
    "`log_loss` взяли как в  `sklearn` – для случая меток целевого класса 0 и 1, а не -1 и 1, как описано в статье.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Вопрос 1.</font> Какова формула градиента функции `MSE`?\n",
    " <br><br>\n",
    " 1. $(p - y) \\hspace{5cm}$ 3. $2(p - y)$\n",
    " <br><br>\n",
    " 2. $\\frac{2}{n}(y - p) \\hspace{4.7cm}$ 4. $\\frac{2}{n}(p - y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Вопрос 2.</font> Какова формула градиента функции `log_loss`?\n",
    " 1. $\\large \\frac{y - p}{y(1 - y)} \\hspace{5.2cm}$ 3. $\\large \\frac{p - y}{p(1 - p)}$\n",
    " <br><br>\n",
    " 2. $\\large \\frac{y - p}{p(1 - p)}\\hspace{5.2cm}$ 4. $\\large \\frac{p - y}{y(1 - y)}$\n",
    " <br><br>\n",
    " *Примечание:* деление на вектор – покомпонентное, например $\\frac{1}{p} = (\\frac{1}{p_1}, \\ldots \\frac{1}{p_n})^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Вопрос 3.</font> Какова формула градиента функции `RMSLE`?\n",
    " 1. $\\frac{1}{n}~(p + 1)~RMSLE^{-1}(y, p) \\log \\frac{p+1}{y+1} \\hspace{5cm}$ 3. $[n~(y + 1)~RMSLE(y, p)]^{-1} \\log \\frac{p+1}{y+1} $\n",
    " <br><br>\n",
    " 2. $[n~(p + 1)~RMSLE(y, p)]^{-1} \\log \\frac{p+1}{y+1} \\hspace{5cm}$ 4. $\\frac{1}{n}~\\frac{y+1}{(p + 1)}~RMSLE^{-1}(y, p) \\log \\frac{p+1}{y+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация алгоритма"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем класс `GradientBoosting`. Детали:\n",
    "- класс наследуется от `sklearn.base.BaseEstimator`;\n",
    "- конструктор принимает параметры `loss` – название фунцкии потерь, которая будет оптимизироваться (`log_loss`, `mse` (по умолчанию) или `rmsle`), `n_estimators` – число деревьев (т.е. число итераций бустинга, по умолчанию – 10), `learning_rate` – шаг градиентного спуска (по умолчанию $10^{-2}$), `max_depth` – ограничение на максимальную глубину деревьев (по умолчанию 3) и `random_state` – сид генерации псевдослучайных чисел, нужен только для деревьев (по умолчанию 17);\n",
    "- в зависимости от переданного значения `loss` инициализируются `objective` и `objective_grad`. Для `MSE` берем `sklearn.metrics.mean_squared_error`, для `log_loss` – `sklearn.metrics.log_loss`, а `RMSLE` и градиенты всех трех функций надо реализовать самостоятельно. При подсчете градиентов не выкидывайте константы типа двойки или $n$;\n",
    "- в реализациях градиентов `log_loss` и `rmsle` будет покомпонентное деление на вектора. Чтобы избежать деление на 0, предварительно замените значения, меньшие $10^{-5}$, на $10^{-5}$. Но только там, где необходимо. Например, в случае вычисления $\\frac{y}{p}$ замены делаем только в векторе $p$;\n",
    "- также в конструкторе создаются списки `loss_by_iter_` и `residuals_by_iter_` для отлаживания работы алгоритма и `trees_` – для хранения обученных деревьев;\n",
    "- класс имеет методы `fit`, `predict` и `predict_proba`\n",
    "- метод `fit` принимает матрицу `X` и вектор `y` (объекты `numpy.array`), а возвращает текущий экземпляр класса `GradientBoosting`, т.е. `self`. Основная логика, конечно же, реализуется здесь. На каждой итерации текущее значение функции потерь записывается в `loss_by_iter_`, значение антиградиента (то что в статье названо псевдо-остатками) – в `residuals_by_iter_` (можно в конструктор добавить флаг `debug=False` и добавлять значения антиградиента только при включенном флаге). Также обученное дерево добавляется в список `trees_`;\n",
    "- метод `predict_proba` возвращает линейную комбинацию прогнозов деревьев. Не забудем тут и про начальное приближение. В случае регрессии название метода будет не очень удачным, но оставим так, чтоб не писать отдельно классификатор и регрессор. В случае классификации к ответу применяется $\\sigma$-преобразование. В реализации $\\sigma$-функции замените значения аргумента, превышающие по модулю 100, на 100 или -100 в зависимости от знака (чтоб избежать underflow & overflow);\n",
    "- метод `predict` в случае регрессии возвращает линейную комбинацию прогнозов деревьев (+ начальное приближение), то есть то же, что и `predict_proba`. В случае классификации метод `predict` задействует метод `predict_proba` и возвращает вектор из 0 и 1, полученный сравнением предсказанных вероятностей с некоторым порогом, при котором максимизируется доля правильных ответов на обучающей выборке. Здесь хорошо было бы решить одномерную задачу оптимизации, но для полной воспроизводимости давайте выбирать порог из `np.linspace(0.01, 1.01, 100)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.datasets import load_breast_cancer, load_boston\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GradientBoosting(BaseEstimator):\n",
    "    \n",
    "    def sigma(self, z):\n",
    "        pass\n",
    "    \n",
    "    def log_loss_grad(self, y, p):\n",
    "        pass\n",
    "    \n",
    "    def mse_grad(self, y, p):\n",
    "        pass\n",
    "    \n",
    "    def rmsle(self, y, p):\n",
    "        pass\n",
    "    \n",
    "    def rmsle_grad(self, y, p):\n",
    "        pass\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Регрессия с игрушечным примером"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_regr_toy = np.arange(7).reshape(-1, 1)\n",
    "y_regr_toy = ((X_regr_toy - 3) ** 2).astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f945fefb128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_regr_toy, y_regr_toy);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите регрессор `GradientBoosting` с функцией потерь `MSE` и параметрами `learning_rate`=0.1,  `max_depth`=3 - 200 итераций. Посмотрите на изменение функции потерь по итерациям бустинга. Можно также посмотреть на приближение и остатки на первых нескольких итерациях, как это делалось в статье."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите регрессор `GradientBoosting` с теми же параметрами, но функцию потерь измените на `RMSLE`. Посмотрите на те же картинки. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация с игрушечным примером"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_clf_toy = np.c_[np.arange(7), (np.arange(7) - 3) ** 2]\n",
    "y_clf_toy = np.array([0, 1, 0, 1, 0, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f945dbfbef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_clf_toy[:, 0], X_clf_toy[:, 1], c=y_clf_toy);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите классификатор `GradientBoosting` с функцией потерь `log_loss` и параметрами `learning_rate`=0.05,  `max_depth`=3 – 10 итераций. Посмотрите на изменение функции потерь по итерациям бустинга. Можно также посмотреть на приближение и остатки на первых нескольких итерациях, как это делалось в статье.\n",
    "\n",
    "\n",
    "\n",
    "<font color='red'>Вопрос 4.</font> Посчитайте предсказанные вероятности отнесения к классу +1 для всех 7 объектов игрушечной выборки. Каковы 2 уникальных значения в полученном векторе?\n",
    "1. 0.42 и 0.77\n",
    "2. 0.36 и 0.82\n",
    "3. 0.48 и 0.53\n",
    "4. 0.46 и 0.75\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регрессия с UCI boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Обучите регрессор `GradientBoosting` с функцией потерь `MSE` и параметрами `learning_rate`=3,  `max_depth`=10 - 300 итераций\n",
    "- Посмотрите на изменение функции потерь по итерациям бустинга\n",
    "- Сделайте прогнозы для отложенной выборки\n",
    "- Постройте распределение ответов `y_test` на отложенной выборке и наложите на него распределение ответов бустинга `test_pred`. Используйте метод `hist` из `matplotlib.pyplot` с параметром `bins=15`\n",
    "\n",
    "<font color='red'>Вопрос 5.</font> Выберите верное утверждение про гистограммы:\n",
    "1. Ответы бустинга в среднем завышены на 10 \n",
    "2. В бине, в который попадает медиана ответов на тестовой выборке (`numpy.median(y_test)`), больше значений из вектора прогнозов `test_pred`, чем из вектора ответов `y_test`\n",
    "3. Бустинг иногда прогнозирует значения, сильно выпадающие за диапазон изменения ответов `y_test`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация с UCI breast cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()\n",
    "X, y = cancer.data, cancer.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите классификатор `GradientBoosting` с функцией потерь `log_loss` и параметрами `learning_rate`=0.01,  `max_depth`=3 - 200 итераций. Посмотрите на изменение функции потерь по итерациям бустинга. Сделайте прогнозы для отложенной выборки – как предсказанные вероятности отнесения к классу +1, так и бинарные прогнозы. Посчитайте ROC AUC для прогноза в виде вероятностей и долю правильных ответов для прогноза в виде меток классов.\n",
    "\n",
    "<font color='red'>Вопрос 6.</font> Каковы получаются ROC AUC и доля правильных ответов на отложенной выборке `(X_test, y_test)`?\n",
    "1. 0.99 и 0.97\n",
    "2. 1 и 0.97\n",
    "3. 0.98 и 0.96\n",
    "4. 0.97 и 0.95"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
